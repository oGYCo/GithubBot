"""
æ•°æ®æ³¨å…¥æœåŠ¡
è´Ÿè´£å®Œæ•´çš„ä»“åº“åˆ†ææµæ°´çº¿ï¼šå…‹éš† -> è§£æ -> åˆ†å— -> å‘é‡åŒ– -> å­˜å‚¨
"""

import os
import time
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from sqlalchemy.orm import Session
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from datetime import datetime, timezone

from ..core.config import settings
from ..db.session import get_db_session
from ..db.models import AnalysisSession, FileMetadata, TaskStatus
from ..utils.git_helper import GitHelper
from ..utils.file_parser import FileParser
from ..services.embedding_manager import EmbeddingManager, EmbeddingConfig
from ..services.vector_store import get_vector_store

logger = logging.getLogger(__name__)


class IngestionService:
    """æ•°æ®æ³¨å…¥æœåŠ¡"""

    def __init__(self):
        self.file_parser = FileParser()
        self.git_helper = GitHelper()

    def process_repository(
            self,
            repo_url: str,
            session_id: str,
            embedding_config: Dict[str, Any],
            task_instance=None
    ) -> bool:
        """
        å¤„ç†ä»“åº“çš„å®Œæ•´æµæ°´çº¿

        Args:
            repo_url: ä»“åº“ URL
            session_id: ä¼šè¯ ID
            embedding_config: Embedding é…ç½®

        Returns:
            bool: æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        db = get_db_session()
        error_occurred = False
        error_messages = []

        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤„ç†ä¸­
            logger.info(f"ğŸ“Š [çŠ¶æ€æ›´æ–°] ä¼šè¯ID: {session_id} - ä»»åŠ¡çŠ¶æ€è®¾ç½®ä¸ºå¤„ç†ä¸­")
            self._update_session_status(db, session_id, TaskStatus.PROCESSING, started_at=datetime.now(timezone.utc))
            self._update_task_progress(task_instance, 5, "ä»»åŠ¡åˆå§‹åŒ–å®Œæˆ")

            # 1. é…ç½®å’Œæ¨¡å‹åŠ è½½å—
            try:
                logger.info(f"âš™ï¸ [é…ç½®åŠ è½½] ä¼šè¯ID: {session_id} - åˆ›å»ºEmbeddingé…ç½®")
                embedding_cfg = EmbeddingConfig.from_dict(embedding_config)
                self._update_task_progress(task_instance, 10, "é…ç½®åŠ è½½å®Œæˆ")

                logger.info(f"ğŸ¤– [æ¨¡å‹åŠ è½½] ä¼šè¯ID: {session_id} - æ­£åœ¨åŠ è½½ {embedding_cfg.provider}/{embedding_cfg.model_name} æ¨¡å‹")
                embedding_model = EmbeddingManager.get_embedding_model(embedding_cfg)
                logger.info(f"âœ… [æ¨¡å‹å°±ç»ª] ä¼šè¯ID: {session_id} - Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ")
                self._update_task_progress(task_instance, 15, "Embeddingæ¨¡å‹åŠ è½½å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ [å…³é”®å¤±è´¥] ä¼šè¯ID: {session_id} - Embeddingé…ç½®æˆ–æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                raise  # è¿™æ˜¯å…³é”®æ­¥éª¤ï¼Œå¤±è´¥åˆ™æ— æ³•ç»§ç»­

            # 2. å‘é‡æ•°æ®åº“åˆ›å»ºå—
            try:
                logger.info(f"ğŸ—„ï¸ [æ•°æ®åº“] ä¼šè¯ID: {session_id} - åˆ›å»ºå‘é‡æ•°æ®åº“é›†åˆ")
                vector_store = get_vector_store()
                if not vector_store.create_collection(session_id, embedding_model):
                    raise Exception("åˆ›å»ºå‘é‡æ•°æ®åº“é›†åˆå¤±è´¥")
                logger.info(f"âœ… [æ•°æ®åº“å°±ç»ª] ä¼šè¯ID: {session_id} - å‘é‡æ•°æ®åº“é›†åˆåˆ›å»ºæˆåŠŸ")
                self._update_task_progress(task_instance, 20, "å‘é‡æ•°æ®åº“é›†åˆåˆ›å»ºå®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ [å…³é”®å¤±è´¥] ä¼šè¯ID: {session_id} - å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
                raise # è¿™æ˜¯å…³é”®æ­¥éª¤ï¼Œå¤±è´¥åˆ™æ— æ³•ç»§ç»­

            # 3. ä»“åº“å…‹éš†å’Œä¿¡æ¯è§£æå—
            repo_path = None
            try:
                logger.info(f"ğŸ“¥ [ä»“åº“å…‹éš†] ä¼šè¯ID: {session_id} - å¼€å§‹å…‹éš†ä»“åº“: {repo_url}")
                repo_path = self.git_helper.clone_repository(repo_url)
                logger.info(f"âœ… [å…‹éš†å®Œæˆ] ä¼šè¯ID: {session_id} - ä»“åº“å…‹éš†åˆ°: {repo_path}")
                self._update_task_progress(task_instance, 30, "ä»“åº“å…‹éš†å®Œæˆ")

                logger.info(f"ğŸ“‹ [ä»“åº“ä¿¡æ¯] ä¼šè¯ID: {session_id} - è§£æä»“åº“ä¿¡æ¯")
                owner, repo_name = self.git_helper.extract_repo_info(repo_url)
                self._update_session_repo_info(db, session_id, repo_name, owner)
                logger.info(f"ğŸ“ [ä»“åº“è¯¦æƒ…] ä¼šè¯ID: {session_id} - ä»“åº“: {owner}/{repo_name}")
                self._update_task_progress(task_instance, 35, "ä»“åº“ä¿¡æ¯è§£æå®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ [å…³é”®å¤±è´¥] ä¼šè¯ID: {session_id} - ä»“åº“å…‹éš†æˆ–ä¿¡æ¯è§£æå¤±è´¥: {e}")
                raise # è¿™æ˜¯å…³é”®æ­¥éª¤ï¼Œå¤±è´¥åˆ™æ— æ³•ç»§ç»­

            # 4. æ–‡ä»¶å¤„ç†å—
            all_documents = []
            try:
                logger.info(f"ğŸ“ [æ–‡ä»¶æ‰«æ] ä¼šè¯ID: {session_id} - å¼€å§‹æ‰«æå’Œå¤„ç†ä»“åº“æ–‡ä»¶")
                processed_files, total_chunks, all_documents = self._process_repository_files(
                    db, session_id, repo_path, task_instance
                )
                logger.info(f"ğŸ“Š [æ‰«æç»“æœ] ä¼šè¯ID: {session_id} - å¤„ç†æ–‡ä»¶: {processed_files}, ç”Ÿæˆå—: {total_chunks}")
                self._update_task_progress(task_instance, 70, f"æ–‡ä»¶å¤„ç†å®Œæˆ: {processed_files}ä¸ªæ–‡ä»¶, {total_chunks}ä¸ªå—")
            except Exception as e:
                logger.error(f"âŒ [é”™è¯¯] ä¼šè¯ID: {session_id} - æ–‡ä»¶å¤„ç†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                error_occurred = True
                error_messages.append(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")

            # 5. å‘é‡åŒ–å’Œå­˜å‚¨å—
            if all_documents:
                try:
                    logger.info(f"ğŸ”„ [å‘é‡åŒ–] ä¼šè¯ID: {session_id} - å¼€å§‹å‘é‡åŒ– {len(all_documents)} ä¸ªæ–‡æ¡£å—")
                    self._vectorize_and_store_documents(db, session_id, all_documents, embedding_model, task_instance, embedding_cfg.batch_size)
                    logger.info(f"âœ… [å‘é‡åŒ–å®Œæˆ] ä¼šè¯ID: {session_id} - æ‰€æœ‰æ–‡æ¡£å‘é‡åŒ–å¹¶å­˜å‚¨å®Œæˆ")
                except Exception as e:
                    logger.error(f"âŒ [é”™è¯¯] ä¼šè¯ID: {session_id} - å‘é‡åŒ–å’Œå­˜å‚¨è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                    error_occurred = True
                    error_messages.append(f"å‘é‡åŒ–å¤±è´¥: {e}")
            else:
                logger.warning(f"âš ï¸ [æ— æ–‡æ¡£] ä¼šè¯ID: {session_id} - ä»“åº“æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡æ¡£å—")
            self._update_task_progress(task_instance, 95, "å‘é‡åŒ–å’Œå­˜å‚¨å®Œæˆ")

            # 6. ä»»åŠ¡å®ŒæˆçŠ¶æ€åˆ¤æ–­
            if error_occurred:
                final_status = TaskStatus.PARTIAL_SUCCESS
                final_message = "ä»»åŠ¡éƒ¨åˆ†æˆåŠŸï¼Œå¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: " + "; ".join(error_messages)
                logger.warning(f"ğŸ [ä»»åŠ¡éƒ¨åˆ†æˆåŠŸ] ä¼šè¯ID: {session_id} - {final_message}")
                self._update_session_status(
                    db, session_id, final_status,
                    error_message=final_message,
                    completed_at=datetime.now(timezone.utc)
                )
                self._update_task_progress(task_instance, 100, "ä»»åŠ¡éƒ¨åˆ†æˆåŠŸ")
                return True # å³ä½¿æœ‰é”™ï¼Œä¹Ÿç®—æµç¨‹è·‘å®Œ
            else:
                logger.info(f"ğŸ [ä»»åŠ¡å®Œæˆ] ä¼šè¯ID: {session_id} - æ ‡è®°ä»»åŠ¡ä¸ºæˆåŠŸçŠ¶æ€")
                self._update_session_status(
                    db, session_id, TaskStatus.SUCCESS,
                    completed_at=datetime.now(timezone.utc)
                )
                self._update_task_progress(task_instance, 100, "ä»»åŠ¡å®Œæˆ")
                logger.info(f"ğŸ‰ [å¤„ç†æˆåŠŸ] ä¼šè¯ID: {session_id} - ä»“åº“ {repo_url} åˆ†æå®Œæˆ")
                return True

        except Exception as e:
            error_msg = str(e)
            logger.error(f"å¤„ç†ä»“åº“æ—¶å‘ç”Ÿå…³é”®å¤±è´¥ {repo_url}: {error_msg}")

            # æ ‡è®°ä»»åŠ¡å¤±è´¥
            self._update_session_status(
                db, session_id, TaskStatus.FAILED,
                error_message=error_msg,
                completed_at=datetime.now(timezone.utc)
            )

            return False

        finally:
            if db:
                db.close()

    def _process_repository_files(
            self,
            db: Session,
            session_id: str,
            repo_path: str,
            task_instance=None
    ) -> Tuple[int, int, List[Document]]:
        """
        å¤„ç†ä»“åº“ä¸­çš„æ‰€æœ‰æ–‡ä»¶

        Args:
            db: æ•°æ®åº“ä¼šè¯
            session_id: ä¼šè¯ ID
            repo_path: ä»“åº“è·¯å¾„

        Returns:
            Tuple[int, int, List[Document]]: (å¤„ç†çš„æ–‡ä»¶æ•°, æ€»å—æ•°, æ‰€æœ‰æ–‡æ¡£å—)
        """
        total_chunks = 0
        processed_files = 0

        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£å—å’Œå…ƒæ•°æ®
        all_documents = []
        all_file_metadata = []

        # æ‰«æä»“åº“æ–‡ä»¶
        logger.info(f"ğŸ” [æ–‡ä»¶æ‰«æ] ä¼šè¯ID: {session_id} - å¼€å§‹æ‰«æä»“åº“æ–‡ä»¶")
        files_to_process = list(self.file_parser.scan_repository(repo_path))
        total_files = len(files_to_process)
        logger.info(f"ğŸ“‹ [æ‰«æå®Œæˆ] ä¼šè¯ID: {session_id} - å‘ç° {total_files} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
        self._update_session_stats(db, session_id, total_files=total_files)  # åˆå§‹æ›´æ–°æ€»æ–‡ä»¶æ•°

        for file_index, (file_path, file_info) in enumerate(files_to_process, 1):
            # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶è·¯å¾„å˜é‡å
            relative_file_path = file_info["file_path"]
            
            # åˆ›å»ºæ–‡ä»¶å…ƒæ•°æ®è®°å½•
            file_metadata = FileMetadata(
                session_id=session_id,
                file_path=relative_file_path,
                file_type=file_info["file_type"],
                file_extension=file_info.get("file_extension"),
                file_size=file_info["file_size"],
                is_processed="pending"
            )

            try:
                # æ˜¾ç¤ºå½“å‰å¤„ç†è¿›åº¦
                if file_index % 10 == 1 or file_index <= 5:  # å‰5ä¸ªæ–‡ä»¶å’Œæ¯10ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡
                    logger.info(f"ğŸ“„ [æ–‡ä»¶å¤„ç†] ä¼šè¯ID: {session_id} - å¤„ç†ç¬¬ {file_index}/{total_files} ä¸ªæ–‡ä»¶: {relative_file_path}")
                
                # æ›´æ–°ä»»åŠ¡è¿›åº¦ (35% åˆ° 70% ä¹‹é—´)
                progress = 35 + int((file_index / total_files) * 35)
                self._update_task_progress(task_instance, progress, f"å¤„ç†æ–‡ä»¶ {file_index}/{total_files}: {relative_file_path}")

                # è¯»å–æ–‡ä»¶å†…å®¹
                content = self.file_parser.read_file_content(file_path)
                if not content:
                    file_metadata.is_processed = "skipped"
                    file_metadata.error_message = "æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹æˆ–æ–‡ä»¶ä¸ºç©º"
                    logger.debug(f"â­ï¸ [è·³è¿‡æ–‡ä»¶] ä¼šè¯ID: {session_id} - æ–‡ä»¶ä¸ºç©º: {relative_file_path}")
                    continue

                # è®¡ç®—è¡Œæ•°
                file_metadata.line_count = len(content.split('\n'))
                logger.debug(f"ğŸ“Š [æ–‡ä»¶ä¿¡æ¯] ä¼šè¯ID: {session_id} - {relative_file_path}: {file_metadata.line_count} è¡Œ, {file_info['file_size']} å­—èŠ‚")

                # è§£æç‰¹æ®Šæ–‡ä»¶
                if file_info["file_type"] in ["config", "document"]:
                    special_info = self.file_parser.parse_special_files(file_path, content)
                    if special_info.get("type") != "unknown":
                        file_metadata.content_summary = f"{special_info.get('type', '')} æ–‡ä»¶"
                        if "dependencies" in special_info:
                            file_metadata.dependencies = special_info["dependencies"]
                        logger.debug(f"ğŸ”§ [ç‰¹æ®Šæ–‡ä»¶] ä¼šè¯ID: {session_id} - {relative_file_path}: {special_info.get('type', '')}")

                # åˆ†å‰²æ–‡æ¡£
                # ä»æ–‡ä»¶ä¿¡æ¯ä¸­è·å–è¯­è¨€ç±»å‹
                file_type, language = self.file_parser.get_file_type_and_language(file_path)
                documents = self.file_parser.split_file_content(
                    content,
                    relative_file_path,
                    language=language
                )

                if documents:
                    # ä¸ºæ¯ä¸ªæ–‡æ¡£å—æ·»åŠ å…¨å±€ç´¢å¼•
                    for i, doc in enumerate(documents):
                        doc.metadata['chunk_index'] = total_chunks + i

                    all_documents.extend(documents)
                    file_metadata.chunk_count = len(documents)
                    total_chunks += len(documents)
                    file_metadata.is_processed = "success"
                    processed_files += 1
                    
                    if len(documents) > 1:
                        logger.debug(f"âœ‚ï¸ [æ–‡æ¡£åˆ†å—] ä¼šè¯ID: {session_id} - {relative_file_path}: ç”Ÿæˆ {len(documents)} ä¸ªå—")
                else:
                    file_metadata.is_processed = "skipped"
                    file_metadata.error_message = "æœªç”Ÿæˆæ–‡æ¡£å—"
                    logger.debug(f"âš ï¸ [æ— å—ç”Ÿæˆ] ä¼šè¯ID: {session_id} - {relative_file_path}: æœªç”Ÿæˆæ–‡æ¡£å—")

            except (IOError, UnicodeDecodeError) as e:
                logger.error(f"ğŸ’¥ [è¯»å–å¤±è´¥] ä¼šè¯ID: {session_id} - æ–‡ä»¶ {relative_file_path}: {str(e)}")
                file_metadata.is_processed = "failed"
                file_metadata.error_message = f"æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}"
            except Exception as e:
                logger.error(f"ğŸ’¥ [å¤„ç†å¤±è´¥] ä¼šè¯ID: {session_id} - æ–‡ä»¶ {relative_file_path}: {str(e)}")
                file_metadata.is_processed = "failed"
                file_metadata.error_message = str(e)

            all_file_metadata.append(file_metadata)

            # æ¯50ä¸ªæ–‡ä»¶å…ƒæ•°æ®ä¿å­˜ä¸€æ¬¡
            if len(all_file_metadata) >= 50:
                self._save_metadata_batch(db, all_file_metadata)
                all_file_metadata.clear() # æ¸…ç©ºåˆ—è¡¨ä»¥ä¾¿æ”¶é›†ä¸‹ä¸€æ‰¹

                self._update_session_stats(
                    db, session_id, processed_files=processed_files, total_chunks=total_chunks
                )
                logger.info(f"å·²æ‰«æ {file_index}/{total_files} ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆ {total_chunks} ä¸ªå—")

        # ä¿å­˜æœ€åä¸€æ‰¹å‰©ä½™çš„æ–‡ä»¶å…ƒæ•°æ®
        if all_file_metadata:
            self._save_metadata_batch(db, all_file_metadata)
            all_file_metadata.clear()

        # æ›´æ–°æœ€ç»ˆçš„æ–‡ä»¶å¤„ç†å’Œåˆ†å—ç»Ÿè®¡
        self._update_session_stats(
            db, session_id, processed_files=processed_files, total_chunks=total_chunks
        )
        logger.info(f"æ–‡ä»¶æ‰«æå®Œæˆã€‚æ€»æ–‡ä»¶æ•°: {total_files}, å·²å¤„ç†: {processed_files}, æ€»å—æ•°: {total_chunks}")

        return processed_files, total_chunks, all_documents


    def _save_metadata_batch(self, db: Session, metadata_batch: List[FileMetadata]):
        """
        ä¿å­˜ä¸€æ‰¹æ–‡ä»¶å…ƒæ•°æ®ã€‚å¦‚æœæ‰¹é‡ä¿å­˜å¤±è´¥ï¼Œåˆ™å°è¯•é€ä¸ªä¿å­˜ã€‚
        """
        if not metadata_batch:
            return

        try:
            db.add_all(metadata_batch)
            db.commit()
            logger.info(f"âœ… [å…ƒæ•°æ®ä¿å­˜] æˆåŠŸä¿å­˜ {len(metadata_batch)} ä¸ªæ–‡ä»¶å…ƒæ•°æ®ã€‚")
        except Exception as e:
            logger.error(f"ğŸ’¥ [å…ƒæ•°æ®æ‰¹é‡ä¿å­˜å¤±è´¥] {str(e)}ã€‚å›é€€åˆ°é€ä¸ªä¿å­˜æ¨¡å¼ã€‚")
            db.rollback()
            for metadata in metadata_batch:
                try:
                    db.add(metadata)
                    db.commit()
                except Exception as individual_e:
                    logger.error(f"ğŸ’¥ [å…ƒæ•°æ®å•ä¸ªä¿å­˜å¤±è´¥] æ–‡ä»¶ {metadata.file_path}: {str(individual_e)}")
                    db.rollback()


    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    def _vectorize_and_store_documents(
            self,
            db: Session,
            session_id: str,
            documents: List[Document],
            embedding_model: Embeddings,
            task_instance=None,
            batch_size: int = None
    ):
        """
        å‘é‡åŒ–æ–‡æ¡£å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“

        Args:
            db: æ•°æ®åº“ä¼šè¯
            session_id: ä¼šè¯ ID
            documents: æ–‡æ¡£åˆ—è¡¨
            embedding_model: Embedding æ¨¡å‹
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        if batch_size is None:
            batch_size = settings.EMBEDDING_BATCH_SIZE
        total_docs = len(documents)
        total_batches = (total_docs + batch_size - 1) // batch_size
        any_batch_failed = False

        logger.info(f"ğŸ”„ [å‘é‡åŒ–å¼€å§‹] ä¼šè¯ID: {session_id} - å¼€å§‹å‘é‡åŒ– {total_docs} ä¸ªæ–‡æ¡£å—ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        logger.info(f"ğŸ“Š [æ‰¹æ¬¡ä¿¡æ¯] ä¼šè¯ID: {session_id} - æ€»å…±éœ€è¦å¤„ç† {total_batches} ä¸ªæ‰¹æ¬¡")

        for i in range(0, total_docs, batch_size):
            batch_num = i // batch_size + 1
            batch_docs = documents[i:i + batch_size]
            batch_texts = [doc.page_content for doc in batch_docs]
            actual_batch_size = len(batch_docs)

            try:
                logger.info(f"âš¡ [æ‰¹æ¬¡å¤„ç†] ä¼šè¯ID: {session_id} - å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹æ¬¡ ({actual_batch_size} ä¸ªæ–‡æ¡£)")
                
                # æ¸…ç†å’ŒéªŒè¯æ–‡æœ¬
                cleaned_texts = []
                valid_docs_indices = []
                for index, text in enumerate(batch_texts):
                    # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
                    if not isinstance(text, str):
                        text = str(text)
                    
                    # è·³è¿‡ç©ºæ–‡æ¡£
                    if not text.strip():
                        continue
                        
                    cleaned_texts.append(text)
                    valid_docs_indices.append(index)

                if not cleaned_texts:
                    logger.warning(f"âš ï¸ [ç©ºæ‰¹æ¬¡] ä¼šè¯ID: {session_id} - æ‰¹æ¬¡ {batch_num} ä¸­æ²¡æœ‰æœ‰æ•ˆæ–‡æ¡£å¯å¤„ç†")
                    continue
                
                # å‘é‡åŒ–æ–‡æœ¬
                start_time = time.time()
                logger.info(f"ğŸ§  [å‘é‡åŒ–ä¸­] ä¼šè¯ID: {session_id} - æ­£åœ¨ä¸ºæ‰¹æ¬¡ {batch_num} ç”Ÿæˆå‘é‡...")
                embeddings = embedding_model.embed_documents(cleaned_texts)
                embedding_time = time.time() - start_time
                logger.info(f"âœ… [å‘é‡ç”Ÿæˆ] ä¼šè¯ID: {session_id} - æ‰¹æ¬¡ {batch_num} å‘é‡åŒ–å®Œæˆï¼Œè€—æ—¶ {embedding_time:.2f}s")

                # åˆ›å»ºå¯¹åº”çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆåªåŒ…å«æœ‰æ•ˆçš„æ–‡æ¡£ï¼‰
                valid_docs = [batch_docs[idx] for idx in valid_docs_indices]

                # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
                logger.info(f"ğŸ’¾ [å­˜å‚¨ä¸­] ä¼šè¯ID: {session_id} - æ­£åœ¨å°†æ‰¹æ¬¡ {batch_num} å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
                success = get_vector_store().add_documents_to_collection(
                    session_id, valid_docs, embeddings, len(valid_docs)
                )

                if not success:
                    raise Exception("å‘é‡æ•°æ®åº“å­˜å‚¨å¤±è´¥")
                logger.info(f"âœ… [å­˜å‚¨å®Œæˆ] ä¼šè¯ID: {session_id} - æ‰¹æ¬¡ {batch_num} æ•°æ®å­˜å‚¨æˆåŠŸ")

                # æ›´æ–°è¿›åº¦
                indexed_chunks = min(i + batch_size, total_docs)
                self._update_session_stats(
                    db, session_id, None, None, None, indexed_chunks
                )
                
                # æ›´æ–°ä»»åŠ¡è¿›åº¦ (70% åˆ° 95% ä¹‹é—´)
                progress = 70 + int((batch_num / total_batches) * 25)
                self._update_task_progress(task_instance, progress, f"å‘é‡åŒ–æ‰¹æ¬¡ {batch_num}/{total_batches}")

                logger.info(
                    f"âœ… [æ‰¹æ¬¡å®Œæˆ] ä¼šè¯ID: {session_id} - æ‰¹æ¬¡ {batch_num}/{total_batches} å®Œæˆï¼Œ"
                    f"å‘é‡åŒ–è€—æ—¶ {embedding_time:.2f}sï¼Œå·²å¤„ç† {indexed_chunks}/{total_docs} ä¸ªæ–‡æ¡£"
                )

            except Exception as e:
                logger.error(f"ğŸ’¥ [æ‰¹æ¬¡å¤±è´¥] ä¼šè¯ID: {session_id} - å‘é‡åŒ–æ‰¹æ¬¡ {batch_num} å¤±è´¥ (æ–‡æ¡£ {i}-{i + actual_batch_size}): {str(e)}")
                any_batch_failed = True
                # ä¸å† re-raiseï¼Œè®°å½•é”™è¯¯å¹¶ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ‰¹æ¬¡
                continue

        if any_batch_failed:
            logger.warning(f"âš ï¸ [å‘é‡åŒ–è­¦å‘Š] ä¼šè¯ID: {session_id} - å‘é‡åŒ–è¿‡ç¨‹ä¸­è‡³å°‘æœ‰ä¸€ä¸ªæ‰¹æ¬¡å¤±è´¥ã€‚")
            # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚çŸ¥é“å‘ç”Ÿäº†éƒ¨åˆ†å¤±è´¥
            raise Exception("å‘é‡åŒ–è¿‡ç¨‹ä¸­è‡³å°‘æœ‰ä¸€ä¸ªæ‰¹æ¬¡å¤±è´¥ï¼Œä½†æµç¨‹å·²ç»§ç»­ã€‚")

        logger.info(f"ğŸ‰ [å‘é‡åŒ–å®Œæˆ] ä¼šè¯ID: {session_id} - æ‰€æœ‰æ–‡æ¡£å‘é‡åŒ–å®Œæˆï¼Œå…±å¤„ç† {total_docs} ä¸ªæ–‡æ¡£å—")

    def _update_session_status(
            self,
            db: Session,
            session_id: str,
            status: TaskStatus,
            error_message: str = None,
            started_at: datetime = None,
            completed_at: datetime = None
    ):
        """æ›´æ–°ä¼šè¯çŠ¶æ€"""
        try:
            session = db.query(AnalysisSession).filter(
                AnalysisSession.session_id == session_id
            ).first()

            if session:
                session.status = status
                if error_message:
                    session.error_message = error_message
                if started_at:
                    session.started_at = started_at
                if completed_at:
                    session.completed_at = completed_at

                db.commit()

        except Exception as e:
            logger.error(f"æ›´æ–°ä¼šè¯çŠ¶æ€å¤±è´¥: {str(e)}")
            db.rollback()

    def _update_session_repo_info(
            self,
            db: Session,
            session_id: str,
            repo_name: str,
            repo_owner: str
    ):
        """æ›´æ–°ä¼šè¯ä»“åº“ä¿¡æ¯"""
        try:
            session = db.query(AnalysisSession).filter(
                AnalysisSession.session_id == session_id
            ).first()

            if session:
                session.repository_name = repo_name
                session.repository_owner = repo_owner
                db.commit()

        except Exception as e:
            logger.error(f"æ›´æ–°ä¼šè¯ä»“åº“ä¿¡æ¯å¤±è´¥: {str(e)}")
            db.rollback()

    def _update_session_stats(
            self,
            db: Session,
            session_id: str,
            total_files: Optional[int] = None,
            processed_files: Optional[int] = None,
            total_chunks: Optional[int] = None,
            indexed_chunks: Optional[int] = None
    ):
        """æ›´æ–°ä¼šè¯ç»Ÿè®¡ä¿¡æ¯"""
        try:
            session = db.query(AnalysisSession).filter(
                AnalysisSession.session_id == session_id
            ).first()

            if session:
                if total_files is not None:
                    session.total_files = total_files
                if processed_files is not None:
                    session.processed_files = processed_files
                if total_chunks is not None:
                    session.total_chunks = total_chunks
                if indexed_chunks is not None:
                    session.indexed_chunks = indexed_chunks

                db.commit()

        except Exception as e:
            logger.error(f"æ›´æ–°ä¼šè¯ç»Ÿè®¡å¤±è´¥: {str(e)}")
            db.rollback()

    def _update_task_progress(self, task_instance, progress: int, status: str):
        """æ›´æ–°Celeryä»»åŠ¡è¿›åº¦"""
        if task_instance:
            try:
                task_instance.update_state(
                    state='PROGRESS',
                    meta={
                        'current': progress,
                        'total': 100,
                        'status': status
                    }
                )
            except Exception as e:
                logger.debug(f"æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {str(e)}")


# å…¨å±€æœåŠ¡å®ä¾‹
ingestion_service = IngestionService()