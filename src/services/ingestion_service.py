"""
æ•°æ®æ³¨å…¥æœåŠ¡
è´Ÿè´£å®Œæ•´çš„ä»“åº“åˆ†ææµæ°´çº¿ï¼šå…‹éš† -> è§£æ -> åˆ†å— -> å‘é‡åŒ– -> å­˜å‚¨
"""

import os
import time
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from sqlalchemy.orm import Session
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from datetime import datetime, timezone

from ..core.config import settings
from ..db.session import get_db_session
from ..db.models import AnalysisSession, FileMetadata, TaskStatus
from ..utils.git_helper import GitHelper
from ..utils.file_parser import FileParser
from ..services.embedding_manager import EmbeddingManager, EmbeddingConfig
from ..services.vector_store import get_vector_store

logger = logging.getLogger(__name__)


class IngestionService:
    """æ•°æ®æ³¨å…¥æœåŠ¡"""

    def __init__(self):
        self.file_parser = FileParser()
        self.git_helper = GitHelper()

    def process_repository(
            self,
            repo_url: str,
            session_id: str,
            embedding_config: Dict[str, Any],
            task_instance=None
    ) -> bool:
        """
        å¤„ç†ä»“åº“çš„å®Œæ•´æµæ°´çº¿

        Args:
            repo_url: ä»“åº“ URL
            session_id: ä¼šè¯ ID
            embedding_config: Embedding é…ç½®

        Returns:
            bool: æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        db = get_db_session()

        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤„ç†ä¸­
            logger.info(f"ğŸ“Š [çŠ¶æ€æ›´æ–°] ä¼šè¯ID: {session_id} - ä»»åŠ¡çŠ¶æ€è®¾ç½®ä¸ºå¤„ç†ä¸­")
            self._update_session_status(db, session_id, TaskStatus.PROCESSING, started_at=datetime.now(timezone.utc))
            self._update_task_progress(task_instance, 5, "ä»»åŠ¡åˆå§‹åŒ–å®Œæˆ")

            # åˆ›å»º embedding é…ç½®å¯¹è±¡
            logger.info(f"âš™ï¸ [é…ç½®åŠ è½½] ä¼šè¯ID: {session_id} - åˆ›å»ºEmbeddingé…ç½®")
            embedding_cfg = EmbeddingConfig(
                provider=embedding_config["provider"],
                model_name=embedding_config["model_name"],
                api_key=embedding_config.get("api_key"),
                api_base=embedding_config.get("api_base"),
                api_version=embedding_config.get("api_version"),
                deployment_name=embedding_config.get("deployment_name"),
                extra_params=embedding_config.get("extra_params") or {}
            )
            self._update_task_progress(task_instance, 10, "é…ç½®åŠ è½½å®Œæˆ")

            # åŠ è½½ embedding æ¨¡å‹
            logger.info(f"ğŸ¤– [æ¨¡å‹åŠ è½½] ä¼šè¯ID: {session_id} - æ­£åœ¨åŠ è½½ {embedding_cfg.provider}/{embedding_cfg.model_name} æ¨¡å‹")
            embedding_model = EmbeddingManager.get_embedding_model(embedding_cfg)
            logger.info(f"âœ… [æ¨¡å‹å°±ç»ª] ä¼šè¯ID: {session_id} - Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ")
            self._update_task_progress(task_instance, 15, "Embeddingæ¨¡å‹åŠ è½½å®Œæˆ")

            # åˆ›å»ºå‘é‡æ•°æ®åº“é›†åˆ
            logger.info(f"ğŸ—„ï¸ [æ•°æ®åº“] ä¼šè¯ID: {session_id} - åˆ›å»ºå‘é‡æ•°æ®åº“é›†åˆ")
            logger.info(f"ğŸ”§ [è°ƒè¯•] ä¼šè¯ID: {session_id} - embedding_model ç±»å‹: {type(embedding_model)}, å€¼: {embedding_model}")
            
            # å…ˆæµ‹è¯• ChromaDB è¿æ¥
            try:
                vector_store = get_vector_store()
                health_status = vector_store.health_check()
                logger.info(f"ğŸ¥ [å¥åº·æ£€æŸ¥] ä¼šè¯ID: {session_id} - ChromaDB çŠ¶æ€: {health_status}")
            except Exception as health_e:
                logger.error(f"âŒ [å¥åº·æ£€æŸ¥å¤±è´¥] ä¼šè¯ID: {session_id} - ChromaDB è¿æ¥å¼‚å¸¸: {str(health_e)}")
                raise Exception(f"ChromaDB è¿æ¥å¤±è´¥: {str(health_e)}")
            
            # åˆ›å»ºé›†åˆ
            logger.info(f"ğŸ”„ [å¼€å§‹åˆ›å»º] ä¼šè¯ID: {session_id} - æ­£åœ¨è°ƒç”¨ create_collection...")
            if not vector_store.create_collection(session_id, embedding_model):
                raise Exception("åˆ›å»ºå‘é‡æ•°æ®åº“é›†åˆå¤±è´¥")
            logger.info(f"âœ… [æ•°æ®åº“å°±ç»ª] ä¼šè¯ID: {session_id} - å‘é‡æ•°æ®åº“é›†åˆåˆ›å»ºæˆåŠŸ")
            self._update_task_progress(task_instance, 20, "å‘é‡æ•°æ®åº“é›†åˆåˆ›å»ºå®Œæˆ")

            # å…‹éš†ä»“åº“å¹¶å¤„ç†
            logger.info(f"ğŸ“¥ [ä»“åº“å…‹éš†] ä¼šè¯ID: {session_id} - å¼€å§‹å…‹éš†ä»“åº“: {repo_url}")
            repo_path = self.git_helper.clone_repository(repo_url)
            logger.info(f"âœ… [å…‹éš†å®Œæˆ] ä¼šè¯ID: {session_id} - ä»“åº“å…‹éš†åˆ°: {repo_path}")
            self._update_task_progress(task_instance, 30, "ä»“åº“å…‹éš†å®Œæˆ")

            # è·å–ä»“åº“ä¿¡æ¯
            logger.info(f"ğŸ“‹ [ä»“åº“ä¿¡æ¯] ä¼šè¯ID: {session_id} - è§£æä»“åº“ä¿¡æ¯")
            repo_info = self.git_helper.get_repository_info(repo_path)
            owner, repo_name = self.git_helper.extract_repo_info(repo_url)
            logger.info(f"ğŸ“ [ä»“åº“è¯¦æƒ…] ä¼šè¯ID: {session_id} - ä»“åº“: {owner}/{repo_name}")

            # æ›´æ–°ä¼šè¯ä¿¡æ¯
            self._update_session_repo_info(db, session_id, repo_name, owner)
            self._update_task_progress(task_instance, 35, "ä»“åº“ä¿¡æ¯è§£æå®Œæˆ")

            # æ‰«æå’Œå¤„ç†æ–‡ä»¶ï¼Œè·å–æ‰€æœ‰å¾…å¤„ç†çš„æ–‡æ¡£
            logger.info(f"ğŸ“ [æ–‡ä»¶æ‰«æ] ä¼šè¯ID: {session_id} - å¼€å§‹æ‰«æå’Œå¤„ç†ä»“åº“æ–‡ä»¶")
            processed_files, total_chunks, all_documents = self._process_repository_files(
                db, session_id, repo_path, task_instance
            )
            logger.info(f"ğŸ“Š [æ‰«æç»“æœ] ä¼šè¯ID: {session_id} - å¤„ç†æ–‡ä»¶: {processed_files}, ç”Ÿæˆå—: {total_chunks}")
            self._update_task_progress(task_instance, 70, f"æ–‡ä»¶å¤„ç†å®Œæˆ: {processed_files}ä¸ªæ–‡ä»¶, {total_chunks}ä¸ªå—")

            # å‘é‡åŒ–å’Œå­˜å‚¨æ–‡æ¡£
            if all_documents:
                logger.info(f"ğŸ”„ [å‘é‡åŒ–] ä¼šè¯ID: {session_id} - å¼€å§‹å‘é‡åŒ– {len(all_documents)} ä¸ªæ–‡æ¡£å—")
                self._vectorize_and_store_documents(db, session_id, all_documents, embedding_model, task_instance)
                logger.info(f"âœ… [å‘é‡åŒ–å®Œæˆ] ä¼šè¯ID: {session_id} - æ‰€æœ‰æ–‡æ¡£å‘é‡åŒ–å¹¶å­˜å‚¨å®Œæˆ")
            else:
                logger.warning(f"âš ï¸ [æ— æ–‡æ¡£] ä¼šè¯ID: {session_id} - ä»“åº“æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡æ¡£å—")
            self._update_task_progress(task_instance, 95, "å‘é‡åŒ–å’Œå­˜å‚¨å®Œæˆ")

            # æ ‡è®°ä»»åŠ¡å®Œæˆ
            logger.info(f"ğŸ [ä»»åŠ¡å®Œæˆ] ä¼šè¯ID: {session_id} - æ ‡è®°ä»»åŠ¡ä¸ºæˆåŠŸçŠ¶æ€")
            self._update_session_status(
                db, session_id, TaskStatus.SUCCESS,
                completed_at=datetime.now(timezone.utc)
            )
            self._update_task_progress(task_instance, 100, "ä»»åŠ¡å®Œæˆ")

            logger.info(f"ğŸ‰ [å¤„ç†æˆåŠŸ] ä¼šè¯ID: {session_id} - ä»“åº“ {repo_url} åˆ†æå®Œæˆ")
            return True

        except Exception as e:
            error_msg = str(e)
            logger.error(f"å¤„ç†ä»“åº“å¤±è´¥ {repo_url}: {error_msg}")

            # æ ‡è®°ä»»åŠ¡å¤±è´¥
            self._update_session_status(
                db, session_id, TaskStatus.FAILED,
                error_message=error_msg,
                completed_at=datetime.now(timezone.utc)
            )

            return False

        finally:
            if db:
                db.close()

    def _process_repository_files(
            self,
            db: Session,
            session_id: str,
            repo_path: str,
            task_instance=None
    ) -> Tuple[int, int, List[Document]]:
        """
        å¤„ç†ä»“åº“ä¸­çš„æ‰€æœ‰æ–‡ä»¶

        Args:
            db: æ•°æ®åº“ä¼šè¯
            session_id: ä¼šè¯ ID
            repo_path: ä»“åº“è·¯å¾„

        Returns:
            Tuple[int, int, List[Document]]: (å¤„ç†çš„æ–‡ä»¶æ•°, æ€»å—æ•°, æ‰€æœ‰æ–‡æ¡£å—)
        """
        total_files = 0
        total_chunks = 0
        processed_files = 0

        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£å—å’Œå…ƒæ•°æ®
        all_documents = []
        all_file_metadata = []

        # æ‰«æä»“åº“æ–‡ä»¶
        logger.info(f"ğŸ” [æ–‡ä»¶æ‰«æ] ä¼šè¯ID: {session_id} - å¼€å§‹æ‰«æä»“åº“æ–‡ä»¶")
        files_to_process = list(self.file_parser.scan_repository(repo_path))
        total_files = len(files_to_process)
        logger.info(f"ğŸ“‹ [æ‰«æå®Œæˆ] ä¼šè¯ID: {session_id} - å‘ç° {total_files} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
        self._update_session_stats(db, session_id, total_files=total_files)  # åˆå§‹æ›´æ–°æ€»æ–‡ä»¶æ•°

        for file_index, (file_path, file_info) in enumerate(files_to_process, 1):
            # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶è·¯å¾„å˜é‡å
            relative_file_path = file_info["file_path"]
            
            # åˆ›å»ºæ–‡ä»¶å…ƒæ•°æ®è®°å½•
            file_metadata = FileMetadata(
                session_id=session_id,
                file_path=relative_file_path,
                file_type=file_info["file_type"],
                file_extension=file_info.get("file_extension"),
                file_size=file_info["file_size"],
                is_processed="pending"
            )

            try:
                # æ˜¾ç¤ºå½“å‰å¤„ç†è¿›åº¦
                if file_index % 10 == 1 or file_index <= 5:  # å‰5ä¸ªæ–‡ä»¶å’Œæ¯10ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡
                    logger.info(f"ğŸ“„ [æ–‡ä»¶å¤„ç†] ä¼šè¯ID: {session_id} - å¤„ç†ç¬¬ {file_index}/{total_files} ä¸ªæ–‡ä»¶: {relative_file_path}")
                
                # æ›´æ–°ä»»åŠ¡è¿›åº¦ (35% åˆ° 70% ä¹‹é—´)
                progress = 35 + int((file_index / total_files) * 35)
                self._update_task_progress(task_instance, progress, f"å¤„ç†æ–‡ä»¶ {file_index}/{total_files}: {relative_file_path}")

                # è¯»å–æ–‡ä»¶å†…å®¹
                content = self.file_parser.read_file_content(file_path)
                if not content:
                    file_metadata.is_processed = "skipped"
                    file_metadata.error_message = "æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹æˆ–æ–‡ä»¶ä¸ºç©º"
                    logger.debug(f"â­ï¸ [è·³è¿‡æ–‡ä»¶] ä¼šè¯ID: {session_id} - æ–‡ä»¶ä¸ºç©º: {relative_file_path}")
                    continue

                # è®¡ç®—è¡Œæ•°
                file_metadata.line_count = len(content.split('\n'))
                logger.debug(f"ğŸ“Š [æ–‡ä»¶ä¿¡æ¯] ä¼šè¯ID: {session_id} - {relative_file_path}: {file_metadata.line_count} è¡Œ, {file_info['file_size']} å­—èŠ‚")

                # è§£æç‰¹æ®Šæ–‡ä»¶
                if file_info["file_type"] in ["config", "document"]:
                    special_info = self.file_parser.parse_special_files(file_path, content)
                    if special_info.get("type") != "unknown":
                        file_metadata.content_summary = f"{special_info.get('type', '')} æ–‡ä»¶"
                        if "dependencies" in special_info:
                            file_metadata.dependencies = special_info["dependencies"]
                        logger.debug(f"ğŸ”§ [ç‰¹æ®Šæ–‡ä»¶] ä¼šè¯ID: {session_id} - {relative_file_path}: {special_info.get('type', '')}")

                # åˆ†å‰²æ–‡æ¡£
                documents = self.file_parser.split_file_content(
                    content,
                    relative_file_path,
                    language=None
                )

                if documents:
                    # ä¸ºæ¯ä¸ªæ–‡æ¡£å—æ·»åŠ å…¨å±€ç´¢å¼•
                    for i, doc in enumerate(documents):
                        doc.metadata['chunk_index'] = total_chunks + i

                    all_documents.extend(documents)
                    file_metadata.chunk_count = len(documents)
                    total_chunks += len(documents)
                    file_metadata.is_processed = "success"
                    processed_files += 1
                    
                    if len(documents) > 1:
                        logger.debug(f"âœ‚ï¸ [æ–‡æ¡£åˆ†å—] ä¼šè¯ID: {session_id} - {relative_file_path}: ç”Ÿæˆ {len(documents)} ä¸ªå—")
                else:
                    file_metadata.is_processed = "skipped"
                    file_metadata.error_message = "æœªç”Ÿæˆæ–‡æ¡£å—"
                    logger.debug(f"âš ï¸ [æ— å—ç”Ÿæˆ] ä¼šè¯ID: {session_id} - {relative_file_path}: æœªç”Ÿæˆæ–‡æ¡£å—")

            except (IOError, UnicodeDecodeError) as e:
                logger.error(f"ğŸ’¥ [è¯»å–å¤±è´¥] ä¼šè¯ID: {session_id} - æ–‡ä»¶ {relative_file_path}: {str(e)}")
                file_metadata.is_processed = "failed"
                file_metadata.error_message = f"æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}"
            except Exception as e:
                logger.error(f"ğŸ’¥ [å¤„ç†å¤±è´¥] ä¼šè¯ID: {session_id} - æ–‡ä»¶ {relative_file_path}: {str(e)}")
                file_metadata.is_processed = "failed"
                file_metadata.error_message = str(e)

            all_file_metadata.append(file_metadata)

            if len(all_file_metadata) % 50 == 0:
                self._update_session_stats(
                    db, session_id, processed_files=processed_files, total_chunks=total_chunks
                )
                logger.info(f"å·²æ‰«æ {len(all_file_metadata)}/{total_files} ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆ {total_chunks} ä¸ªå—")

        # æ‰¹é‡ä¿å­˜æ–‡ä»¶å…ƒæ•°æ®
        try:
            db.add_all(all_file_metadata)
            db.commit()
            logger.info(f"å·²ä¿å­˜ {len(all_file_metadata)} ä¸ªæ–‡ä»¶çš„å…ƒæ•°æ®")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶å…ƒæ•°æ®å¤±è´¥: {str(e)}")
            db.rollback()

        # æ›´æ–°æœ€ç»ˆçš„æ–‡ä»¶å¤„ç†å’Œåˆ†å—ç»Ÿè®¡
        self._update_session_stats(
            db, session_id, processed_files=processed_files, total_chunks=total_chunks
        )
        logger.info(f"æ–‡ä»¶æ‰«æå®Œæˆã€‚æ€»æ–‡ä»¶æ•°: {total_files}, å·²å¤„ç†: {processed_files}, æ€»å—æ•°: {total_chunks}")

        return processed_files, total_chunks, all_documents

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    def _vectorize_and_store_documents(
            self,
            db: Session,
            session_id: str,
            documents: List[Document],
            embedding_model: Embeddings,
            task_instance=None,
            batch_size: int = None
    ):
        """
        å‘é‡åŒ–æ–‡æ¡£å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“

        Args:
            db: æ•°æ®åº“ä¼šè¯
            session_id: ä¼šè¯ ID
            documents: æ–‡æ¡£åˆ—è¡¨
            embedding_model: Embedding æ¨¡å‹
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        batch_size = batch_size or settings.EMBEDDING_BATCH_SIZE
        total_docs = len(documents)
        total_batches = (total_docs + batch_size - 1) // batch_size

        logger.info(f"ğŸ”„ [å‘é‡åŒ–å¼€å§‹] ä¼šè¯ID: {session_id} - å¼€å§‹å‘é‡åŒ– {total_docs} ä¸ªæ–‡æ¡£å—ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        logger.info(f"ğŸ“Š [æ‰¹æ¬¡ä¿¡æ¯] ä¼šè¯ID: {session_id} - æ€»å…±éœ€è¦å¤„ç† {total_batches} ä¸ªæ‰¹æ¬¡")

        for i in range(0, total_docs, batch_size):
            batch_num = i // batch_size + 1
            batch_docs = documents[i:i + batch_size]
            batch_texts = [doc.page_content for doc in batch_docs]
            actual_batch_size = len(batch_docs)

            try:
                logger.info(f"âš¡ [æ‰¹æ¬¡å¤„ç†] ä¼šè¯ID: {session_id} - å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹æ¬¡ ({actual_batch_size} ä¸ªæ–‡æ¡£)")
                
                # æ¸…ç†å’ŒéªŒè¯æ–‡æœ¬
                cleaned_texts = []
                for text in batch_texts:
                    # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
                    if not isinstance(text, str):
                        text = str(text)
                    
                    # è·³è¿‡ç©ºæ–‡æ¡£
                    if not text.strip():
                        continue
                        
                    cleaned_texts.append(text)
                
                if not cleaned_texts:
                    logger.warning(f"âš ï¸ [ç©ºæ‰¹æ¬¡] ä¼šè¯ID: {session_id} - æ‰¹æ¬¡ä¸­æ²¡æœ‰æœ‰æ•ˆæ–‡æ¡£")
                    continue
                
                # å‘é‡åŒ–æ–‡æœ¬
                start_time = time.time()
                logger.debug(f"ğŸ§  [å‘é‡åŒ–ä¸­] ä¼šè¯ID: {session_id} - æ­£åœ¨ç”Ÿæˆå‘é‡...")
                embeddings = embedding_model.embed_documents(cleaned_texts)
                embedding_time = time.time() - start_time
                logger.debug(f"âœ… [å‘é‡ç”Ÿæˆ] ä¼šè¯ID: {session_id} - å‘é‡åŒ–å®Œæˆï¼Œè€—æ—¶ {embedding_time:.2f}s")

                # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
                logger.debug(f"ğŸ’¾ [å­˜å‚¨ä¸­] ä¼šè¯ID: {session_id} - æ­£åœ¨å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
                success = get_vector_store().add_documents_to_collection(
                    session_id, batch_docs, embeddings, len(batch_docs)
                )

                if not success:
                    raise Exception("å‘é‡æ•°æ®åº“å­˜å‚¨å¤±è´¥")
                logger.debug(f"âœ… [å­˜å‚¨å®Œæˆ] ä¼šè¯ID: {session_id} - æ‰¹æ¬¡æ•°æ®å­˜å‚¨æˆåŠŸ")

                # æ›´æ–°è¿›åº¦
                indexed_chunks = min(i + batch_size, total_docs)
                self._update_session_stats(
                    db, session_id, None, None, None, indexed_chunks
                )
                
                # æ›´æ–°ä»»åŠ¡è¿›åº¦ (70% åˆ° 95% ä¹‹é—´)
                progress = 70 + int((batch_num / total_batches) * 25)
                self._update_task_progress(task_instance, progress, f"å‘é‡åŒ–æ‰¹æ¬¡ {batch_num}/{total_batches}")

                logger.info(
                    f"âœ… [æ‰¹æ¬¡å®Œæˆ] ä¼šè¯ID: {session_id} - æ‰¹æ¬¡ {batch_num}/{total_batches} å®Œæˆï¼Œ"
                    f"å‘é‡åŒ–è€—æ—¶ {embedding_time:.2f}sï¼Œå·²å¤„ç† {indexed_chunks}/{total_docs} ä¸ªæ–‡æ¡£"
                )

            except Exception as e:
                logger.error(f"ğŸ’¥ [æ‰¹æ¬¡å¤±è´¥] ä¼šè¯ID: {session_id} - å‘é‡åŒ–æ‰¹æ¬¡ {batch_num} å¤±è´¥ (æ–‡æ¡£ {i}-{i + actual_batch_size}): {str(e)}")
                # é‡è¯•æœºåˆ¶ä¼šè‡ªåŠ¨å¤„ç†
                raise

        logger.info(f"ğŸ‰ [å‘é‡åŒ–å®Œæˆ] ä¼šè¯ID: {session_id} - æ‰€æœ‰æ–‡æ¡£å‘é‡åŒ–å®Œæˆï¼Œå…±å¤„ç† {total_docs} ä¸ªæ–‡æ¡£å—")

    def _update_session_status(
            self,
            db: Session,
            session_id: str,
            status: TaskStatus,
            error_message: str = None,
            started_at: datetime = None,
            completed_at: datetime = None
    ):
        """æ›´æ–°ä¼šè¯çŠ¶æ€"""
        try:
            session = db.query(AnalysisSession).filter(
                AnalysisSession.session_id == session_id
            ).first()

            if session:
                session.status = status
                if error_message:
                    session.error_message = error_message
                if started_at:
                    session.started_at = started_at
                if completed_at:
                    session.completed_at = completed_at

                db.commit()

        except Exception as e:
            logger.error(f"æ›´æ–°ä¼šè¯çŠ¶æ€å¤±è´¥: {str(e)}")
            db.rollback()

    def _update_session_repo_info(
            self,
            db: Session,
            session_id: str,
            repo_name: str,
            repo_owner: str
    ):
        """æ›´æ–°ä¼šè¯ä»“åº“ä¿¡æ¯"""
        try:
            session = db.query(AnalysisSession).filter(
                AnalysisSession.session_id == session_id
            ).first()

            if session:
                session.repository_name = repo_name
                session.repository_owner = repo_owner
                db.commit()

        except Exception as e:
            logger.error(f"æ›´æ–°ä¼šè¯ä»“åº“ä¿¡æ¯å¤±è´¥: {str(e)}")
            db.rollback()

    def _update_session_stats(
            self,
            db: Session,
            session_id: str,
            total_files: Optional[int] = None,
            processed_files: Optional[int] = None,
            total_chunks: Optional[int] = None,
            indexed_chunks: Optional[int] = None
    ):
        """æ›´æ–°ä¼šè¯ç»Ÿè®¡ä¿¡æ¯"""
        try:
            session = db.query(AnalysisSession).filter(
                AnalysisSession.session_id == session_id
            ).first()

            if session:
                if total_files is not None:
                    session.total_files = total_files
                if processed_files is not None:
                    session.processed_files = processed_files
                if total_chunks is not None:
                    session.total_chunks = total_chunks
                if indexed_chunks is not None:
                    session.indexed_chunks = indexed_chunks

                db.commit()

        except Exception as e:
            logger.error(f"æ›´æ–°ä¼šè¯ç»Ÿè®¡å¤±è´¥: {str(e)}")
            db.rollback()

    def _update_task_progress(self, task_instance, progress: int, status: str):
        """æ›´æ–°Celeryä»»åŠ¡è¿›åº¦"""
        if task_instance:
            try:
                task_instance.update_state(
                    state='PROGRESS',
                    meta={
                        'current': progress,
                        'total': 100,
                        'status': status
                    }
                )
            except Exception as e:
                logger.debug(f"æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {str(e)}")


# å…¨å±€æœåŠ¡å®ä¾‹
ingestion_service = IngestionService()