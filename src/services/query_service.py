"""
æŸ¥è¯¢æœåŠ¡ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜
å®ç°æ··åˆæ£€ç´¢ï¼ˆå‘é‡æ£€ç´¢ + BM25 å…³é”®è¯æ£€ç´¢ï¼‰å’Œé‡æ’åº
ç„¶åæ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æœ¬å¿«å—ç”Ÿæˆç›¸åº”çš„å›ç­”
"""

import time
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from sqlalchemy.orm import Session
from rank_bm25 import BM25Okapi

from ..core.config import settings
from ..db.session import get_db_session
from ..db.models import AnalysisSession, QueryLog, TaskStatus
from ..services.embedding_manager import EmbeddingManager, EmbeddingConfig
from ..services.llm_manager import LLMManager, LLMConfig
from ..services.vector_store import get_vector_store
from ..schemas.repository import (
    QueryRequest, QueryResponse, RetrievedChunk,
    GenerationMode, LLMConfig as LLMConfigSchema
)

logger = logging.getLogger(__name__)


class QueryService:
    """æŸ¥è¯¢æœåŠ¡"""

    def __init__(self):
        self._bm25_cache = {}  # ç¼“å­˜ BM25 ç´¢å¼•
        self._documents_cache = {}  # ç¼“å­˜æ–‡æ¡£å†…å®¹

    def query(self, request: QueryRequest) -> QueryResponse:
        """
        å¤„ç†æŸ¥è¯¢è¯·æ±‚

        Args:
            request: æŸ¥è¯¢è¯·æ±‚

        Returns:
            QueryResponse: æŸ¥è¯¢å“åº”
        """
        start_time = time.time()
        db = get_db_session()

        try:
            # éªŒè¯ä¼šè¯
            session = self._validate_session(db, request.session_id)
            if not session:
                return QueryResponse(
                    answer="ä¼šè¯ä¸å­˜åœ¨æˆ–åˆ†ææœªå®Œæˆ",
                    generation_mode=request.generation_mode
                )

            logger.info(f"ğŸš€ [æŸ¥è¯¢å¼€å§‹] ä¼šè¯ID: {request.session_id} - é—®é¢˜: {request.question[:100]}{'...' if len(request.question) > 100 else ''}")
            logger.info(f"âš™ï¸ [æŸ¥è¯¢é…ç½®] ä¼šè¯ID: {request.session_id} - ç”Ÿæˆæ¨¡å¼: {request.generation_mode}")
            
            # æ‰§è¡Œæ··åˆæ£€ç´¢
            logger.info(f"ğŸ” [æ£€ç´¢é˜¶æ®µ] ä¼šè¯ID: {request.session_id} - å¼€å§‹æ‰§è¡Œæ··åˆæ£€ç´¢")
            retrieval_start = time.time()
            retrieved_chunks = self._hybrid_retrieval(
                session.session_id,
                session.embedding_config,
                request.question
            )
            retrieval_time = int((time.time() - retrieval_start) * 1000)
            logger.info(f"âœ… [æ£€ç´¢å®Œæˆ] ä¼šè¯ID: {request.session_id} - æ£€ç´¢è€—æ—¶: {retrieval_time}ms, è·å¾— {len(retrieved_chunks)} ä¸ªä¸Šä¸‹æ–‡")

            # å‡†å¤‡å“åº”
            response = QueryResponse(
                retrieved_context=retrieved_chunks,
                generation_mode=request.generation_mode,
                retrieval_time=retrieval_time
            )

            # æ ¹æ®ç”Ÿæˆæ¨¡å¼å¤„ç†
            if request.generation_mode == GenerationMode.SERVICE and request.llm_config:
                # æœåŠ¡ç«¯ç”Ÿæˆç­”æ¡ˆ
                logger.info(f"ğŸ¤– [ç”Ÿæˆé˜¶æ®µ] ä¼šè¯ID: {request.session_id} - å¼€å§‹ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ")
                generation_start = time.time()
                answer = self._generate_answer(
                    request.question,
                    retrieved_chunks,
                    request.llm_config
                )
                generation_time = int((time.time() - generation_start) * 1000)
                logger.info(f"âœ… [ç”Ÿæˆå®Œæˆ] ä¼šè¯ID: {request.session_id} - ç”Ÿæˆè€—æ—¶: {generation_time}ms, ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")

                response.answer = answer
                response.generation_time = generation_time
            else:
                logger.info(f"ğŸ“¤ [æ’ä»¶æ¨¡å¼] ä¼šè¯ID: {request.session_id} - ä»…è¿”å›æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œä¸ç”Ÿæˆç­”æ¡ˆ")

            response.total_time = int((time.time() - start_time) * 1000)
            logger.info(f"ğŸ‰ [æŸ¥è¯¢å®Œæˆ] ä¼šè¯ID: {request.session_id} - æ€»è€—æ—¶: {response.total_time}ms")

            # è®°å½•æŸ¥è¯¢æ—¥å¿—
            self._log_query(
                db, request, response, retrieved_chunks
            )

            return response

        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}")
            return QueryResponse(
                answer=f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}",
                generation_mode=request.generation_mode,
                total_time=int((time.time() - start_time) * 1000)
            )

        finally:
            if db:
                db.close()

    def _validate_session(self, db: Session, session_id: str) -> Optional[AnalysisSession]:
        """
        éªŒè¯ä¼šè¯çŠ¶æ€

        Args:
            db: æ•°æ®åº“ä¼šè¯
            session_id: ä¼šè¯ ID

        Returns:
            Optional[AnalysisSession]: ä¼šè¯å¯¹è±¡æˆ– None
        """
        session = db.query(AnalysisSession).filter(
            AnalysisSession.session_id == session_id
        ).first()

        if not session:
            logger.warning(f"ä¼šè¯ä¸å­˜åœ¨: {session_id}")
            return None

        if session.status != TaskStatus.SUCCESS:
            logger.warning(f"ä¼šè¯åˆ†ææœªå®Œæˆ: {session_id}, çŠ¶æ€: {session.status}")
            return None

        return session

    def _hybrid_retrieval(
            self,
            session_id: str,
            embedding_config: Dict[str, Any],
            question: str
    ) -> List[RetrievedChunk]:
        """
        æ··åˆæ£€ç´¢ï¼šå‘é‡æ£€ç´¢ + BM25 å…³é”®è¯æ£€ç´¢

        Args:
            session_id: ä¼šè¯ ID
            embedding_config: Embedding é…ç½®
            question: ç”¨æˆ·é—®é¢˜

        Returns:
            List[RetrievedChunk]: æ£€ç´¢ç»“æœ
        """
        logger.info(f"ğŸ” [æ··åˆæ£€ç´¢å¼€å§‹] ä¼šè¯ID: {session_id} - å¼€å§‹æ‰§è¡Œæ··åˆæ£€ç´¢ç­–ç•¥")
        
        # 1. å‘é‡æ£€ç´¢
        logger.info(f"ğŸ“Š [æ­¥éª¤1/4] ä¼šè¯ID: {session_id} - æ‰§è¡Œå‘é‡æ£€ç´¢")
        vector_results = self._vector_search(session_id, embedding_config, question)

        # 2. BM25 å…³é”®è¯æ£€ç´¢
        logger.info(f"ğŸ“Š [æ­¥éª¤2/4] ä¼šè¯ID: {session_id} - æ‰§è¡ŒBM25å…³é”®è¯æ£€ç´¢")
        bm25_results = self._bm25_search(session_id, question)

        # 3. RRF èåˆ
        logger.info(f"ğŸ“Š [æ­¥éª¤3/4] ä¼šè¯ID: {session_id} - æ‰§è¡ŒRRFèåˆç®—æ³•")
        final_results = self._reciprocal_rank_fusion(vector_results, bm25_results)

        # 4. å–å‰ N ä¸ªç»“æœ
        logger.info(f"ğŸ“Š [æ­¥éª¤4/4] ä¼šè¯ID: {session_id} - ç­›é€‰æœ€ç»ˆç»“æœ")
        top_results = final_results[:settings.FINAL_CONTEXT_TOP_K]
        
        logger.info(f"âœ… [æ··åˆæ£€ç´¢å®Œæˆ] ä¼šè¯ID: {session_id} - æœ€ç»ˆè¿”å› {len(top_results)} ä¸ªä¸Šä¸‹æ–‡å—")
        
        # è®°å½•æœ€ç»ˆç»“æœçš„ç»Ÿè®¡ä¿¡æ¯
        if top_results:
            total_chars = sum(len(chunk.content) for chunk in top_results)
            avg_score = sum(chunk.score for chunk in top_results) / len(top_results)
            logger.info(f"ğŸ“ˆ [ç»“æœç»Ÿè®¡] ä¼šè¯ID: {session_id} - æ€»å­—ç¬¦æ•°: {total_chars}, å¹³å‡åˆ†æ•°: {avg_score:.4f}")
        
        return top_results

    def _vector_search(
            self,
            session_id: str,
            embedding_config: Dict[str, Any],
            question: str
    ) -> List[Tuple[str, float, Dict[str, Any]]]:
        """
        å‘é‡æ£€ç´¢

        Args:
            session_id: ä¼šè¯ ID
            embedding_config: Embedding é…ç½®
            question: ç”¨æˆ·é—®é¢˜

        Returns:
            List[Tuple[str, float, Dict[str, Any]]]: (æ–‡æ¡£ID, åˆ†æ•°, å…ƒæ•°æ®)
        """
        try:
            logger.info(f"ğŸ” [å‘é‡æ£€ç´¢] ä¼šè¯ID: {session_id} - å¼€å§‹å‘é‡æ£€ç´¢ï¼Œé—®é¢˜é•¿åº¦: {len(question)} å­—ç¬¦")
            
            # åˆ›å»º embedding é…ç½®å¯¹è±¡
            embedding_cfg = EmbeddingConfig(
                provider=embedding_config["provider"],
                model_name=embedding_config["model_name"],
                api_key=embedding_config.get("api_key"),
                api_base=embedding_config.get("api_base"),
                api_version=embedding_config.get("api_version"),
                deployment_name=embedding_config.get("deployment_name"),
                extra_params=embedding_config.get("extra_params", {})
            )
            logger.debug(f"ğŸ¤– [æ¨¡å‹é…ç½®] ä¼šè¯ID: {session_id} - ä½¿ç”¨ {embedding_cfg.provider}/{embedding_cfg.model_name} æ¨¡å‹")

            # åŠ è½½ embedding æ¨¡å‹
            logger.debug(f"âš¡ [æ¨¡å‹åŠ è½½] ä¼šè¯ID: {session_id} - æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...")
            embedding_model = EmbeddingManager.get_embedding_model(embedding_cfg)
            logger.debug(f"âœ… [æ¨¡å‹å°±ç»ª] ä¼šè¯ID: {session_id} - Embedding æ¨¡å‹åŠ è½½å®Œæˆ")

            # å‘é‡åŒ–é—®é¢˜
            logger.debug(f"ğŸ§  [é—®é¢˜å‘é‡åŒ–] ä¼šè¯ID: {session_id} - æ­£åœ¨å°†é—®é¢˜è½¬æ¢ä¸ºå‘é‡...")
            question_embedding = embedding_model.embed_query(question)
            logger.debug(f"âœ… [å‘é‡ç”Ÿæˆ] ä¼šè¯ID: {session_id} - é—®é¢˜å‘é‡åŒ–å®Œæˆï¼Œç»´åº¦: {len(question_embedding)}")

            # åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
            logger.debug(f"ğŸ” [æ•°æ®åº“æ£€ç´¢] ä¼šè¯ID: {session_id} - æ­£åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ç›¸ä¼¼æ–‡æ¡£...")
            results = get_vector_store().query_collection(
                collection_name=session_id,
                query_embedding=question_embedding,
                n_results=settings.VECTOR_SEARCH_TOP_K
            )
            logger.debug(f"ğŸ“Š [æ£€ç´¢ç»“æœ] ä¼šè¯ID: {session_id} - å‘é‡æ•°æ®åº“è¿”å›ç»“æœ")

            # è½¬æ¢ç»“æœæ ¼å¼
            vector_results = []
            if results["ids"] and results["ids"][0]:
                logger.info(f"âœ… [æ£€ç´¢æˆåŠŸ] ä¼šè¯ID: {session_id} - æ‰¾åˆ° {len(results['ids'][0])} ä¸ªç›¸ä¼¼æ–‡æ¡£")
                for i, doc_id in enumerate(results["ids"][0]):
                    distance = results["distances"][0][i]
                    # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè·ç¦»è¶Šå°ï¼Œåˆ†æ•°è¶Šé«˜ï¼‰
                    score = 1.0 / (1.0 + distance)
                    metadata = results["metadatas"][0][i]
                    vector_results.append((doc_id, score, metadata))
                    
                    if i < 3:  # åªè®°å½•å‰3ä¸ªç»“æœçš„è¯¦ç»†ä¿¡æ¯
                        file_path = metadata.get('file_path', 'unknown')
                        logger.debug(f"ğŸ“„ [ç›¸ä¼¼æ–‡æ¡£] æ’å{i+1}: {file_path}, è·ç¦»: {distance:.4f}, åˆ†æ•°: {score:.4f}")
            else:
                logger.warning(f"âš ï¸ [æ— ç»“æœ] ä¼šè¯ID: {session_id} - å‘é‡æ£€ç´¢æœªæ‰¾åˆ°ç›¸ä¼¼æ–‡æ¡£")

            logger.info(f"ğŸ¯ [å‘é‡æ£€ç´¢å®Œæˆ] ä¼šè¯ID: {session_id} - è¿”å› {len(vector_results)} ä¸ªç»“æœ")
            return vector_results

        except Exception as e:
            logger.error(f"âŒ [å‘é‡æ£€ç´¢å¤±è´¥] ä¼šè¯ID: {session_id} - {str(e)}")
            return []

    def _bm25_search(
            self,
            session_id: str,
            question: str
    ) -> List[Tuple[str, float, Dict[str, Any]]]:
        """
        BM25 å…³é”®è¯æ£€ç´¢

        Args:
            session_id: ä¼šè¯ ID
            question: ç”¨æˆ·é—®é¢˜

        Returns:
            List[Tuple[str, float, Dict[str, Any]]]: (æ–‡æ¡£ID, åˆ†æ•°, å…ƒæ•°æ®)
        """
        try:
            logger.info(f"ğŸ”¤ [BM25æ£€ç´¢] ä¼šè¯ID: {session_id} - å¼€å§‹å…³é”®è¯æ£€ç´¢")
            
            # è·å–æˆ–æ„å»º BM25 ç´¢å¼•
            bm25_index = self._get_bm25_index(session_id)
            if not bm25_index:
                logger.warning(f"âš ï¸ [ç´¢å¼•ç¼ºå¤±] ä¼šè¯ID: {session_id} - BM25ç´¢å¼•ä¸å­˜åœ¨")
                return []

            # åˆ†è¯ï¼ˆç®€å•ç©ºæ ¼åˆ†å‰²ï¼‰
            query_tokens = question.lower().split()
            logger.debug(f"ğŸ“ [åˆ†è¯ç»“æœ] ä¼šè¯ID: {session_id} - æŸ¥è¯¢è¯: {query_tokens}")

            # BM25 æœç´¢
            logger.debug(f"ğŸ” [BM25è®¡ç®—] ä¼šè¯ID: {session_id} - æ­£åœ¨è®¡ç®—BM25åˆ†æ•°...")
            doc_scores = bm25_index.get_scores(query_tokens)

            # è·å–æ–‡æ¡£ä¿¡æ¯
            documents = self._documents_cache.get(session_id, [])
            logger.debug(f"ğŸ“š [æ–‡æ¡£ç¼“å­˜] ä¼šè¯ID: {session_id} - ç¼“å­˜ä¸­æœ‰ {len(documents)} ä¸ªæ–‡æ¡£")

            # æ’åºå¹¶å–å‰ N ä¸ª
            scored_docs = [
                (documents[i]["id"], score, documents[i]["metadata"])
                for i, score in enumerate(doc_scores)
                if score > 0
            ]
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            
            top_results = scored_docs[:settings.BM25_SEARCH_TOP_K]
            logger.info(f"âœ… [BM25å®Œæˆ] ä¼šè¯ID: {session_id} - æ‰¾åˆ° {len([s for s in doc_scores if s > 0])} ä¸ªåŒ¹é…æ–‡æ¡£ï¼Œè¿”å›å‰ {len(top_results)} ä¸ª")
            
            # è®°å½•å‰å‡ ä¸ªç»“æœçš„è¯¦ç»†ä¿¡æ¯
            for i, (doc_id, score, metadata) in enumerate(top_results[:3]):
                file_path = metadata.get('file_path', 'unknown')
                logger.debug(f"ğŸ“„ [BM25ç»“æœ] æ’å{i+1}: {file_path}, BM25åˆ†æ•°: {score:.4f}")

            return top_results

        except Exception as e:
            logger.error(f"âŒ [BM25æ£€ç´¢å¤±è´¥] ä¼šè¯ID: {session_id} - {str(e)}")
            return []

    def _get_bm25_index(self, session_id: str):
        """
        è·å–æˆ–æ„å»º BM25 ç´¢å¼•

        Args:
            session_id: ä¼šè¯ ID

        Returns:
            BM25Okapi ç´¢å¼•æˆ– None
        """
        # æ£€æŸ¥ç¼“å­˜
        if session_id in self._bm25_cache:
            return self._bm25_cache[session_id]

        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£
            documents = get_vector_store().get_all_documents_from_collection(session_id)
            if not documents:
                return None

            # å‡†å¤‡æ–‡æ¡£æ–‡æœ¬
            doc_texts = []
            for doc in documents:
                # ä½¿ç”¨å…ƒæ•°æ®ä¸­çš„å†…å®¹
                content = doc["metadata"].get("content", doc["content"])
                doc_texts.append(content.lower().split())

            # æ„å»º BM25 ç´¢å¼•
            bm25_index = BM25Okapi(doc_texts)

            # ç¼“å­˜ç´¢å¼•å’Œæ–‡æ¡£
            self._bm25_cache[session_id] = bm25_index
            self._documents_cache[session_id] = documents

            logger.info(f"ä¸ºä¼šè¯ {session_id} æ„å»ºäº† BM25 ç´¢å¼•ï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£")
            return bm25_index

        except Exception as e:
            logger.error(f"æ„å»º BM25 ç´¢å¼•å¤±è´¥: {str(e)}")
            return None

    def _reciprocal_rank_fusion(
            self,
            vector_results: List[Tuple[str, float, Dict[str, Any]]],
            bm25_results: List[Tuple[str, float, Dict[str, Any]]],
            k: int = 60
    ) -> List[RetrievedChunk]:
        """
        RRF (Reciprocal Rank Fusion) ç®—æ³•èåˆä¸¤ä¸ªæ£€ç´¢ç»“æœ

        Args:
            vector_results: å‘é‡æ£€ç´¢ç»“æœ
            bm25_results: BM25 æ£€ç´¢ç»“æœ
            k: RRF å‚æ•°

        Returns:
            List[RetrievedChunk]: èåˆåçš„ç»“æœ
        """
        logger.info(f"ğŸ”€ [RRFèåˆ] å¼€å§‹èåˆæ£€ç´¢ç»“æœ - å‘é‡ç»“æœ: {len(vector_results)} ä¸ª, BM25ç»“æœ: {len(bm25_results)} ä¸ª")
        
        # åˆ›å»ºæ–‡æ¡£ ID åˆ°ä¿¡æ¯çš„æ˜ å°„
        doc_info = {}

        # å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
        logger.debug(f"ğŸ“Š [å¤„ç†å‘é‡ç»“æœ] æ­£åœ¨å¤„ç† {len(vector_results)} ä¸ªå‘é‡æ£€ç´¢ç»“æœ...")
        for rank, (doc_id, score, metadata) in enumerate(vector_results):
            if doc_id not in doc_info:
                doc_info[doc_id] = {
                    "metadata": metadata,
                    "content": metadata.get("content", ""),
                    "vector_rank": rank + 1,
                    "bm25_rank": None,
                    "rrf_score": 0.0
                }
            rrf_contribution = 1.0 / (k + rank + 1)
            doc_info[doc_id]["rrf_score"] += rrf_contribution
            
            if rank < 3:  # è®°å½•å‰3ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                file_path = metadata.get('file_path', 'unknown')
                logger.debug(f"ğŸ“„ [å‘é‡è´¡çŒ®] {file_path} - æ’å: {rank+1}, RRFè´¡çŒ®: {rrf_contribution:.4f}")

        # å¤„ç† BM25 æ£€ç´¢ç»“æœ
        logger.debug(f"ğŸ“Š [å¤„ç†BM25ç»“æœ] æ­£åœ¨å¤„ç† {len(bm25_results)} ä¸ªBM25æ£€ç´¢ç»“æœ...")
        for rank, (doc_id, score, metadata) in enumerate(bm25_results):
            if doc_id not in doc_info:
                doc_info[doc_id] = {
                    "metadata": metadata,
                    "content": metadata.get("content", ""),
                    "vector_rank": None,
                    "bm25_rank": rank + 1,
                    "rrf_score": 0.0
                }
            else:
                doc_info[doc_id]["bm25_rank"] = rank + 1
            rrf_contribution = 1.0 / (k + rank + 1)
            doc_info[doc_id]["rrf_score"] += rrf_contribution
            
            if rank < 3:  # è®°å½•å‰3ä¸ªçš„è¯¦ç»†ä¿¡æ¯
                file_path = metadata.get('file_path', 'unknown')
                logger.debug(f"ğŸ“„ [BM25è´¡çŒ®] {file_path} - æ’å: {rank+1}, RRFè´¡çŒ®: {rrf_contribution:.4f}")

        # æŒ‰ RRF åˆ†æ•°æ’åº
        logger.debug(f"ğŸ”„ [RRFæ’åº] æ­£åœ¨æŒ‰RRFåˆ†æ•°æ’åº {len(doc_info)} ä¸ªæ–‡æ¡£...")
        sorted_docs = sorted(
            doc_info.items(),
            key=lambda x: x[1]["rrf_score"],
            reverse=True
        )

        # è½¬æ¢ä¸º RetrievedChunk æ ¼å¼
        retrieved_chunks = []
        for i, (doc_id, info) in enumerate(sorted_docs):
            chunk = RetrievedChunk(
                id=doc_id,
                content=info["content"],
                file_path=info["metadata"].get("file_path", ""),
                start_line=info["metadata"].get("start_line"),
                score=info["rrf_score"],
                metadata=info["metadata"]
            )
            retrieved_chunks.append(chunk)
            
            # è®°å½•å‰å‡ ä¸ªæœ€ç»ˆç»“æœçš„è¯¦ç»†ä¿¡æ¯
            if i < 5:
                file_path = info["metadata"].get('file_path', 'unknown')
                vector_rank = info["vector_rank"] or "N/A"
                bm25_rank = info["bm25_rank"] or "N/A"
                logger.debug(f"ğŸ† [æœ€ç»ˆæ’å{i+1}] {file_path} - RRFåˆ†æ•°: {info['rrf_score']:.4f}, å‘é‡æ’å: {vector_rank}, BM25æ’å: {bm25_rank}")

        logger.info(f"âœ… [RRFèåˆå®Œæˆ] èåˆåå…± {len(retrieved_chunks)} ä¸ªç»“æœ")
        return retrieved_chunks

    def _generate_answer(
            self,
            question: str,
            retrieved_chunks: List[RetrievedChunk],
            llm_config: LLMConfigSchema
    ) -> str:
        """
        ä½¿ç”¨ LLM ç”Ÿæˆç­”æ¡ˆ

        Args:
            question: ç”¨æˆ·é—®é¢˜
            retrieved_chunks: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            llm_config: LLM é…ç½®

        Returns:
            str: ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        try:
            logger.info(f"ğŸ¤– [LLMç”Ÿæˆ] å¼€å§‹ç”Ÿæˆç­”æ¡ˆ - æ¨¡å‹: {llm_config.provider}/{llm_config.model_name}")
            logger.info(f"ğŸ“ [ä¸Šä¸‹æ–‡å‡†å¤‡] ä½¿ç”¨ {len(retrieved_chunks)} ä¸ªæ–‡æ¡£å—ä½œä¸ºä¸Šä¸‹æ–‡")
            
            # åˆ›å»º LLM é…ç½®å¯¹è±¡
            logger.debug(f"âš™ï¸ [LLMé…ç½®] æä¾›å•†: {llm_config.provider}, æ¨¡å‹: {llm_config.model_name}, æ¸©åº¦: {llm_config.temperature}, æœ€å¤§ä»¤ç‰Œ: {llm_config.max_tokens}")
            llm_cfg = LLMConfig(
                provider=llm_config.provider.value if hasattr(llm_config.provider, 'value') else llm_config.provider,
                model_name=llm_config.model_name,
                api_key=llm_config.api_key,
                api_base=llm_config.api_base,
                api_version=llm_config.api_version,
                deployment_name=llm_config.deployment_name,
                temperature=llm_config.temperature,
                max_tokens=llm_config.max_tokens,
                **llm_config.extra_params
            )

            # åŠ è½½ LLM æ¨¡å‹
            logger.info(f"ğŸ”§ [æ¨¡å‹åŠ è½½] æ­£åœ¨åŠ è½½LLMæ¨¡å‹...")
            llm = LLMManager.get_llm(llm_cfg)
            logger.info(f"âœ… [æ¨¡å‹å°±ç»ª] LLMæ¨¡å‹åŠ è½½å®Œæˆ")

            # æ„å»º prompt
            logger.info(f"ğŸ“‹ [æ„å»ºPrompt] æ­£åœ¨æ„å»ºä¸Šä¸‹æ–‡å’Œæç¤ºè¯...")
            context = self._build_context(retrieved_chunks)
            prompt = self._build_prompt(question, context)
            
            # è®¡ç®—ä¸Šä¸‹æ–‡ç»Ÿè®¡ä¿¡æ¯
            context_chars = len(context)
            prompt_chars = len(prompt)
            logger.info(f"ğŸ“Š [Promptç»Ÿè®¡] ä¸Šä¸‹æ–‡é•¿åº¦: {context_chars} å­—ç¬¦, å®Œæ•´Prompté•¿åº¦: {prompt_chars} å­—ç¬¦")

            # ç”Ÿæˆç­”æ¡ˆ
            logger.info(f"ğŸš€ [å¼€å§‹ç”Ÿæˆ] æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ...")
            response = llm.invoke(prompt)
            logger.info(f"âœ… [ç”Ÿæˆå®Œæˆ] LLMå“åº”å·²æ¥æ”¶")

            # æå–ç­”æ¡ˆæ–‡æœ¬
            if hasattr(response, 'content'):
                answer = response.content
            else:
                answer = str(response)
            
            logger.info(f"ğŸ“¤ [ç­”æ¡ˆè¾“å‡º] ç”Ÿæˆç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")
            return answer

        except Exception as e:
            logger.error(f"âŒ [ç”Ÿæˆå¤±è´¥] LLMç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {str(e)}"

    def _build_context(self, retrieved_chunks: List[RetrievedChunk]) -> str:
        """
        æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²

        Args:
            retrieved_chunks: æ£€ç´¢åˆ°çš„æ–‡æ¡£å—

        Returns:
            str: æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡
        """
        context_parts = []

        for i, chunk in enumerate(retrieved_chunks):
            context_part = f"[æ–‡æ¡£ {i+1}] æ–‡ä»¶: {chunk.file_path}"
            if chunk.start_line:
                context_part += f" (è¡Œ {chunk.start_line})"
            context_part += f"\n{chunk.content}\n"
            context_parts.append(context_part)

        return "\n".join(context_parts)

    def _build_prompt(self, question: str, context: str) -> str:
        """
        æ„å»º LLM prompt

        Args:
            question: ç”¨æˆ·é—®é¢˜
            context: ä¸Šä¸‹æ–‡

        Returns:
            str: å®Œæ•´çš„ prompt
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åˆ†æåŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„ä»£ç ä»“åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æ ¹æ®ä¸Šè¿°ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚å›ç­”æ—¶è¯·ï¼š
1. æä¾›å‡†ç¡®ã€å…·ä½“çš„ä¿¡æ¯
2. å¼•ç”¨ç›¸å…³çš„æ–‡ä»¶åå’Œè¡Œå·
3. è§£é‡Šä»£ç çš„åŠŸèƒ½å’Œé€»è¾‘
4. å¦‚æœæ¶‰åŠå¤šä¸ªæ–‡ä»¶ï¼Œè¯·è¯´æ˜å®ƒä»¬ä¹‹é—´çš„å…³ç³»

å›ç­”ï¼š"""

        return prompt

    def _log_query(
            self,
            db: Session,
            request: QueryRequest,
            response: QueryResponse,
            retrieved_chunks: List[RetrievedChunk]
    ):
        """
        è®°å½•æŸ¥è¯¢æ—¥å¿—

        Args:
            db: æ•°æ®åº“ä¼šè¯
            request: æŸ¥è¯¢è¯·æ±‚
            response: æŸ¥è¯¢å“åº”
            retrieved_chunks: æ£€ç´¢ç»“æœ
        """
        try:
            query_log = QueryLog(
                session_id=request.session_id,
                question=request.question,
                answer=response.answer,
                retrieved_chunks_count=len(retrieved_chunks),
                generation_mode=request.generation_mode,
                llm_config=request.llm_config.model_dump() if request.llm_config else None,
                retrieval_time=response.retrieval_time,
                generation_time=response.generation_time,
                total_time=response.total_time
            )

            db.add(query_log)
            db.commit()

        except Exception as e:
            logger.error(f"è®°å½•æŸ¥è¯¢æ—¥å¿—å¤±è´¥: {str(e)}")
            db.rollback()


# å…¨å±€æœåŠ¡å®ä¾‹
query_service = QueryService()