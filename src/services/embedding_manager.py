"""
Embedding æ¨¡å‹ç®¡ç†å™¨
è´Ÿè´£æ ¹æ®é…ç½®åŠ¨æ€åŠ è½½å’Œå®ä¾‹åŒ–ä¸åŒçš„ Embedding æ¨¡å‹
æ”¯æŒ OpenAIã€Azureã€HuggingFaceã€Ollamaã€Geminiã€DeepSeekã€åƒé—®ç­‰å¤šç§æä¾›å•†
æä¾›æ‰¹é‡å‘é‡åŒ–ã€é€Ÿç‡é™åˆ¶å¤„ç†ã€å¼‚å¸¸é‡è¯•ç­‰é«˜çº§åŠŸèƒ½
"""

import logging
import time
import asyncio
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass, field
from langchain_openai import OpenAIEmbeddings, AzureOpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.embeddings import Embeddings
from ..core.config import settings
logger = logging.getLogger(__name__)


@dataclass
class EmbeddingConfig:
    """Embedding æ¨¡å‹é…ç½®ç±»"""
    provider: str
    model_name: str
    api_key: Optional[str] = None
    api_base: Optional[str] = None
    api_version: Optional[str] = None
    deployment_name: Optional[str] = None
    batch_size: int = 32
    max_retries: int = 3
    retry_delay: float = 1.0
    timeout: int = 60
    extra_params: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        self.provider = self.provider.lower()

        # éªŒè¯å¿…éœ€å‚æ•°
        if not self.model_name:
            raise ValueError("model_name ä¸èƒ½ä¸ºç©º")

        # æ ¹æ®æä¾›å•†è®¾ç½®é»˜è®¤å€¼
        if self.provider == "azure" and not self.api_version:
            self.api_version = "2024-02-01"

        if self.provider == "ollama" and not self.api_base:
            self.api_base = "http://localhost:11434"

    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> 'EmbeddingConfig':
        """ä»å­—å…¸åˆ›å»ºé…ç½®"""
        # ç¡®ä¿ extra_params ä¸ä¸º None
        if config_dict.get('extra_params') is None:
            config_dict = config_dict.copy()
            config_dict['extra_params'] = {}
        return cls(**config_dict)


class EmbeddingError(Exception):
    """Embedding ç›¸å…³å¼‚å¸¸"""
    pass


class RateLimitError(EmbeddingError):
    """é€Ÿç‡é™åˆ¶å¼‚å¸¸"""
    pass


class APIKeyError(EmbeddingError):
    """APIå¯†é’¥å¼‚å¸¸"""
    pass


class BatchEmbeddingProcessor:
    """æ‰¹é‡å‘é‡åŒ–å¤„ç†å™¨"""

    def __init__(self, embedding_model: Embeddings, config: EmbeddingConfig):
        self.embedding_model = embedding_model
        self.config = config
        self.logger = logging.getLogger(__name__)

    async def embed_documents_with_retry(self, texts: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡å‘é‡åŒ–æ–‡æ¡£ï¼Œæ”¯æŒé‡è¯•å’Œé€Ÿç‡é™åˆ¶å¤„ç†

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            å‘é‡åˆ—è¡¨

        Raises:
            EmbeddingError: å‘é‡åŒ–å¤±è´¥
        """
        if not texts:
            return []

        all_embeddings = []

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(texts), self.config.batch_size):
            batch = texts[i:i + self.config.batch_size]
            batch_embeddings = await self._embed_batch_with_retry(batch)
            all_embeddings.extend(batch_embeddings)

        return all_embeddings

    async def _embed_batch_with_retry(self, batch: List[str]) -> List[List[float]]:
        """
        å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼Œæ”¯æŒé‡è¯•

        Args:
            batch: æ–‡æœ¬æ‰¹æ¬¡

        Returns:
            å‘é‡åˆ—è¡¨
        """
        last_exception = None

        for attempt in range(self.config.max_retries + 1):
            try:
                self.logger.debug(f"å¤„ç†æ‰¹æ¬¡ï¼Œå¤§å°: {len(batch)}, å°è¯•: {attempt + 1}")

                # è°ƒç”¨å®é™…çš„å‘é‡åŒ–
                embeddings = await self._call_embedding_api(batch)

                # éªŒè¯ç»“æœ
                if len(embeddings) != len(batch):
                    raise EmbeddingError(f"è¿”å›çš„å‘é‡æ•°é‡({len(embeddings)})ä¸è¾“å…¥æ–‡æœ¬æ•°é‡({len(batch)})ä¸åŒ¹é…")

                self.logger.debug(f"æ‰¹æ¬¡å¤„ç†æˆåŠŸï¼Œè¿”å› {len(embeddings)} ä¸ªå‘é‡")
                return embeddings

            except Exception as e:
                last_exception = e
                self.logger.warning(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥ (å°è¯• {attempt + 1}/{self.config.max_retries + 1}): {str(e)}")

                # åˆ¤æ–­æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯
                if self._is_rate_limit_error(e):
                    if attempt < self.config.max_retries:
                        # æŒ‡æ•°é€€é¿
                        delay = self.config.retry_delay * (2 ** attempt)
                        self.logger.info(f"é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {delay} ç§’åé‡è¯•")
                        await asyncio.sleep(delay)
                        continue
                    else:
                        raise RateLimitError(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä»é‡åˆ°é€Ÿç‡é™åˆ¶: {str(e)}")

                # åˆ¤æ–­æ˜¯å¦æ˜¯APIå¯†é’¥é”™è¯¯
                if self._is_api_key_error(e):
                    raise APIKeyError(f"APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ: {str(e)}")

                # å…¶ä»–é”™è¯¯ï¼Œå¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šå°±ç»§ç»­
                if attempt < self.config.max_retries:
                    await asyncio.sleep(self.config.retry_delay)
                    continue

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        raise EmbeddingError(f"æ‰¹æ¬¡å¤„ç†æœ€ç»ˆå¤±è´¥: {str(last_exception)}")

    async def _call_embedding_api(self, texts: List[str]) -> List[List[float]]:
        """
        è°ƒç”¨å®é™…çš„å‘é‡åŒ–API

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            å‘é‡åˆ—è¡¨
        """
        try:
            # å¦‚æœæ¨¡å‹æ”¯æŒå¼‚æ­¥ï¼Œä½¿ç”¨å¼‚æ­¥æ–¹æ³•
            if hasattr(self.embedding_model, 'aembed_documents'):
                return await self.embedding_model.aembed_documents(texts)
            else:
                # å¦åˆ™åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥æ–¹æ³•
                import asyncio
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(
                    None,
                    self.embedding_model.embed_documents,
                    texts
                )
        except Exception as e:
            self.logger.error(f"è°ƒç”¨embedding APIå¤±è´¥: {str(e)}")
            raise

    def _is_rate_limit_error(self, error: Exception) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯"""
        error_str = str(error).lower()
        rate_limit_indicators = [
            'rate limit',
            'too many requests',
            'quota exceeded',
            '429',
            'rate_limit_exceeded'
        ]
        return any(indicator in error_str for indicator in rate_limit_indicators)

    def _is_api_key_error(self, error: Exception) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯APIå¯†é’¥é”™è¯¯"""
        error_str = str(error).lower()
        api_key_indicators = [
            'api key',
            'invalid key',
            'unauthorized',
            '401',
            'authentication',
            'invalid_api_key'
        ]
        return any(indicator in error_str for indicator in api_key_indicators)


class EmbeddingManager:
    """Embedding æ¨¡å‹ç®¡ç†å™¨"""

    # æ”¯æŒçš„æä¾›å•†æ˜ å°„
    SUPPORTED_PROVIDERS = {
        'openai': '_create_openai_embeddings',
        'azure': '_create_azure_embeddings',
        'azure_openai': '_create_azure_embeddings',
        'huggingface': '_create_huggingface_embeddings',
        'hf': '_create_huggingface_embeddings',
        'ollama': '_create_ollama_embeddings',
        'google': '_create_google_embeddings',
        'gemini': '_create_google_embeddings',
        'deepseek': '_create_deepseek_embeddings',
        'qwen': '_create_qwen_embeddings',
        'zhipu': '_create_zhipu_embeddings',
        'baichuan': '_create_baichuan_embeddings',
        'cohere': '_create_cohere_embeddings',
        'mistral': '_create_mistral_embeddings',
        'jina': '_create_jina_embeddings',
    }

    @staticmethod
    def get_embedding_model(config: EmbeddingConfig) -> Embeddings:
        """
        æ ¹æ®é…ç½®åŠ¨æ€åˆ›å»º Embedding æ¨¡å‹å®ä¾‹

        Args:
            config: Embedding æ¨¡å‹é…ç½®

        Returns:
            LangChain Embeddings å®ä¾‹

        Raises:
            ValueError: å½“æä¾›å•†ä¸æ”¯æŒæ—¶
            EmbeddingError: å½“æ¨¡å‹åŠ è½½å¤±è´¥æ—¶
        """
        logger.info(f"æ­£åœ¨åŠ è½½ {config.provider} çš„ {config.model_name} æ¨¡å‹")
        logger.info(f"ğŸ” [è°ƒè¯•] EmbeddingManager - æ¥æ”¶åˆ°çš„config: provider={config.provider}, model={config.model_name}, api_key={'***' if config.api_key else 'None'}")

        # æ£€æŸ¥æä¾›å•†æ˜¯å¦æ”¯æŒ
        if config.provider not in EmbeddingManager.SUPPORTED_PROVIDERS:
            supported = list(EmbeddingManager.SUPPORTED_PROVIDERS.keys())
            raise ValueError(f"ä¸æ”¯æŒçš„ embedding æä¾›å•†: {config.provider}ã€‚æ”¯æŒçš„æä¾›å•†: {supported}")

        try:
            # åŠ¨æ€è°ƒç”¨ç›¸åº”çš„åˆ›å»ºæ–¹æ³•
            method_name = EmbeddingManager.SUPPORTED_PROVIDERS[config.provider]
            logger.info(f"ğŸ” [è°ƒè¯•] EmbeddingManager - å°†è°ƒç”¨æ–¹æ³•: {method_name}")
            method = getattr(EmbeddingManager, method_name)
            result = method(config)
            logger.info(f"ğŸ” [è°ƒè¯•] EmbeddingManager - åˆ›å»ºçš„æ¨¡å‹ç±»å‹: {type(result)}")
            return result

        except Exception as e:
            logger.error(f"åŠ è½½ {config.provider} æ¨¡å‹å¤±è´¥: {str(e)}")
            raise EmbeddingError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}") from e

    @staticmethod
    def create_batch_processor(config: EmbeddingConfig) -> BatchEmbeddingProcessor:
        """
        åˆ›å»ºæ‰¹é‡å¤„ç†å™¨

        Args:
            config: Embedding é…ç½®

        Returns:
            æ‰¹é‡å¤„ç†å™¨å®ä¾‹
        """
        embedding_model = EmbeddingManager.get_embedding_model(config)
        return BatchEmbeddingProcessor(embedding_model, config)


    @staticmethod
    def _create_openai_embeddings(config: EmbeddingConfig) -> OpenAIEmbeddings:
        """åˆ›å»º OpenAI Embeddings å®ä¾‹"""
        try:
            params = {
                "model": config.model_name,
                "show_progress_bar": True,
                "max_retries": config.max_retries,
                "timeout": config.timeout,
                **config.extra_params
            }

            if config.api_key:
                params["api_key"] = config.api_key
            if config.api_base:
                params["base_url"] = config.api_base

            return OpenAIEmbeddings(**params)
        except Exception as e:
            raise EmbeddingError(f"åˆ›å»º OpenAI æ¨¡å‹å¤±è´¥: {str(e)}") from e

    @staticmethod
    def _create_azure_embeddings(config: EmbeddingConfig) -> AzureOpenAIEmbeddings:
        """åˆ›å»º Azure OpenAI Embeddings å®ä¾‹"""
        try:
            params = {
                "model": config.model_name,
                "show_progress_bar": True,
                "max_retries": config.max_retries,
                "timeout": config.timeout,
                **config.extra_params
            }

            if config.api_key:
                params["api_key"] = config.api_key
            if config.api_base:
                params["azure_endpoint"] = config.api_base
            if config.api_version:
                params["api_version"] = config.api_version
            if config.deployment_name:
                params["azure_deployment"] = config.deployment_name

            return AzureOpenAIEmbeddings(**params)
        except Exception as e:
            raise EmbeddingError(f"åˆ›å»º Azure OpenAI æ¨¡å‹å¤±è´¥: {str(e)}") from e

    @staticmethod
    def _create_huggingface_embeddings(config: EmbeddingConfig) -> HuggingFaceEmbeddings:
        """åˆ›å»º HuggingFace Embeddings å®ä¾‹"""
        try:
            params = {
                "model_name": config.model_name,
                "show_progress": True,
                **config.extra_params
            }

            # å¦‚æœæŒ‡å®šäº† API åŸºç¡€åœ°å€ï¼Œåˆ™ä½¿ç”¨ API æ–¹å¼
            if config.api_base:
                params["api_url"] = config.api_base
                if config.api_key:
                    params["api_key"] = config.api_key

            return HuggingFaceEmbeddings(**params)
        except Exception as e:
            raise EmbeddingError(f"åˆ›å»º HuggingFace æ¨¡å‹å¤±è´¥: {str(e)}") from e

    @staticmethod
    def _create_ollama_embeddings(config: EmbeddingConfig) -> OllamaEmbeddings:
        """åˆ›å»º Ollama Embeddings å®ä¾‹"""
        try:
            params = {
                "model": config.model_name,
                "base_url": config.api_base or "http://localhost:11434",
                **config.extra_params
            }

            return OllamaEmbeddings(**params)
        except Exception as e:
            raise EmbeddingError(f"åˆ›å»º Ollama æ¨¡å‹å¤±è´¥: {str(e)}") from e

    @staticmethod
    def _create_google_embeddings(config: EmbeddingConfig) -> GoogleGenerativeAIEmbeddings:
        """åˆ›å»º Google Generative AI Embeddings å®ä¾‹"""
        try:
            params = {
                "model": config.model_name,
                **config.extra_params
            }

            if config.api_key:
                params["google_api_key"] = config.api_key

            return GoogleGenerativeAIEmbeddings(**params)
        except Exception as e:
            raise EmbeddingError(f"åˆ›å»º Google æ¨¡å‹å¤±è´¥: {str(e)}") from e

    @staticmethod
    def _create_deepseek_embeddings(config: EmbeddingConfig) -> OpenAIEmbeddings:
        """åˆ›å»º DeepSeek Embeddings å®ä¾‹ï¼ˆä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ï¼‰"""
        try:
            params = {
                "model": config.model_name,
                "base_url": config.api_base or "https://api.deepseek.com/v1",
                "show_progress_bar": True,
                "max_retries": config.max_retries,
                "timeout": config.timeout,
                **config.extra_params
            }

            if config.api_key:
                params["api_key"] = config.api_key

            return OpenAIEmbeddings(**params)
        except Exception as e:
            raise EmbeddingError(f"åˆ›å»º DeepSeek æ¨¡å‹å¤±è´¥: {str(e)}") from e

    @staticmethod
    def _create_qwen_embeddings(config: EmbeddingConfig) -> OpenAIEmbeddings:
        """åˆ›å»º é€šä¹‰åƒé—® Embeddings å®ä¾‹ï¼ˆä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ï¼‰"""
        try:
            # API Key ä¼˜å…ˆçº§ï¼šé…ç½®ä¸­çš„ api_key > ç¯å¢ƒå˜é‡ QWEN_API_KEY > ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY
            api_key = config.api_key or settings.QWEN_API_KEY or settings.DASHSCOPE_API_KEY
            
            if not api_key:
                raise EmbeddingError("é€šä¹‰åƒé—®æ¨¡å‹éœ€è¦ API Keyï¼Œè¯·åœ¨è¯·æ±‚ä¸­æä¾› api_key æˆ–åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® QWEN_API_KEY æˆ– DASHSCOPE_API_KEY")
            
            params = {
                "model": config.model_name,
                "api_key": api_key,
                "base_url": config.api_base or "https://dashscope.aliyuncs.com/compatible-mode/v1",
                "show_progress_bar": True,
                "max_retries": config.max_retries,
                "timeout": config.timeout,
                "tiktoken_enabled": False,  # å¯¹äºé OpenAI å®ç°ç¦ç”¨ tiktoken
                **config.extra_params
            }

            return OpenAIEmbeddings(**params)
        except Exception as e:
            raise EmbeddingError(f"åˆ›å»º é€šä¹‰åƒé—® æ¨¡å‹å¤±è´¥: {str(e)}") from e

    @staticmethod
    def _create_zhipu_embeddings(config: EmbeddingConfig) -> OpenAIEmbeddings:
        """åˆ›å»º æ™ºè°± AI Embeddings å®ä¾‹ï¼ˆä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ï¼‰"""
        try:
            params = {
                "model": config.model_name,
                "base_url": config.api_base or "https://open.bigmodel.cn/api/paas/v4",
                "show_progress_bar": True,
                "max_retries": config.max_retries,
                "timeout": config.timeout,
                **config.extra_params
            }

            if config.api_key:
                params["api_key"] = config.api_key

            return OpenAIEmbeddings(**params)
        except Exception as e:
            raise EmbeddingError(f"åˆ›å»º æ™ºè°± AI æ¨¡å‹å¤±è´¥: {str(e)}") from e

    @staticmethod
    def _create_baichuan_embeddings(config: EmbeddingConfig) -> OpenAIEmbeddings:
        """åˆ›å»º ç™¾å· AI Embeddings å®ä¾‹ï¼ˆä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ï¼‰"""
        try:
            params = {
                "model": config.model_name,
                "base_url": config.api_base or "https://api.baichuan-ai.com/v1",
                "show_progress_bar": True,
                "max_retries": config.max_retries,
                "timeout": config.timeout,
                **config.extra_params
            }

            if config.api_key:
                params["api_key"] = config.api_key

            return OpenAIEmbeddings(**params)
        except Exception as e:
            raise EmbeddingError(f"åˆ›å»º ç™¾å· AI æ¨¡å‹å¤±è´¥: {str(e)}") from e

    @staticmethod
    def _create_cohere_embeddings(config: EmbeddingConfig):
        """åˆ›å»º Cohere Embeddings å®ä¾‹"""
        try:
            # å°è¯•å¯¼å…¥ Cohere embeddings
            try:
                from langchain_cohere import CohereEmbeddings
            except ImportError:
                raise EmbeddingError("éœ€è¦å®‰è£… langchain_cohere: pip install langchain_cohere")

            params = {
                "model": config.model_name,
                "max_retries": config.max_retries,
                **config.extra_params
            }

            if config.api_key:
                params["cohere_api_key"] = config.api_key

            return CohereEmbeddings(**params)
        except Exception as e:
            raise EmbeddingError(f"åˆ›å»º Cohere æ¨¡å‹å¤±è´¥: {str(e)}") from e

    @staticmethod
    def _create_mistral_embeddings(config: EmbeddingConfig):
        """åˆ›å»º Mistral Embeddings å®ä¾‹"""
        try:
            # å°è¯•å¯¼å…¥ Mistral embeddings
            try:
                from langchain_mistralai import MistralAIEmbeddings
            except ImportError:
                raise EmbeddingError("éœ€è¦å®‰è£… langchain_mistralai: pip install langchain_mistralai")

            params = {
                "model": config.model_name,
                "max_retries": config.max_retries,
                **config.extra_params
            }

            if config.api_key:
                params["mistral_api_key"] = config.api_key
            if config.api_base:
                params["endpoint"] = config.api_base

            return MistralAIEmbeddings(**params)
        except Exception as e:
            raise EmbeddingError(f"åˆ›å»º Mistral æ¨¡å‹å¤±è´¥: {str(e)}") from e

    @staticmethod
    def _create_jina_embeddings(config: EmbeddingConfig) -> OpenAIEmbeddings:
        """åˆ›å»º Jina AI Embeddings å®ä¾‹ï¼ˆä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ï¼‰"""
        try:
            params = {
                "model": config.model_name,
                "base_url": config.api_base or "https://api.jina.ai/v1",
                "show_progress_bar": True,
                "max_retries": config.max_retries,
                "timeout": config.timeout,
                **config.extra_params
            }

            if config.api_key:
                params["api_key"] = config.api_key

            return OpenAIEmbeddings(**params)
        except Exception as e:
            raise EmbeddingError(f"åˆ›å»º Jina AI æ¨¡å‹å¤±è´¥: {str(e)}") from e

    @staticmethod
    def get_supported_providers() -> List[str]:
        """è·å–æ”¯æŒçš„æä¾›å•†åˆ—è¡¨"""
        return list(EmbeddingManager.SUPPORTED_PROVIDERS.keys())

    @staticmethod
    def validate_config(config: EmbeddingConfig) -> bool:
        """
        éªŒè¯é…ç½®æ˜¯å¦æœ‰æ•ˆ

        Args:
            config: é…ç½®å¯¹è±¡

        Returns:
            é…ç½®æ˜¯å¦æœ‰æ•ˆ

        Raises:
            ValueError: é…ç½®æ— æ•ˆæ—¶
        """
        if not config.provider:
            raise ValueError("provider ä¸èƒ½ä¸ºç©º")

        if not config.model_name:
            raise ValueError("model_name ä¸èƒ½ä¸ºç©º")

        if config.provider not in EmbeddingManager.SUPPORTED_PROVIDERS:
            supported = EmbeddingManager.get_supported_providers()
            raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {config.provider}ã€‚æ”¯æŒçš„æä¾›å•†: {supported}")

        # æ£€æŸ¥ç‰¹å®šæä¾›å•†çš„å¿…éœ€å‚æ•°
        if config.provider in ['openai', 'azure', 'deepseek', 'qwen', 'zhipu', 'baichuan', 'jina']:
            if not config.api_key:
                logger.warning(f"{config.provider} é€šå¸¸éœ€è¦ API å¯†é’¥")

        if config.provider == 'azure':
            if not config.api_base:
                raise ValueError("Azure æä¾›å•†éœ€è¦ api_base (Azure endpoint)")

        return True



def get_embedding_model(
        provider: str,
        model_name: str,
        api_key: Optional[str] = None,
        **kwargs
) -> Embeddings:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ ¹æ®å‚æ•°åˆ›å»º Embedding æ¨¡å‹

    Args:
        provider: æä¾›å•†åç§°
        model_name: æ¨¡å‹åç§°
        api_key: API å¯†é’¥
        **kwargs: å…¶ä»–é…ç½®å‚æ•°

    Returns:
        LangChain Embeddings å®ä¾‹
    """
    config = EmbeddingConfig(
        provider=provider,
        model_name=model_name,
        api_key=api_key,
        **kwargs
    )
    return EmbeddingManager.get_embedding_model(config)


def create_embedding_config_from_request(request_data: Dict[str, Any]) -> EmbeddingConfig:
    """
    ä» API è¯·æ±‚æ•°æ®åˆ›å»º EmbeddingConfig

    Args:
        request_data: API è¯·æ±‚ä¸­çš„ embedding_config æ•°æ®

    Returns:
        EmbeddingConfig å®ä¾‹
    """
    return EmbeddingConfig.from_dict(request_data)


async def embed_texts_with_config(
        texts: List[str],
        config: EmbeddingConfig
) -> List[List[float]]:
    """
    ä½¿ç”¨é…ç½®å¯¹æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–çš„ä¾¿æ·å‡½æ•°

    Args:
        texts: è¦å‘é‡åŒ–çš„æ–‡æœ¬åˆ—è¡¨
        config: Embedding é…ç½®

    Returns:
        å‘é‡åˆ—è¡¨
    """
    processor = EmbeddingManager.create_batch_processor(config)
    return await processor.embed_documents_with_retry(texts)


# é¢„å®šä¹‰çš„å¸¸ç”¨æ¨¡å‹é…ç½®
COMMON_EMBEDDING_MODELS = {
    "openai": {
        "text-embedding-3-small": "text-embedding-3-small",
        "text-embedding-3-large": "text-embedding-3-large",
        "text-embedding-ada-002": "text-embedding-ada-002",
    },
    "azure": {
        "text-embedding-3-small": "text-embedding-3-small",
        "text-embedding-3-large": "text-embedding-3-large",
        "text-embedding-ada-002": "text-embedding-ada-002",
    },
    "huggingface": {
        "bge-large-zh-v1.5": "BAAI/bge-large-zh-v1.5",
        "bge-base-zh-v1.5": "BAAI/bge-base-zh-v1.5",
        "bge-small-zh-v1.5": "BAAI/bge-small-zh-v1.5",
        "bge-large-en-v1.5": "BAAI/bge-large-en-v1.5",
        "bge-base-en-v1.5": "BAAI/bge-base-en-v1.5",
        "bge-small-en-v1.5": "BAAI/bge-small-en-v1.5",
        "text2vec-base": "shibing624/text2vec-base-chinese",
        "text2vec-large": "shibing624/text2vec-large-chinese",
        "m3e-base": "moka-ai/m3e-base",
        "m3e-large": "moka-ai/m3e-large",
        "gte-large": "thenlper/gte-large",
        "gte-base": "thenlper/gte-base",
        "sentence-t5-base": "sentence-transformers/sentence-t5-base",
        "all-mpnet-base-v2": "sentence-transformers/all-mpnet-base-v2",
        "all-MiniLM-L6-v2": "sentence-transformers/all-MiniLM-L6-v2",
    },
    "ollama": {
        "nomic-embed-text": "nomic-embed-text",
        "mxbai-embed-large": "mxbai-embed-large",
        "bge-large": "bge-large",
        "bge-base": "bge-base",
        "snowflake-arctic-embed": "snowflake-arctic-embed",
        "all-minilm": "all-minilm",
    },
    "google": {
        "embedding-001": "models/embedding-001",
        "text-embedding-004": "models/text-embedding-004",
        "textembedding-gecko": "models/textembedding-gecko",
        "textembedding-gecko-multilingual": "models/textembedding-gecko-multilingual",
    },
    "deepseek": {
        "deepseek-embeddings": "deepseek-embeddings",
    },
    "qwen": {
        "text-embedding-v1": "text-embedding-v1",
        "text-embedding-v2": "text-embedding-v2",
        "text-embedding-v3": "text-embedding-v3",
        "text-embedding-v4": "text-embedding-v4",
    },
    "zhipu": {
        "embedding-2": "embedding-2",
        "embedding-3": "embedding-3",
    },
    "baichuan": {
        "Baichuan-Text-Embedding": "Baichuan-Text-Embedding",
    },
    "cohere": {
        "embed-english-v3.0": "embed-english-v3.0",
        "embed-multilingual-v3.0": "embed-multilingual-v3.0",
        "embed-english-light-v3.0": "embed-english-light-v3.0",
        "embed-multilingual-light-v3.0": "embed-multilingual-light-v3.0",
    },
    "mistral": {
        "mistral-embed": "mistral-embed",
    },
    "jina": {
        "jina-embeddings-v2-base-en": "jina-embeddings-v2-base-en",
        "jina-embeddings-v2-base-zh": "jina-embeddings-v2-base-zh",
        "jina-embeddings-v2-base-de": "jina-embeddings-v2-base-de",
        "jina-embeddings-v2-base-es": "jina-embeddings-v2-base-es",
    }
}


def get_available_models(provider: str) -> Dict[str, str]:
    """
    è·å–æŒ‡å®šæä¾›å•†çš„å¯ç”¨æ¨¡å‹åˆ—è¡¨

    Args:
        provider: æä¾›å•†åç§°

    Returns:
        æ¨¡å‹åç§°åˆ°æ¨¡å‹IDçš„æ˜ å°„å­—å…¸
    """
    return COMMON_EMBEDDING_MODELS.get(provider.lower(), {})


def get_all_providers() -> List[str]:
    """è·å–æ‰€æœ‰æ”¯æŒçš„æä¾›å•†åˆ—è¡¨"""
    return list(COMMON_EMBEDDING_MODELS.keys())


def get_provider_info(provider: str) -> Dict[str, Any]:
    """
    è·å–æä¾›å•†ä¿¡æ¯

    Args:
        provider: æä¾›å•†åç§°

    Returns:
        åŒ…å«æä¾›å•†ä¿¡æ¯çš„å­—å…¸
    """
    provider = provider.lower()

    info = {
        "name": provider,
        "models": get_available_models(provider),
        "requires_api_key": provider in ['openai', 'azure', 'google', 'deepseek', 'qwen', 'zhipu', 'baichuan', 'cohere', 'mistral', 'jina'],
        "requires_endpoint": provider in ['azure', 'ollama'],
        "supports_local": provider in ['huggingface', 'ollama'],
    }

    # æ·»åŠ é»˜è®¤ç«¯ç‚¹ä¿¡æ¯
    default_endpoints = {
        'openai': 'https://api.openai.com/v1',
        'deepseek': 'https://api.deepseek.com/v1',
        'qwen': 'https://dashscope.aliyuncs.com/compatible-mode/v1',
        'zhipu': 'https://open.bigmodel.cn/api/paas/v4',
        'baichuan': 'https://api.baichuan-ai.com/v1',
        'jina': 'https://api.jina.ai/v1',
        'ollama': 'http://localhost:11434',
        'cohere': 'https://api.cohere.ai',
        'mistral': 'https://api.mistral.ai',
    }

    if provider in default_endpoints:
        info["default_endpoint"] = default_endpoints[provider]

    return info


def get_recommended_models() -> Dict[str, Dict[str, str]]:
    """
    è·å–æ¨èçš„æ¨¡å‹é…ç½®

    Returns:
        æŒ‰ç”¨é€”åˆ†ç±»çš„æ¨èæ¨¡å‹
    """
    return {
        "ä¸­æ–‡é€šç”¨": {
            "provider": "qwen",
            "model": "text-embedding-v4",
            "description": "é˜¿é‡Œäº‘é€šä¹‰åƒé—®æœ€æ–°å‘é‡åŒ–æ¨¡å‹ï¼Œæ”¯æŒ100+è¯­ç§å’Œä»£ç "
        },
        "ä¸­æ–‡æœ¬åœ°": {
            "provider": "huggingface",
            "model": "bge-large-zh-v1.5",
            "description": "é€‚åˆä¸­æ–‡æ–‡æ¡£çš„æœ¬åœ°éƒ¨ç½²å‘é‡åŒ–æ¨¡å‹"
        },
        "è‹±æ–‡é€šç”¨": {
            "provider": "openai",
            "model": "text-embedding-3-large",
            "description": "OpenAI æœ€æ–°çš„å¤§å‹å‘é‡åŒ–æ¨¡å‹"
        },
        "å¤šè¯­è¨€": {
            "provider": "huggingface",
            "model": "gte-large",
            "description": "æ”¯æŒå¤šç§è¯­è¨€çš„é€šç”¨æ¨¡å‹"
        },
        "ä»£ç ": {
            "provider": "openai",
            "model": "text-embedding-3-small",
            "description": "é€‚åˆä»£ç å‘é‡åŒ–çš„è½»é‡æ¨¡å‹"
        },
        "æœ¬åœ°éƒ¨ç½²": {
            "provider": "ollama",
            "model": "nomic-embed-text",
            "description": "æœ¬åœ°éƒ¨ç½²çš„å¼€æºå‘é‡åŒ–æ¨¡å‹"
        },
        "ç»æµå‹": {
            "provider": "huggingface",
            "model": "all-MiniLM-L6-v2",
            "description": "è½»é‡çº§ã€å¿«é€Ÿçš„å‘é‡åŒ–æ¨¡å‹"
        }
    }