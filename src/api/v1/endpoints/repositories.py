from fastapi import APIRouter, HTTPException, BackgroundTasks
from ....schemas.repository import *
import uuid
from ....services.task_queue import task_queue
from ....worker.tasks import process_repository_task
from ....db.session import get_db_session
from ....db.models import AnalysisSession, TaskStatus
from ....services.query_service import QueryService
from datetime import datetime, timezone
import logging

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/repos",
    tags=["repos"],
    responses={404: {"description": "Not found"}},
)


@router.post("/analyze")
async def analyze(req: RepoAnalyzeRequest):
    """
    åˆ†æä»“åº“
    æ¥æ”¶åŒ…å« embedding_config çš„è¯·æ±‚ï¼Œå¹¶å°†ä»»åŠ¡æ¨é€åˆ° Celery é˜Ÿåˆ—è¿›è¡Œå¼‚æ­¥å¤„ç†
    """
    try:
        logger.info(f"ğŸš€ [APIè¯·æ±‚] æ”¶åˆ°ä»“åº“åˆ†æè¯·æ±‚ - URL: {req.repo_url}")
        logger.info(f"âš™ï¸ [è¯·æ±‚é…ç½®] Embeddingæä¾›å•†: {req.embedding_config.provider}, æ¨¡å‹: {req.embedding_config.model_name}")
        
        # ç”Ÿæˆå”¯ä¸€çš„ä¼šè¯ID
        session_id = str(uuid.uuid4())
        logger.info(f"ğŸ†” [ä¼šè¯åˆ›å»º] ç”Ÿæˆä¼šè¯ID: {session_id}")
        
        # å°†ä»»åŠ¡æ¨é€åˆ° Celery
        logger.info(f"ğŸ“¤ [ä»»åŠ¡é˜Ÿåˆ—] æ­£åœ¨æ¨é€ä»»åŠ¡åˆ°Celeryé˜Ÿåˆ—...")
        task = process_repository_task.delay(
            repo_url=req.repo_url,
            session_id=session_id,
            embedding_config=req.embedding_config.model_dump()
        )
        logger.info(f"âœ… [ä»»åŠ¡é˜Ÿåˆ—] ä»»åŠ¡æ¨é€æˆåŠŸ - ä»»åŠ¡ID: {task.id}")
        
        # åˆ›å»ºæ•°æ®åº“ä¼šè¯è®°å½•å¹¶ä¿å­˜ task_idï¼ˆåœ¨åŒä¸€ä¸ªäº‹åŠ¡ä¸­ï¼‰
        logger.info(f"ğŸ’¾ [æ•°æ®åº“] æ­£åœ¨åˆ›å»ºä¼šè¯è®°å½•å¹¶ä¿å­˜ä»»åŠ¡ID...")
        db = get_db_session()
        try:
            analysis_session = AnalysisSession(
                session_id=session_id,
                repository_url=req.repo_url,
                status=TaskStatus.PENDING,
                embedding_config=req.embedding_config.model_dump(),
                created_at=datetime.now(timezone.utc),
                task_id=task.id  # ç›´æ¥åœ¨åˆ›å»ºæ—¶è®¾ç½®task_id
            )
            db.add(analysis_session)
            db.commit()
            logger.info(f"âœ… [æ•°æ®åº“] ä¼šè¯è®°å½•åˆ›å»ºæˆåŠŸï¼Œä»»åŠ¡IDå·²ä¿å­˜: {session_id} -> {task.id}")
        except Exception as e:
            logger.error(f"âŒ [æ•°æ®åº“é”™è¯¯] åˆ›å»ºä¼šè¯è®°å½•å¤±è´¥: {str(e)}")
            db.rollback()
            # å¦‚æœæ•°æ®åº“æ“ä½œå¤±è´¥ï¼Œå–æ¶ˆå·²åˆ›å»ºçš„Celeryä»»åŠ¡
            try:
                task.revoke(terminate=True)
                logger.info(f"ğŸ›‘ [ä»»åŠ¡å–æ¶ˆ] ç”±äºæ•°æ®åº“é”™è¯¯ï¼Œå·²å–æ¶ˆCeleryä»»åŠ¡: {task.id}")
            except Exception as revoke_error:
                logger.error(f"âŒ [å–æ¶ˆå¤±è´¥] æ— æ³•å–æ¶ˆCeleryä»»åŠ¡: {revoke_error}")
            raise HTTPException(status_code=500, detail=f"Failed to create session: {str(e)}")
        finally:
            if db:
                db.close()
        
        response = {
            "session_id": session_id,
            "task_id": task.id,
            "status": "queued",
            "message": "Repository analysis has been queued for processing"
        }
        logger.info(f"ğŸ‰ [APIå“åº”] åˆ†æè¯·æ±‚å¤„ç†å®Œæˆ - ä¼šè¯ID: {session_id}")
        return response
        
    except Exception as e:
        logger.error(f"ğŸ’¥ [APIé”™è¯¯] å¯åŠ¨åˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to start analysis: {str(e)}")


@router.get("/status/{session_id}")
async def status(session_id: str):
    """
    è·å–åˆ†æä¼šè¯çŠ¶æ€
    ä»æ•°æ®åº“ä¸­è·å–æŒ‡å®šä¼šè¯çš„çŠ¶æ€ä¿¡æ¯
    """
    try:
        logger.info(f"ğŸ“Š [çŠ¶æ€æŸ¥è¯¢] æ”¶åˆ°çŠ¶æ€æŸ¥è¯¢è¯·æ±‚ - ä¼šè¯ID: {session_id}")
        
        db = get_db_session()
        try:
            logger.debug(f"ğŸ’¾ [æ•°æ®åº“æŸ¥è¯¢] æ­£åœ¨æŸ¥è¯¢ä¼šè¯çŠ¶æ€...")
            session = db.query(AnalysisSession).filter(
                AnalysisSession.session_id == session_id
            ).first()
            
            if not session:
                logger.warning(f"âš ï¸ [ä¼šè¯ä¸å­˜åœ¨] æœªæ‰¾åˆ°ä¼šè¯: {session_id}")
                raise HTTPException(status_code=404, detail="Session not found")
            
            logger.info(f"âœ… [çŠ¶æ€è·å–] ä¼šè¯çŠ¶æ€: {session.status}, ä»“åº“: {session.repository_url}")
            
            response = {
                "session_id": session_id,
                "status": session.status.value if hasattr(session.status, 'value') else session.status,
                "repository_url": session.repository_url,
                "repository_name": session.repository_name,
                "repository_owner": session.repository_owner,
                "total_files": session.total_files,
                "processed_files": session.processed_files,
                "total_chunks": session.total_chunks,
                "indexed_chunks": session.indexed_chunks,
                "created_at": session.created_at.isoformat() if session.created_at else None,
                "started_at": session.started_at.isoformat() if session.started_at else None,
                "completed_at": session.completed_at.isoformat() if session.completed_at else None,
                "error_message": session.error_message
            }
            
            logger.debug(f"ğŸ“ˆ [è¿›åº¦ç»Ÿè®¡] å¤„ç†æ–‡ä»¶: {session.processed_files}/{session.total_files}, ç´¢å¼•å—: {session.indexed_chunks}/{session.total_chunks}")
            return response
            
        finally:
            if db:
                db.close()
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [çŠ¶æ€æŸ¥è¯¢é”™è¯¯] è·å–ä¼šè¯çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get session status: {str(e)}")

@router.post("/query")
async def query(req: QueryRequest):
    """
    Receive requests containing generation_mode and llm_config, then push to Celery
    for async processing
    """
    logger.info(f"ğŸ” [æŸ¥è¯¢è¯·æ±‚] æ”¶åˆ°æŸ¥è¯¢è¯·æ±‚ - ç›®æ ‡ä¼šè¯: {req.session_id}")
    logger.info(f"â“ [æŸ¥è¯¢å†…å®¹] é—®é¢˜: {req.question[:100]}{'...' if len(req.question) > 100 else ''}")
    logger.info(f"âš™ï¸ [æŸ¥è¯¢é…ç½®] ç”Ÿæˆæ¨¡å¼: {req.generation_mode}")
    
    if req.llm_config:
        logger.info(f"ğŸ¤– [LLMé…ç½®] æä¾›å•†: {req.llm_config.provider}, æ¨¡å‹: {req.llm_config.model_name}")
    
    # ç”Ÿæˆå”¯ä¸€çš„session_id
    session_id = str(uuid.uuid4())
    logger.info(f"ğŸ†” [ä»»åŠ¡ä¼šè¯] ç”ŸæˆæŸ¥è¯¢ä»»åŠ¡ä¼šè¯ID: {session_id}")
    
    # å°†ä»»åŠ¡æ¨é€åˆ°Celery
    logger.info(f"ğŸ“¤ [ä»»åŠ¡é˜Ÿåˆ—] æ­£åœ¨æ¨é€æŸ¥è¯¢ä»»åŠ¡åˆ°é˜Ÿåˆ—...")
    task_id = await task_queue.push_query_task(session_id, req)
    logger.info(f"âœ… [ä»»åŠ¡é˜Ÿåˆ—] æŸ¥è¯¢ä»»åŠ¡æ¨é€æˆåŠŸ - ä»»åŠ¡ID: {task_id}")
    
    response = {
        "session_id": session_id,
        "task_id": task_id,
        "status": "queued",
        "message": "Query task has been queued for processing"
    }
    
    logger.info(f"ğŸ‰ [æŸ¥è¯¢å“åº”] æŸ¥è¯¢è¯·æ±‚å¤„ç†å®Œæˆ - ä»»åŠ¡ä¼šè¯ID: {session_id}")
    return response

@router.delete("/analyze/{session_id}")
async def cancel_analysis(session_id: str):
    """
    åœæ­¢ä»“åº“åˆ†æä»»åŠ¡
    """
    try:
        logger.info(f"ğŸ›‘ [åœæ­¢è¯·æ±‚] æ”¶åˆ°åœæ­¢ä»“åº“åˆ†æè¯·æ±‚ - ä¼šè¯ID: {session_id}")
        
        # ä»æ•°æ®åº“è·å–ä»»åŠ¡ä¿¡æ¯
        db = get_db_session()
        try:
            analysis_session = db.query(AnalysisSession).filter(
                AnalysisSession.session_id == session_id
            ).first()
            
            if not analysis_session:
                logger.warning(f"âš ï¸ [ä¼šè¯ä¸å­˜åœ¨] æœªæ‰¾åˆ°ä¼šè¯ - ä¼šè¯ID: {session_id}")
                raise HTTPException(status_code=404, detail="Analysis session not found")
            
            # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
            if analysis_session.status in [TaskStatus.SUCCESS, TaskStatus.FAILED, TaskStatus.CANCELLED]:
                logger.info(f"â„¹ï¸ [ä»»åŠ¡å·²å®Œæˆ] ä»»åŠ¡å·²å¤„äºç»ˆæ€ - çŠ¶æ€: {analysis_session.status.value}")
                return {
                    "session_id": session_id,
                    "status": analysis_session.status.value,
                    "message": f"Task is already in final state: {analysis_session.status.value}"
                }
            
            # è·å– Celery ä»»åŠ¡ID
            task_id = analysis_session.task_id
            if not task_id:
                logger.error(f"âŒ [ä»»åŠ¡IDç¼ºå¤±] ä¼šè¯ç¼ºå°‘ä»»åŠ¡ID - ä¼šè¯ID: {session_id}")
                raise HTTPException(status_code=400, detail="Task ID not found for this session")
            
            # å–æ¶ˆ Celery ä»»åŠ¡
            logger.info(f"ğŸ›‘ [å–æ¶ˆä»»åŠ¡] æ­£åœ¨å–æ¶ˆCeleryä»»åŠ¡ - ä»»åŠ¡ID: {task_id}")
            cancel_success = await task_queue.cancel_repository_task(task_id)
            
            if cancel_success:
                # æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸ºå·²å–æ¶ˆ
                analysis_session.status = TaskStatus.CANCELLED
                analysis_session.completed_at = datetime.now(timezone.utc)
                analysis_session.error_message = "Task cancelled by user request"
                db.commit()
                
                logger.info(f"âœ… [å–æ¶ˆæˆåŠŸ] ä»“åº“åˆ†æä»»åŠ¡å·²æˆåŠŸå–æ¶ˆ - ä¼šè¯ID: {session_id}")
                return {
                    "session_id": session_id,
                    "status": "cancelled",
                    "message": "Repository analysis task has been cancelled successfully"
                }
            else:
                logger.error(f"âŒ [å–æ¶ˆå¤±è´¥] æ— æ³•å–æ¶ˆCeleryä»»åŠ¡ - ä»»åŠ¡ID: {task_id}")
                raise HTTPException(status_code=500, detail="Failed to cancel the task")
                
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"âŒ [æ•°æ®åº“é”™è¯¯] åœæ­¢ä»»åŠ¡æ—¶å‘ç”Ÿæ•°æ®åº“é”™è¯¯: {str(e)}")
            db.rollback()
            raise HTTPException(status_code=500, detail="Database error while cancelling task")
        finally:
            if db:
                db.close()
                
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ğŸ’¥ [åœæ­¢é”™è¯¯] åœæ­¢ä»“åº“åˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to cancel analysis: {str(e)}")


@router.get("/query/status/{session_id}")
async def query_status(session_id: str):
    """
    Get only the basic status information of a query task (without result data)
    Returns: task status, progress info, and basic metadata - optimized for frequent polling
    """
    try:
        logger.info(f"ğŸ“Š [æŸ¥è¯¢çŠ¶æ€] æ”¶åˆ°æŸ¥è¯¢çŠ¶æ€è¯·æ±‚ - ä»»åŠ¡ä¼šè¯ID: {session_id}")
        
        logger.debug(f"ğŸ” [çŠ¶æ€æ£€æŸ¥] æ­£åœ¨è·å–ä»»åŠ¡çŠ¶æ€...")
        status = await task_queue.get_task_status(session_id)
        
        # è·å–åŸºæœ¬ä»»åŠ¡ä¿¡æ¯ï¼ˆä¸åŒ…å«å®Œæ•´ç»“æœï¼‰
        task_info = await task_queue.get_task_info(session_id)
        
        response = {
            "session_id": session_id,
            "status": status.lower(),
            "ready": task_info.get("ready", False),
            "successful": task_info.get("successful"),
            "message": {
                "PENDING": "Task is queued and waiting to be processed",
                "STARTED": "Task is currently being processed", 
                "SUCCESS": "Task completed successfully",
                "FAILURE": "Task failed to complete",
                "RETRY": "Task is being retried",
                "REVOKED": "Task was cancelled"
            }.get(status, "Task status unknown")
        }
        
        # å¦‚æœä»»åŠ¡å¤±è´¥ï¼ŒåŒ…å«é”™è¯¯ä¿¡æ¯ä½†ä¸åŒ…å«å®Œæ•´ç»“æœ
        if status == "FAILURE" and task_info.get("error"):
            response["error"] = task_info.get("error")
            logger.error(f"âŒ [ä»»åŠ¡å¤±è´¥] æŸ¥è¯¢ä»»åŠ¡å¤±è´¥ - é”™è¯¯: {task_info.get('error')}")
        
        logger.info(f"ğŸ“ˆ [çŠ¶æ€å“åº”] ä»»åŠ¡çŠ¶æ€: {status}, æ˜¯å¦å°±ç»ª: {task_info.get('ready', False)}")
        return response
        
    except Exception as e:
        logger.error(f"âŒ [çŠ¶æ€æŸ¥è¯¢é”™è¯¯] è·å–æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving task status: {str(e)}")

@router.get("/query/result/{session_id}")
async def query_result(session_id: str):
    """
    Get only the final result data of a completed query task
    Returns: the actual query response data (answer, retrieved_context, etc.) without status metadata
    """
    try:
        logger.info(f"ğŸ“„ [ç»“æœè·å–] æ”¶åˆ°æŸ¥è¯¢ç»“æœè¯·æ±‚ - ä»»åŠ¡ä¼šè¯ID: {session_id}")
        
        result = await task_queue.get_query_result(session_id)
        
        if result is None:
            logger.warning(f"âš ï¸ [ç»“æœæœªæ‰¾åˆ°] ä»»åŠ¡ç»“æœä¸å­˜åœ¨æˆ–ä»åœ¨å¤„ç†ä¸­ - ä¼šè¯ID: {session_id}")
            raise HTTPException(status_code=404, detail="Task result not found or still processing")
        
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å¤±è´¥
        if isinstance(result, dict) and result.get("success") == False:
            error_msg = result.get('error', 'Unknown error')
            logger.error(f"âŒ [ä»»åŠ¡å¤±è´¥] æŸ¥è¯¢ä»»åŠ¡å¤±è´¥ - é”™è¯¯: {error_msg}")
            raise HTTPException(
                status_code=400, 
                detail=f"Task failed: {error_msg}"
            )
        
        # åªè¿”å›å®é™…çš„æŸ¥è¯¢æ•°æ®ï¼Œå»é™¤åŒ…è£…å±‚
        if isinstance(result, dict) and "data" in result:
            query_data = result["data"]
            logger.info(f"âœ… [ç»“æœè¿”å›] æˆåŠŸè¿”å›æŸ¥è¯¢æ•°æ® - åŒ…å«ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡")
            return query_data
        else:
            logger.info(f"âœ… [ç»“æœè¿”å›] æˆåŠŸè¿”å›æŸ¥è¯¢ç»“æœ")
            return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [ç»“æœè·å–é”™è¯¯] è·å–æŸ¥è¯¢ç»“æœå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving result: {str(e)}")

@router.get("/query/info/{session_id}")
async def query_task_info(session_id: str):
    """
    Get comprehensive task information including status, result, timing, and debug info
    Returns: complete task metadata with execution details - ideal for debugging and monitoring
    """
    try:
        logger.info(f"ğŸ” [ä»»åŠ¡ä¿¡æ¯] æ”¶åˆ°ä»»åŠ¡ä¿¡æ¯æŸ¥è¯¢è¯·æ±‚ - ä»»åŠ¡ä¼šè¯ID: {session_id}")
        
        # è·å–åŸºç¡€ä»»åŠ¡ä¿¡æ¯
        task_info = await task_queue.get_task_info(session_id)
        
        # è·å–å®Œæ•´ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
        result = await task_queue.get_query_result(session_id)
        
        # æ„å»ºå¢å¼ºçš„ä»»åŠ¡ä¿¡æ¯å“åº”
        enhanced_info = {
            "session": session_id,
            "status": task_info.get("status", "UNKNOWN"),
            "ready": task_info.get("ready", False),
            "successful": task_info.get("successful"),
            "execution_info": {
                "has_result": result is not None,
                "result_type": type(result).__name__ if result else None,
                "error": task_info.get("error"),
                "traceback": task_info.get("traceback")
            }
        }
        
        # å¦‚æœæœ‰ç»“æœï¼Œæ·»åŠ ç»“æœæ‘˜è¦ä¿¡æ¯
        if result and isinstance(result, dict):
            if result.get("success") and "data" in result:
                data = result["data"]
                enhanced_info["result_summary"] = {
                    "success": True,
                    "has_answer": "answer" in data if isinstance(data, dict) else False,
                    "context_chunks": len(data.get("retrieved_context", [])) if isinstance(data, dict) else 0,
                    "generation_mode": data.get("generation_mode") if isinstance(data, dict) else None,
                    "timing": {
                        "retrieval_time": data.get("retrieval_time") if isinstance(data, dict) else None,
                        "generation_time": data.get("generation_time") if isinstance(data, dict) else None,
                        "total_time": data.get("total_time") if isinstance(data, dict) else None
                    }
                }
            else:
                enhanced_info["result_summary"] = {
                    "success": False,
                    "error": result.get("error", "Unknown error")
                }
        
        # å¦‚æœéœ€è¦å®Œæ•´ç»“æœï¼Œå¯ä»¥é€‰æ‹©æ€§åŒ…å«
        # enhanced_info["full_result"] = result  # å¯é€‰ï¼šåŒ…å«å®Œæ•´ç»“æœ
        
        logger.info(f"ğŸ“Š [ä¿¡æ¯æ±‡æ€»] ä»»åŠ¡çŠ¶æ€: {enhanced_info['status']}, æ˜¯å¦æˆåŠŸ: {enhanced_info['successful']}, æœ‰ç»“æœ: {enhanced_info['execution_info']['has_result']}")
        return enhanced_info
        
    except Exception as e:
        logger.error(f"âŒ [ä¿¡æ¯è·å–é”™è¯¯] è·å–ä»»åŠ¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving task info: {str(e)}")

@router.delete("/cache")
async def clear_cache():
    """
    Clear BM25 cache to apply improved tokenization and file name matching logic
    """
    try:
        logger.info("ğŸ§¹ [ç¼“å­˜æ¸…ç†] æ”¶åˆ°æ¸…é™¤BM25ç¼“å­˜è¯·æ±‚")
        
        # åˆ›å»ºQueryServiceå®ä¾‹å¹¶æ¸…é™¤ç¼“å­˜
        query_service = QueryService()
        query_service.clear_cache()
        
        logger.info("âœ… [ç¼“å­˜æ¸…ç†] BM25ç¼“å­˜å·²æˆåŠŸæ¸…é™¤")
        return {
            "status": "success",
            "message": "BM25 cache cleared successfully"
        }
        
    except Exception as e:
        logger.error(f"âŒ [ç¼“å­˜æ¸…ç†é”™è¯¯] æ¸…é™¤BM25ç¼“å­˜å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to clear cache: {str(e)}")
