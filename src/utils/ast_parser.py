"""
ASTè§£æå™¨
è´Ÿè´£æŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰çš„ç”Ÿæˆå’Œåˆ†æ
"""

import logging
import os
import re
from typing import Any, Dict, List, Optional, Set, Callable
from langchain_core.documents import Document
from tree_sitter import Language, Parser, Node

# åŠ¨æ€å¯¼å…¥è¯­è¨€è§£æå™¨
AVAILABLE_PARSERS = {}

# è¯­è¨€æ¨¡å—æ˜ å°„
LANGUAGE_MODULES = {
    'python': 'tree_sitter_python',
    'javascript': 'tree_sitter_javascript',
    'typescript': 'tree_sitter_typescript', 
    'java': 'tree_sitter_java',
    'cpp': 'tree_sitter_cpp',
    'go': 'tree_sitter_go',
    'rust': 'tree_sitter_rust',
    'csharp': 'tree_sitter_c_sharp'
}

# åŠ¨æ€åŠ è½½å¯é€‰è¯­è¨€è§£æå™¨
for lang, module_name in LANGUAGE_MODULES.items():
    try:
        module = __import__(module_name, fromlist=[module_name])
        AVAILABLE_PARSERS[lang] = module
    except ImportError:
        pass

# åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ æ›´å¥½çš„å¯¼å…¥å¤„ç†
try:
    from .file_parser import FileType
    from ..core.config import settings
except ImportError:
    # ç›´æ¥è¿è¡Œæ—¶çš„å›é€€
    class FileType:
        CODE = "code"
        TEXT = "text"
        BINARY = "binary"
    
    class Settings:
        AST_MAX_FILE_SIZE = 1024 * 1024
        AST_SUPPORTED_LANGUAGES = []
    
    settings = Settings()

logger = logging.getLogger(__name__)

class AstParser:
    # è¯­è¨€é…ç½®ç¼“å­˜
    _LANGUAGE_CONFIGS = {
        'python': {'extensions': {'.py'}, 'node_types': {
            'class_definition', 'function_definition', 'assignment', 
            'decorated_definition', 'import_statement', 'import_from_statement'
        }},
        'javascript': {'extensions': {'.js', '.jsx', '.mjs'}, 'node_types': {
            'class_declaration', 'function_declaration', 'method_definition',
            'arrow_function', 'variable_declaration', 'import_statement', 'export_statement'
        }},
        'typescript': {'extensions': {'.ts', '.tsx'}, 'node_types': {
            'class_declaration', 'function_declaration', 'method_definition',
            'arrow_function', 'variable_declaration', 'import_statement', 'export_statement'
        }},
        'java': {'extensions': {'.java'}, 'node_types': {
            'class_declaration', 'interface_declaration', 'method_declaration',
            'field_declaration', 'import_declaration', 'package_declaration'
        }},
        'cpp': {'extensions': {'.cpp', '.cc', '.cxx', '.c++', '.hpp', '.h'}, 'node_types': {
            'class_specifier', 'struct_specifier', 'function_definition',
            'declaration', 'preproc_include'
        }},
        'go': {'extensions': {'.go'}, 'node_types': {
            'type_declaration', 'function_declaration', 'method_declaration',
            'var_declaration', 'import_declaration', 'package_clause'
        }},
        'rust': {'extensions': {'.rs'}, 'node_types': {
            'struct_item', 'enum_item', 'impl_item', 'function_item',
            'let_declaration', 'use_declaration'
        }},
        'csharp': {'extensions': {'.cs'}, 'node_types': {
            'class_declaration', 'interface_declaration', 'struct_declaration',
            'method_declaration', 'property_declaration', 'field_declaration', 'using_directive'
        }}
    }

    def __init__(self, 
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200,
                 min_chunk_size: int = 100,
                 max_chunk_size: int = 2000):
        """åˆå§‹åŒ–ASTè§£æå™¨
        
        Args:
            chunk_size: ç›®æ ‡å—å¤§å°ï¼ˆéç©ºç™½å­—ç¬¦æ•°ï¼‰
            chunk_overlap: å—é‡å å¤§å°
            min_chunk_size: æœ€å°å—å¤§å°
            max_chunk_size: æœ€å¤§å—å¤§å°
        """
        self.parsers: Dict[str, Parser] = {}
        self._extension_to_language = {}
        self._element_extractors_cache = {}
        
        # åˆ†å—é…ç½®
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        
        self._init_languages()

    def _init_languages(self):
        """åˆå§‹åŒ–æ”¯æŒçš„ç¼–ç¨‹è¯­è¨€"""
        initialized_count = 0
        
        for lang_name, config in self._LANGUAGE_CONFIGS.items():
            if lang_name not in AVAILABLE_PARSERS:
                logger.debug(f"âš ï¸ {lang_name} è§£æå™¨æ¨¡å—æœªå®‰è£…ï¼Œè·³è¿‡åˆå§‹åŒ–")
                continue
                
            try:
                module = AVAILABLE_PARSERS[lang_name]
                
                # è·å–è¯­è¨€å¯¹è±¡
                language = None

                if lang_name == 'typescript' and hasattr(module, 'language_typescript'):
                    language = Language(module.language_typescript())
                elif lang_name == 'typescript' and hasattr(module, 'typescript'):
                    language = Language(module.typescript())
                else:
                    language = Language(module.language())

                parser = Parser(language)
                
                self.parsers[lang_name] = parser
                
                # æ„å»ºæ‰©å±•åæ˜ å°„
                for ext in config['extensions']:
                    self._extension_to_language[ext] = lang_name
                
                initialized_count += 1
                logger.debug(f"âœ… åˆå§‹åŒ– {lang_name} è§£æå™¨æˆåŠŸ")
                
            except Exception as e:
                logger.warning(f"âš ï¸ åˆå§‹åŒ– {lang_name} è§£æå™¨å¤±è´¥: {e}")

        logger.info(f"ğŸ”§ ASTè§£æå™¨åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒ {initialized_count} ç§è¯­è¨€: {list(self.parsers.keys())}")

    def _detect_language_from_extension(self, file_path: str) -> Optional[str]:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åæ£€æµ‹ç¼–ç¨‹è¯­è¨€"""
        ext = os.path.splitext(file_path)[1].lower()
        return self._extension_to_language.get(ext)

    def should_use_ast_parsing(self, file_info: Dict[str, Any], language: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨ASTè§£æ
        
        Args:
            file_info: æ–‡ä»¶ä¿¡æ¯
            language: ç¼–ç¨‹è¯­è¨€
            
        Returns:
            bool: æ˜¯å¦ä½¿ç”¨ASTè§£æ
        """
        # åªå¯¹ä»£ç æ–‡ä»¶ä½¿ç”¨ASTè§£æ
        if file_info.get("file_type") != FileType.CODE:
            return False
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶
        max_size = getattr(settings, 'AST_MAX_FILE_SIZE', 1024 * 1024)  # 1MB
        if file_info.get("file_size", 0) > max_size:
            logger.debug(f"âš ï¸ æ–‡ä»¶è¿‡å¤§ï¼Œè·³è¿‡ASTè§£æ: {file_info.get('file_path')}")
            return False
        
        # æ£€æŸ¥è¯­è¨€æ”¯æŒ
        detected_lang = self._detect_language_from_extension(file_info.get('file_path', ''))
        return (detected_lang in self.parsers or 
                language.lower() in self.parsers)

    def parse_with_ast(self, content: str, file_path: str, language: str) -> List[Document]:
        """
        ä½¿ç”¨ASTè§£ææ–‡ä»¶å†…å®¹

        Args:
            content: æ–‡ä»¶å†…å®¹
            file_path: æ–‡ä»¶è·¯å¾„
            language: ç¼–ç¨‹è¯­è¨€

        Returns:
            List[Document]: è§£æå¾—åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        """
        # ç¡®å®šå®é™…ä½¿ç”¨çš„è¯­è¨€
        actual_language = self._determine_language(file_path, language)
        if not actual_language:
            return self._create_fallback_document(content, file_path, language, "unsupported_language")

        try:
            # ä½¿ç”¨å¯¹åº”è¯­è¨€çš„è§£æå™¨
            parser = self.parsers[actual_language]
            tree = parser.parse(content.encode('utf8'))
            
            if tree.root_node.has_error:
                logger.warning(f"âš ï¸ ASTåŒ…å«è¯­æ³•é”™è¯¯: {file_path}")
            
            documents = []
            source_bytes = content.encode('utf8')
            
            # æå–ä»£ç å…ƒç´ 
            self._extract_code_elements(tree.root_node, source_bytes, file_path, documents, actual_language)
            
            # åº”ç”¨åˆ†å—å’Œåˆå¹¶ç­–ç•¥
            processed_documents = self._process_documents_with_chunking(documents, file_path, actual_language)
            
            logger.debug(f"âœ… ASTè§£æå®Œæˆ: {file_path} ({actual_language}), æå–äº† {len(documents)} ä¸ªä»£ç å…ƒç´ ï¼Œå¤„ç†å {len(processed_documents)} ä¸ªæ–‡æ¡£å—")
            return processed_documents
            
        except Exception as e:
            logger.error(f"âŒ ASTè§£æå¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
            return self._create_fallback_document(content, file_path, language, "ast_parsing_failed")

    def _count_non_whitespace_chars(self, text: str) -> int:
        """è®¡ç®—éç©ºç™½å­—ç¬¦æ•°"""
        return len(re.sub(r'\s', '', text))

    def _process_documents_with_chunking(self, documents: List[Document], file_path: str, language: str) -> List[Document]:
        """
        å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—å’Œåˆå¹¶å¤„ç†
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            file_path: æ–‡ä»¶è·¯å¾„
            language: ç¼–ç¨‹è¯­è¨€
            
        Returns:
            List[Document]: å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            return documents
            
        processed_docs = []
        
        # é¦–å…ˆå¤„ç†éœ€è¦åˆ†å—çš„å¤§æ–‡æ¡£
        for doc in documents:
            non_ws_count = self._count_non_whitespace_chars(doc.page_content)
            
            if non_ws_count > self.max_chunk_size:
                # éœ€è¦åˆ†å—
                chunked_docs = self._chunk_large_document(doc, file_path, language)
                processed_docs.extend(chunked_docs)
            else:
                processed_docs.append(doc)
        
        # ç„¶ååˆå¹¶å°æ–‡æ¡£
        merged_docs = self._merge_small_documents(processed_docs, file_path, language)
        
        return merged_docs

    def _chunk_large_document(self, doc: Document, file_path: str, language: str) -> List[Document]:
        """
        åˆ†å—å¤§æ–‡æ¡£
        
        Args:
            doc: è¦åˆ†å—çš„æ–‡æ¡£
            file_path: æ–‡ä»¶è·¯å¾„
            language: ç¼–ç¨‹è¯­è¨€
            
        Returns:
            List[Document]: åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        """
        content = doc.page_content
        lines = content.split('\n')
        chunks = []
        
        current_chunk_lines = []
        current_non_ws_count = 0
        
        for line in lines:
            line_non_ws = self._count_non_whitespace_chars(line)
            
            # æ£€æŸ¥æ·»åŠ è¿™ä¸€è¡Œæ˜¯å¦ä¼šè¶…è¿‡ç›®æ ‡å¤§å°
            if (current_non_ws_count + line_non_ws > self.chunk_size and 
                current_chunk_lines and 
                current_non_ws_count >= self.min_chunk_size):
                
                # åˆ›å»ºå½“å‰å—
                chunk_content = '\n'.join(current_chunk_lines)
                chunk_doc = self._create_chunk_document(
                    chunk_content, doc, len(chunks), file_path, language
                )
                chunks.append(chunk_doc)
                
                # å¤„ç†é‡å 
                overlap_lines = self._get_overlap_lines(current_chunk_lines)
                current_chunk_lines = overlap_lines + [line]
                current_non_ws_count = self._count_non_whitespace_chars('\n'.join(current_chunk_lines))
            else:
                current_chunk_lines.append(line)
                current_non_ws_count += line_non_ws
        
        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk_lines:
            chunk_content = '\n'.join(current_chunk_lines)
            chunk_doc = self._create_chunk_document(
                chunk_content, doc, len(chunks), file_path, language
            )
            chunks.append(chunk_doc)
        
        logger.debug(f"ğŸ“„ å¤§æ–‡æ¡£åˆ†å—: {doc.metadata.get('element_name', 'Unknown')} -> {len(chunks)} ä¸ªå—")
        return chunks

    def _get_overlap_lines(self, lines: List[str]) -> List[str]:
        """è·å–é‡å çš„è¡Œ"""
        if not lines or self.chunk_overlap <= 0:
            return []
            
        overlap_chars = 0
        overlap_lines = []
        
        # ä»æœ«å°¾å¼€å§‹è®¡ç®—é‡å 
        for line in reversed(lines):
            line_non_ws = self._count_non_whitespace_chars(line)
            if overlap_chars + line_non_ws <= self.chunk_overlap:
                overlap_lines.insert(0, line)
                overlap_chars += line_non_ws
            else:
                break
                
        return overlap_lines

    def _create_chunk_document(self, content: str, original_doc: Document, 
                             chunk_index: int, file_path: str, language: str) -> Document:
        """åˆ›å»ºåˆ†å—æ–‡æ¡£"""
        metadata = original_doc.metadata.copy()
        metadata.update({
            "is_chunk": True,
            "chunk_index": chunk_index,
            "original_element_name": metadata.get("element_name", "Unknown"),
            "chunk_non_ws_chars": self._count_non_whitespace_chars(content)
        })
        
        return Document(
            page_content=content,
            metadata=metadata
        )

    def _merge_small_documents(self, documents: List[Document], file_path: str, language: str) -> List[Document]:
        """
        åˆå¹¶å°æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            file_path: æ–‡ä»¶è·¯å¾„
            language: ç¼–ç¨‹è¯­è¨€
            
        Returns:
            List[Document]: åˆå¹¶åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            return documents
            
        merged_docs = []
        current_merge_group = []
        current_merge_size = 0
        
        # æŒ‰å…ƒç´ ç±»å‹åˆ†ç»„ï¼Œä¼˜å…ˆçº§ï¼šimport < assignment < function < class
        element_priority = {
            "import": 1,
            "assignment": 2, 
            "function": 3,
            "decorated_definition": 3,
            "class": 4
        }
        
        # æŒ‰ä¼˜å…ˆçº§å’Œä½ç½®æ’åº
        sorted_docs = sorted(documents, key=lambda doc: (
            element_priority.get(doc.metadata.get("element_type", "unknown"), 5),
            doc.metadata.get("start_line", 0)
        ))
        
        for doc in sorted_docs:
            non_ws_count = self._count_non_whitespace_chars(doc.page_content)
            
            # å¦‚æœæ–‡æ¡£å·²ç»è¶³å¤Ÿå¤§ï¼Œç›´æ¥æ·»åŠ 
            if non_ws_count >= self.min_chunk_size:
                # å…ˆå¤„ç†å½“å‰åˆå¹¶ç»„
                if current_merge_group:
                    merged_doc = self._create_merged_document(current_merge_group, file_path, language)
                    merged_docs.append(merged_doc)
                    current_merge_group = []
                    current_merge_size = 0
                
                merged_docs.append(doc)
                continue
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆå¹¶
            can_merge = self._can_merge_documents(current_merge_group, doc)
            
            if (can_merge and 
                current_merge_size + non_ws_count <= self.chunk_size):
                # åŠ å…¥å½“å‰åˆå¹¶ç»„
                current_merge_group.append(doc)
                current_merge_size += non_ws_count
            else:
                # ç»“æŸå½“å‰åˆå¹¶ç»„ï¼Œå¼€å§‹æ–°çš„
                if current_merge_group:
                    merged_doc = self._create_merged_document(current_merge_group, file_path, language)
                    merged_docs.append(merged_doc)
                
                current_merge_group = [doc]
                current_merge_size = non_ws_count
        
        # å¤„ç†æœ€åä¸€ä¸ªåˆå¹¶ç»„
        if current_merge_group:
            merged_doc = self._create_merged_document(current_merge_group, file_path, language)
            merged_docs.append(merged_doc)
        
        logger.debug(f"ğŸ”— æ–‡æ¡£åˆå¹¶: {len(documents)} -> {len(merged_docs)} ä¸ªæ–‡æ¡£")
        return merged_docs

    def _can_merge_documents(self, current_group: List[Document], new_doc: Document) -> bool:
        """åˆ¤æ–­æ–‡æ¡£æ˜¯å¦å¯ä»¥åˆå¹¶"""
        if not current_group:
            return True
            
        # ç›¸åŒç±»å‹çš„å…ƒç´ å¯ä»¥åˆå¹¶
        last_doc = current_group[-1]
        last_type = last_doc.metadata.get("element_type", "")
        new_type = new_doc.metadata.get("element_type", "")
        
        # å¯¼å…¥è¯­å¥å¯ä»¥åˆå¹¶
        if last_type == "import" and new_type == "import":
            return True
            
        # åŒç±»å‹çš„èµ‹å€¼å¯ä»¥åˆå¹¶
        if last_type == "assignment" and new_type == "assignment":
            return True
            
        # å°å‡½æ•°å¯ä»¥åˆå¹¶
        if (last_type in ["function", "decorated_definition"] and 
            new_type in ["function", "decorated_definition"]):
            last_size = self._count_non_whitespace_chars(last_doc.page_content)
            new_size = self._count_non_whitespace_chars(new_doc.page_content)
            if last_size < self.min_chunk_size and new_size < self.min_chunk_size:
                return True
        
        return False

    def _create_merged_document(self, docs: List[Document], file_path: str, language: str) -> Document:
        """åˆ›å»ºåˆå¹¶æ–‡æ¡£"""
        if len(docs) == 1:
            return docs[0]
            
        # åˆå¹¶å†…å®¹
        contents = [doc.page_content for doc in docs]
        merged_content = '\n\n'.join(contents)
        
        # åˆå¹¶å…ƒæ•°æ®
        element_types = [doc.metadata.get("element_type", "") for doc in docs]
        element_names = [doc.metadata.get("element_name", "") for doc in docs]
        
        # ç¡®å®šä¸»è¦ç±»å‹
        type_counts = {}
        for et in element_types:
            type_counts[et] = type_counts.get(et, 0) + 1
        main_type = max(type_counts, key=type_counts.get) if type_counts else "merged"
        
        # åˆ›å»ºåˆå¹¶çš„å…ƒæ•°æ®
        merged_metadata = {
            "file_path": file_path,
            "language": language,
            "element_type": main_type,
            "element_name": f"merged_{main_type}",
            "is_merged": True,
            "merged_count": len(docs),
            "merged_elements": element_names,
            "start_line": min(doc.metadata.get("start_line", 0) for doc in docs),
            "end_line": max(doc.metadata.get("end_line", 0) for doc in docs),
            "merged_non_ws_chars": self._count_non_whitespace_chars(merged_content)
        }
        
        return Document(
            page_content=merged_content,
            metadata=merged_metadata
        )

    def _determine_language(self, file_path: str, language: str) -> Optional[str]:
        """ç¡®å®šè¦ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€"""
        # ä¼˜å…ˆä½¿ç”¨æ–‡ä»¶æ‰©å±•åæ£€æµ‹
        detected_lang = self._detect_language_from_extension(file_path)
        if detected_lang and detected_lang in self.parsers:
            return detected_lang
        
        # å…¶æ¬¡ä½¿ç”¨ä¼ å…¥çš„è¯­è¨€å‚æ•°
        normalized_lang = language.lower()
        if normalized_lang in self.parsers:
            return normalized_lang
            
        return None

    def _create_fallback_document(self, content: str, file_path: str, language: str, error_type: str) -> List[Document]:
        """åˆ›å»ºå›é€€æ–‡æ¡£"""
        if error_type == "unsupported_language":
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„è¯­è¨€: {language}, æ–‡ä»¶: {file_path}")
        
        return [Document(
            page_content=content,
            metadata={
                "file_path": file_path,
                "language": language,
                "element_type": "file",
                error_type: True
            }
        )]

    def _extract_code_elements(self, node: Node, source_bytes: bytes, file_path: str, 
                             documents: List[Document], language: str):
        """
        é€’å½’æå–ä»£ç å…ƒç´ 
        
        Args:
            node: ASTèŠ‚ç‚¹
            source_bytes: æºä»£ç å­—èŠ‚
            file_path: æ–‡ä»¶è·¯å¾„
            documents: æ–‡æ¡£åˆ—è¡¨
            language: ç¼–ç¨‹è¯­è¨€
        """
        # è·å–å…ƒç´ æå–å™¨ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        extractors = self._get_element_extractors_cached(language)
        
        # å¦‚æœå½“å‰èŠ‚ç‚¹æ˜¯ç›®æ ‡ç±»å‹ï¼Œæå–å®ƒ
        if node.type in extractors:
            try:
                doc = extractors[node.type](node, source_bytes, file_path, language)
                if doc:
                    documents.append(doc)
            except Exception as e:
                logger.warning(f"âš ï¸ æå–èŠ‚ç‚¹å¤±è´¥: {node.type} in {file_path}, é”™è¯¯: {str(e)}")
        
        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        for child in node.children:
            self._extract_code_elements(child, source_bytes, file_path, documents, language)

    def _get_element_extractors_cached(self, language: str) -> Dict[str, Callable]:
        """è·å–å…ƒç´ æå–å™¨ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if language not in self._element_extractors_cache:
            self._element_extractors_cache[language] = self._build_element_extractors(language)
        return self._element_extractors_cache[language]

    def _build_element_extractors(self, language: str) -> Dict[str, Callable]:
        """æ„å»ºå…ƒç´ æå–å™¨æ˜ å°„"""
        # åŸºç¡€æå–å™¨
        extractors = {}
        
        # æ ¹æ®è¯­è¨€é…ç½®æ·»åŠ æå–å™¨
        config = self._LANGUAGE_CONFIGS.get(language, {})
        node_types = config.get('node_types', set())
        
        for node_type in node_types:
            if 'class' in node_type or 'struct' in node_type or 'interface' in node_type or 'enum' in node_type:
                extractors[node_type] = self._extract_class
            elif 'function' in node_type or 'method' in node_type:
                extractors[node_type] = self._extract_function
            elif 'import' in node_type or 'export' in node_type or 'package' in node_type or 'using' in node_type or 'use_declaration' in node_type:
                extractors[node_type] = self._extract_import
            elif 'assignment' in node_type or 'declaration' in node_type or 'var_' in node_type or 'let_' in node_type or 'field_' in node_type or 'property_' in node_type:
                extractors[node_type] = self._extract_assignment
            elif 'decorated' in node_type:
                extractors[node_type] = self._extract_decorated_definition
                
        return extractors

    def _extract_class(self, node: Node, source_bytes: bytes, file_path: str, language: str = "python") -> Document:
        """æå–ç±»å®šä¹‰"""
        content = source_bytes[node.start_byte:node.end_byte].decode('utf8')
        
        # æå–ç±»å - æ ¹æ®è¯­è¨€è°ƒæ•´
        class_name = self._extract_identifier(node, source_bytes, language)
        
        return Document(
            page_content=content,
            metadata={
                "file_path": file_path,
                "element_type": "class",
                "element_name": class_name,
                "start_line": node.start_point[0] + 1,
                "end_line": node.end_point[0] + 1,
                "language": language
            }
        )

    def _extract_function(self, node: Node, source_bytes: bytes, file_path: str, language: str = "python") -> Document:
        """æå–å‡½æ•°å®šä¹‰"""
        content = source_bytes[node.start_byte:node.end_byte].decode('utf8')
        
        # æå–å‡½æ•°å
        function_name = self._extract_identifier(node, source_bytes, language)
        
        return Document(
            page_content=content,
            metadata={
                "file_path": file_path,
                "element_type": "function",
                "element_name": function_name,
                "start_line": node.start_point[0] + 1,
                "end_line": node.end_point[0] + 1,
                "language": language
            }
        )

    def _extract_import(self, node: Node, source_bytes: bytes, file_path: str, language: str = "python") -> Document:
        """æå–å¯¼å…¥è¯­å¥"""
        content = source_bytes[node.start_byte:node.end_byte].decode('utf8')
        
        return Document(
            page_content=content,
            metadata={
                "file_path": file_path,
                "element_type": "import",
                "element_name": content.strip(),
                "start_line": node.start_point[0] + 1,
                "end_line": node.end_point[0] + 1,
                "language": language
            }
        )

    def _extract_assignment(self, node: Node, source_bytes: bytes, file_path: str, language: str = "python") -> Document:
        """æå–å˜é‡èµ‹å€¼"""
        content = source_bytes[node.start_byte:node.end_byte].decode('utf8')
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ¨¡å—çº§åˆ«çš„èµ‹å€¼
        if language == 'python':
            parent = node.parent
            while parent:
                if parent.type in ['function_definition', 'class_definition']:
                    return None
                parent = parent.parent
        
        # æå–å˜é‡å
        variable_name = self._extract_variable_name(node, source_bytes, language)
        
        return Document(
            page_content=content,
            metadata={
                "file_path": file_path,
                "element_type": "assignment",
                "element_name": variable_name,
                "start_line": node.start_point[0] + 1,
                "end_line": node.end_point[0] + 1,
                "language": language
            }
        )

    def _extract_decorated_definition(self, node: Node, source_bytes: bytes, file_path: str, language: str = "python") -> Document:
        """æå–è£…é¥°å™¨å®šä¹‰"""
        content = source_bytes[node.start_byte:node.end_byte].decode('utf8')
        
        # æŸ¥æ‰¾è¢«è£…é¥°çš„å®šä¹‰
        definition_name = self._extract_identifier(node, source_bytes, language)
        
        return Document(
            page_content=content,
            metadata={
                "file_path": file_path,
                "element_type": "decorated_definition",
                "element_name": definition_name,
                "start_line": node.start_point[0] + 1,
                "end_line": node.end_point[0] + 1,
                "language": language
            }
        )

    def _extract_identifier(self, node: Node, source_bytes: bytes, language: str) -> str:
        """æå–æ ‡è¯†ç¬¦åç§°"""
        # è¯­è¨€ç‰¹å®šçš„æ ‡è¯†ç¬¦æå–ç­–ç•¥
        if language == 'javascript' or language == 'typescript':
            return self._extract_js_identifier(node, source_bytes)
        elif language == 'java':
            return self._extract_java_identifier(node, source_bytes)
        elif language == 'python':
            return self._extract_python_identifier(node, source_bytes)
        else:
            # é€šç”¨æå–é€»è¾‘
            return self._extract_generic_identifier(node, source_bytes)

    def _extract_js_identifier(self, node: Node, source_bytes: bytes) -> str:
        """æå–JavaScript/TypeScriptæ ‡è¯†ç¬¦"""
        # JavaScriptæ–¹æ³•å®šä¹‰çš„ç‰¹æ®Šå¤„ç†
        if node.type == 'method_definition':
            # æŸ¥æ‰¾property_identifierèŠ‚ç‚¹
            for child in node.children:
                if child.type == 'property_identifier':
                    return source_bytes[child.start_byte:child.end_byte].decode('utf8')
        
        # å‡½æ•°å£°æ˜å’Œç®­å¤´å‡½æ•°
        if node.type in ['function_declaration', 'arrow_function']:
            for child in node.children:
                if child.type == 'identifier':
                    return source_bytes[child.start_byte:child.end_byte].decode('utf8')
        
        # å˜é‡å£°æ˜
        if node.type == 'variable_declaration':
            for child in node.children:
                if child.type == 'variable_declarator':
                    for grandchild in child.children:
                        if grandchild.type == 'identifier':
                            return source_bytes[grandchild.start_byte:grandchild.end_byte].decode('utf8')
        
        # é€šç”¨æŸ¥æ‰¾
        return self._extract_generic_identifier(node, source_bytes)

    def _extract_python_identifier(self, node: Node, source_bytes: bytes) -> str:
        """æå–Pythonæ ‡è¯†ç¬¦"""
        # ç›´æ¥æŸ¥æ‰¾identifierèŠ‚ç‚¹
        for child in node.children:
            if child.type == "identifier":
                return source_bytes[child.start_byte:child.end_byte].decode('utf8')
        
        # é€’å½’æŸ¥æ‰¾
        return self._extract_identifier_recursive(node, source_bytes, max_depth=2)

    def _extract_java_identifier(self, node: Node, source_bytes: bytes) -> str:
        """æå–Javaæ ‡è¯†ç¬¦"""
        # Javaç‰¹å®šçš„æ ‡è¯†ç¬¦æå–
        for child in node.children:
            if child.type == "identifier":
                return source_bytes[child.start_byte:child.end_byte].decode('utf8')
        
        return self._extract_generic_identifier(node, source_bytes)

    def _extract_generic_identifier(self, node: Node, source_bytes: bytes) -> str:
        """é€šç”¨æ ‡è¯†ç¬¦æå–"""
        # ç›´æ¥æŸ¥æ‰¾identifierèŠ‚ç‚¹
        for child in node.children:
            if child.type == "identifier":
                return source_bytes[child.start_byte:child.end_byte].decode('utf8')
        
        # é€’å½’æŸ¥æ‰¾ï¼ˆé™åˆ¶æ·±åº¦ï¼‰
        return self._extract_identifier_recursive(node, source_bytes, max_depth=3)

    def _extract_identifier_recursive(self, node: Node, source_bytes: bytes, max_depth: int = 2) -> str:
        """é€’å½’æå–æ ‡è¯†ç¬¦ï¼ˆé™åˆ¶æ·±åº¦ï¼‰"""
        if max_depth <= 0:
            return "Unknown"
            
        for child in node.children:
            if child.type in ["identifier", "property_identifier"]:
                return source_bytes[child.start_byte:child.end_byte].decode('utf8')
            
            # é€’å½’æŸ¥æ‰¾
            result = self._extract_identifier_recursive(child, source_bytes, max_depth - 1)
            if result != "Unknown":
                return result
        
        return "Unknown"

    def _extract_variable_name(self, node: Node, source_bytes: bytes, language: str) -> str:
        """æå–å˜é‡å"""
        # å¿«é€Ÿè·¯å¾„ï¼šPythonç®€å•èµ‹å€¼
        if language == 'python':
            content = source_bytes[node.start_byte:node.end_byte].decode('utf8')
            if '=' in content:
                return content.split('=')[0].strip()
        
        # é€šç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾variable_declaratoræˆ–identifier
        for child in node.children:
            if child.type == 'variable_declarator':
                identifier = self._extract_identifier(child, source_bytes, language)
                if identifier != "Unknown":
                    return identifier
            elif child.type == 'identifier':
                return source_bytes[child.start_byte:child.end_byte].decode('utf8')
        
        return "Unknown"

    def get_supported_languages(self) -> List[str]:
        """è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨"""
        return list(self.parsers.keys())

    def get_language_extensions(self, language: str) -> Set[str]:
        """è·å–è¯­è¨€æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å"""
        config = self._LANGUAGE_CONFIGS.get(language, {})
        return config.get('extensions', set())
