"""
ASTè§£æå™¨
è´Ÿè´£æŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰çš„ç”Ÿæˆå’Œåˆ†æ
"""

import logging
import os
import re
import json
from typing import Any, Dict, List, Optional, Set, Callable, NamedTuple
from langchain_core.documents import Document
from tree_sitter import Language, Parser, Node

# ç®€å•çš„ä¼ªèŠ‚ç‚¹ç±»ï¼Œç”¨äºå¤§ç±»åˆ†è§£
class MockNode(NamedTuple):
    start_byte: int
    end_byte: int
    type: str

# åŠ¨æ€å¯¼å…¥è¯­è¨€è§£æå™¨
AVAILABLE_PARSERS = {}

# è¯­è¨€æ¨¡å—æ˜ å°„
LANGUAGE_MODULES = {
    'python': 'tree_sitter_python',
    'javascript': 'tree_sitter_javascript',
    'typescript': 'tree_sitter_typescript', 
    'java': 'tree_sitter_java',
    'cpp': 'tree_sitter_cpp',
    'go': 'tree_sitter_go',
    'rust': 'tree_sitter_rust',
    'csharp': 'tree_sitter_c_sharp'
}

# åŠ¨æ€åŠ è½½å¯é€‰è¯­è¨€è§£æå™¨
for lang, module_name in LANGUAGE_MODULES.items():
    try:
        module = __import__(module_name, fromlist=[module_name])
        AVAILABLE_PARSERS[lang] = module
    except ImportError:
        pass

# åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ æ›´å¥½çš„å¯¼å…¥å¤„ç†
try:
    from .file_parser import FileType
    from ..core.config import settings
except ImportError:
    # ç›´æ¥è¿è¡Œæ—¶çš„å›é€€
    class FileType:
        CODE = "code"
        TEXT = "text"
        BINARY = "binary"
    
    class Settings:
        AST_MAX_FILE_SIZE = 1024 * 1024
        AST_SUPPORTED_LANGUAGES = []
    
    settings = Settings()

logger = logging.getLogger(__name__)

class AstParser:
    # è¯­è¨€é…ç½®ç¼“å­˜
    _LANGUAGE_CONFIGS = {
        'python': {'extensions': {'.py'}, 'node_types': {
            'class_definition', 'function_definition', 'assignment', 
            'decorated_definition', 'import_statement', 'import_from_statement'
        }},
        'javascript': {'extensions': {'.js', '.jsx', '.mjs'}, 'node_types': {
            'class_declaration', 'function_declaration', 'method_definition',
            'arrow_function', 'variable_declaration', 'import_statement', 'export_statement'
        }},
        'typescript': {'extensions': {'.ts', '.tsx'}, 'node_types': {
            'class_declaration', 'function_declaration', 'method_definition',
            'arrow_function', 'variable_declaration', 'import_statement', 'export_statement'
        }},
        'java': {'extensions': {'.java'}, 'node_types': {
            'class_declaration', 'interface_declaration', 'method_declaration',
            'field_declaration', 'import_declaration', 'package_declaration'
        }},
        'cpp': {'extensions': {'.cpp', '.cc', '.cxx', '.c++', '.hpp', '.h'}, 'node_types': {
            'class_specifier', 'struct_specifier', 'function_definition',
            'declaration', 'preproc_include'
        }},
        'go': {'extensions': {'.go'}, 'node_types': {
            'type_declaration', 'function_declaration', 'method_declaration',
            'var_declaration', 'import_declaration', 'package_clause'
        }},
        'rust': {'extensions': {'.rs'}, 'node_types': {
            'struct_item', 'enum_item', 'impl_item', 'function_item',
            'let_declaration', 'use_declaration'
        }},
        'csharp': {'extensions': {'.cs'}, 'node_types': {
            'class_declaration', 'interface_declaration', 'struct_declaration',
            'method_declaration', 'property_declaration', 'field_declaration', 'using_directive'
        }}
    }

    def __init__(self, 
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200,
                 min_chunk_size: int = 100,
                 max_chunk_size: int = 2000,
                 class_decompose_threshold: float = 2.5):
        """åˆå§‹åŒ–ASTè§£æå™¨
        
        Args:
            chunk_size: ç›®æ ‡å—å¤§å°ï¼ˆéç©ºç™½å­—ç¬¦æ•°ï¼‰
            chunk_overlap: å—é‡å å¤§å°
            min_chunk_size: æœ€å°å—å¤§å°
            max_chunk_size: æœ€å¤§å—å¤§å°
            class_decompose_threshold: å¤§ç±»åˆ†è§£é˜ˆå€¼å€æ•°ï¼ˆç›¸å¯¹äºchunk_sizeï¼‰
        """
        self.parsers: Dict[str, Parser] = {}
        self._extension_to_language = {}
        self._element_extractors_cache = {}
        
        # åˆ†å—é…ç½®
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        self.class_decompose_threshold = class_decompose_threshold
        
        self._init_languages()

    def _init_languages(self):
        """åˆå§‹åŒ–æ”¯æŒçš„ç¼–ç¨‹è¯­è¨€"""
        initialized_count = 0
        
        for lang_name, config in self._LANGUAGE_CONFIGS.items():
            if lang_name not in AVAILABLE_PARSERS:
                logger.debug(f"âš ï¸ {lang_name} è§£æå™¨æ¨¡å—æœªå®‰è£…ï¼Œè·³è¿‡åˆå§‹åŒ–")
                continue
                
            try:
                module = AVAILABLE_PARSERS[lang_name]
                
                # è·å–è¯­è¨€å¯¹è±¡
                language = None

                if lang_name == 'typescript' and hasattr(module, 'language_typescript'):
                    language = Language(module.language_typescript())
                elif lang_name == 'typescript' and hasattr(module, 'typescript'):
                    language = Language(module.typescript())
                else:
                    language = Language(module.language())

                parser = Parser(language)
                
                self.parsers[lang_name] = parser
                
                # æ„å»ºæ‰©å±•åæ˜ å°„
                for ext in config['extensions']:
                    self._extension_to_language[ext] = lang_name
                
                initialized_count += 1
                logger.debug(f"âœ… åˆå§‹åŒ– {lang_name} è§£æå™¨æˆåŠŸ")
                
            except Exception as e:
                logger.warning(f"âš ï¸ åˆå§‹åŒ– {lang_name} è§£æå™¨å¤±è´¥: {e}")

        logger.info(f"ğŸ”§ ASTè§£æå™¨åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒ {initialized_count} ç§è¯­è¨€: {list(self.parsers.keys())}")

    def _detect_language_from_extension(self, file_path: str) -> Optional[str]:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åæ£€æµ‹ç¼–ç¨‹è¯­è¨€"""
        ext = os.path.splitext(file_path)[1].lower()
        return self._extension_to_language.get(ext)

    def should_use_ast_parsing(self, file_info: Dict[str, Any], language: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨ASTè§£æ
        
        Args:
            file_info: æ–‡ä»¶ä¿¡æ¯
            language: ç¼–ç¨‹è¯­è¨€
            
        Returns:
            bool: æ˜¯å¦ä½¿ç”¨ASTè§£æ
        """
        # åªå¯¹ä»£ç æ–‡ä»¶ä½¿ç”¨ASTè§£æ
        if file_info.get("file_type") != FileType.CODE:
            return False
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶
        max_size = getattr(settings, 'AST_MAX_FILE_SIZE', 1024 * 1024)  # 1MB
        if file_info.get("file_size", 0) > max_size:
            logger.debug(f"âš ï¸ æ–‡ä»¶è¿‡å¤§ï¼Œè·³è¿‡ASTè§£æ: {file_info.get('file_path')}")
            return False
        
        # æ£€æŸ¥è¯­è¨€æ”¯æŒ
        detected_lang = self._detect_language_from_extension(file_info.get('file_path', ''))
        return (detected_lang in self.parsers or 
                language.lower() in self.parsers)

    def parse_with_ast(self, content: str, file_path: str, language: str) -> List[Document]:
        """
        ä½¿ç”¨ASTè§£ææ–‡ä»¶å†…å®¹

        Args:
            content: æ–‡ä»¶å†…å®¹
            file_path: æ–‡ä»¶è·¯å¾„
            language: ç¼–ç¨‹è¯­è¨€

        Returns:
            List[Document]: è§£æå¾—åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        """
        # ç¡®å®šå®é™…ä½¿ç”¨çš„è¯­è¨€
        actual_language = self._determine_language(file_path, language)
        if not actual_language:
            return self._create_fallback_document(content, file_path, language, "unsupported_language")

        try:
            # ä½¿ç”¨å¯¹åº”è¯­è¨€çš„è§£æå™¨
            parser = self.parsers[actual_language]
            tree = parser.parse(content.encode('utf8'))
            
            if tree.root_node.has_error:
                logger.warning(f"âš ï¸ ASTåŒ…å«è¯­æ³•é”™è¯¯: {file_path}")
            
            documents = []
            source_bytes = content.encode('utf8')
            
            # æå–ä»£ç å…ƒç´ 
            self._extract_code_elements(tree.root_node, source_bytes, file_path, documents, actual_language)
            
            # åº”ç”¨åˆ†å—å’Œåˆå¹¶ç­–ç•¥
            processed_documents = self._process_documents_with_chunking(documents, file_path, actual_language)
            
            logger.debug(f"âœ… ASTè§£æå®Œæˆ: {file_path} ({actual_language}), æå–äº† {len(documents)} ä¸ªä»£ç å…ƒç´ ï¼Œå¤„ç†å {len(processed_documents)} ä¸ªæ–‡æ¡£å—")
            return processed_documents
            
        except Exception as e:
            logger.error(f"âŒ ASTè§£æå¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
            return self._create_fallback_document(content, file_path, language, "ast_parsing_failed")

    def _count_non_whitespace_chars(self, text: str) -> int:
        """è®¡ç®—éç©ºç™½å­—ç¬¦æ•°"""
        return len(re.sub(r'\s', '', text))

    def _process_documents_with_chunking(self, documents: List[Document], file_path: str, language: str) -> List[Document]:
        """
        å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—å’Œåˆå¹¶å¤„ç†
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            file_path: æ–‡ä»¶è·¯å¾„
            language: ç¼–ç¨‹è¯­è¨€
            
        Returns:
            List[Document]: å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            return documents
            
        processed_docs = []
        
        # é¦–å…ˆå¤„ç†éœ€è¦åˆ†å—çš„å¤§æ–‡æ¡£
        for doc in documents:
            non_ws_count = self._count_non_whitespace_chars(doc.page_content)
            
            if non_ws_count > self.max_chunk_size:
                # éœ€è¦åˆ†å—
                chunked_docs = self._chunk_large_document(doc, file_path, language)
                processed_docs.extend(chunked_docs)
            else:
                processed_docs.append(doc)
        
        # ç„¶ååˆå¹¶å°æ–‡æ¡£
        merged_docs = self._merge_small_documents(processed_docs, file_path, language)
        
        return merged_docs

    def _chunk_large_document(self, doc: Document, file_path: str, language: str) -> List[Document]:
        """
        åˆ†å—å¤§æ–‡æ¡£ï¼ˆè¯­æ³•æ„ŸçŸ¥ä¼˜å…ˆï¼Œé•¿åº¦å…œåº•ï¼‰
        
        Args:
            doc: è¦åˆ†å—çš„æ–‡æ¡£
            file_path: æ–‡ä»¶è·¯å¾„
            language: ç¼–ç¨‹è¯­è¨€
            
        Returns:
            List[Document]: åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        æ€è·¯ï¼š
        1) å…ˆç”¨å¯¹åº”è¯­è¨€çš„ parser è§£æ doc.page_contentã€‚
        2) å°½é‡åœ¨è¯­æ³•èŠ‚ç‚¹è¾¹ç•Œï¼ˆè¯­å¥ã€ç±»æˆå‘˜ã€å‡½æ•°ä½“è¯­å¥ç­‰ï¼‰è¿›è¡Œåˆ‡åˆ†å¹¶æŒ‰ chunk_size èšåˆã€‚
        3) å¦‚æœè¯­æ³•ä¸å¯ç”¨æˆ–å¤±è´¥ï¼Œé€€åŒ–ä¸ºåŸæ¥çš„æŒ‰è¡Œåˆ‡åˆ†é€»è¾‘ã€‚
        4) ä¿æŒ chunk_overlapï¼ˆæŒ‰éç©ºç™½å­—ç¬¦æ•°ï¼‰ä½œä¸ºä¸Šä¸‹æ–‡é‡å ã€‚
        """
        content = doc.page_content
        chunks: List[Document] = []

        # ä¼˜å…ˆè¯­æ³•æ„ŸçŸ¥åˆ‡åˆ†
        try:
            if language not in self.parsers:
                raise RuntimeError("parser_not_available")

            parser = self.parsers[language]
            source_bytes = content.encode("utf8")
            tree = parser.parse(source_bytes)
            root = tree.root_node

            # è·å–å€™é€‰è¯­æ³•å•å…ƒï¼ˆå°½é‡æ˜¯è¯­å¥æˆ–æˆå‘˜ï¼‰ï¼Œå¤±è´¥åˆ™æŠ›å‡ºå¼‚å¸¸èµ°å…œåº•
            units = self._get_syntax_units_for_chunking(root, source_bytes, language)
            if not units:
                raise RuntimeError("no_syntax_units")

            # åŸºäºè¯­æ³•å•å…ƒèšåˆå½¢æˆå—
            current_parts: List[str] = []
            current_non_ws = 0
            chunk_idx = 0

            def flush_chunk():
                nonlocal current_parts, current_non_ws, chunk_idx
                if not current_parts:
                    return
                # ç”¨æ¢è¡Œç¬¦è¿æ¥è¯­æ³•å•å…ƒï¼Œä¿æŒä»£ç ç»“æ„
                chunk_text = "\n".join(current_parts)
                chunk_doc = self._create_chunk_document(
                    chunk_text, doc, chunk_idx, file_path, language
                )
                chunks.append(chunk_doc)
                chunk_idx += 1
                
                # é‡å ç­–ç•¥ï¼šä¿ç•™æœ€åå‡ ä¸ªè¾ƒå°çš„è¯­æ³•å•å…ƒä½œä¸ºä¸‹ä¸€å—çš„å¼€å¤´
                overlap_parts = []
                overlap_non_ws = 0
                
                # ä»åå¾€å‰æ·»åŠ å•å…ƒï¼Œç›´åˆ°æ¥è¿‘é‡å å¤§å°
                for part in reversed(current_parts):
                    part_non_ws = self._count_non_whitespace_chars(part)
                    if overlap_non_ws + part_non_ws <= self.chunk_overlap:
                        overlap_parts.insert(0, part)
                        overlap_non_ws += part_non_ws
                    else:
                        break
                
                current_parts = overlap_parts
                current_non_ws = overlap_non_ws

            for u_start, u_end in units:
                part = source_bytes[u_start:u_end].decode("utf8").strip()
                if not part:  # è·³è¿‡ç©ºå†…å®¹
                    continue
                    
                part_len = self._count_non_whitespace_chars(part)

                # å¤„ç†è¶…å¤§å•ä¸ªè¯­æ³•å•å…ƒï¼šå¦‚æœå•ä¸ªå•å…ƒè¶…è¿‡max_chunk_sizeï¼Œå°è¯•è¿›ä¸€æ­¥åˆ†è§£
                if part_len > self.max_chunk_size:
                    logger.debug(f"å‘ç°è¶…å¤§è¯­æ³•å•å…ƒ({part_len}å­—ç¬¦)ï¼Œå°è¯•è¿›ä¸€æ­¥åˆ†è§£")
                    # å…ˆä¿å­˜å½“å‰å—
                    if current_parts:
                        flush_chunk()
                    
                    # å°è¯•åˆ†è§£è¶…å¤§å•å…ƒ
                    large_unit_chunks = self._decompose_large_unit(part, doc, len(chunks), file_path, language)
                    chunks.extend(large_unit_chunks)
                    continue

                # æ”¹è¿›çš„åˆ†å—é€»è¾‘ï¼š
                # 1. å¦‚æœå½“å‰å—å·²ç»è¶³å¤Ÿå¤§ï¼Œä¸”æ·»åŠ æ–°å•å…ƒä¼šè¶…è¿‡chunk_sizeï¼Œåˆ™åˆ†å—
                # 2. æ™ºèƒ½åˆ†å—ï¼šè€ƒè™‘è¯­æ³•å•å…ƒçš„é‡è¦æ€§
                should_chunk = False
                
                if current_parts:  # å·²æœ‰å†…å®¹
                    if current_non_ws + part_len > self.chunk_size and current_non_ws >= self.min_chunk_size:
                        should_chunk = True
                    # å¦‚æœå½“å‰å—å·²è¾¾åˆ°ç›®æ ‡å¤§å°çš„80%ï¼Œä¸”æ–°å•å…ƒä¼šä½¿å…¶æ˜æ˜¾è¶…è¿‡ï¼Œä¹Ÿåˆ†å—
                    elif (current_non_ws >= self.chunk_size * 0.8 and 
                          current_non_ws + part_len > self.chunk_size * 1.2):
                        should_chunk = True
                    # æ™ºèƒ½è¾¹ç•Œï¼šå¦‚æœæ˜¯ç±»æˆ–å‡½æ•°å®šä¹‰ï¼Œå€¾å‘äºåœ¨æ­¤åˆ†ç•Œ
                    elif (current_non_ws >= self.chunk_size * 0.6 and 
                          self._is_major_boundary(part) and
                          current_non_ws + part_len > self.chunk_size * 1.5):
                        should_chunk = True
                
                if should_chunk:
                    flush_chunk()

                current_parts.append(part)
                current_non_ws += part_len

            # æœ€åä¸€ä¸ªå—
            if current_parts:
                chunk_text = "\n".join(current_parts)
                chunk_doc = self._create_chunk_document(
                    chunk_text, doc, len(chunks), file_path, language
                )
                chunks.append(chunk_doc)

            logger.debug(
                f"ğŸ“„ å¤§æ–‡æ¡£åˆ†å—(è¯­æ³•): {doc.metadata.get('element_name', 'Unknown')} -> {len(chunks)} ä¸ªå—")
            if chunks:
                return chunks

        except Exception as e:
            # æ”¹è¿›çš„é”™è¯¯å¤„ç†ï¼šè®°å½•å…·ä½“é”™è¯¯ä½†ç»§ç»­å¤„ç†
            logger.debug(f"è¯­æ³•åˆ†å—å¤±è´¥ï¼Œå›é€€åˆ°è¡Œåˆ†å—: {str(e)}")
            pass

        # å…œåº•ï¼šæŒ‰è¡Œåˆ‡åˆ†ï¼ˆåŸé€»è¾‘ï¼‰
        lines = content.split('\n')
        current_chunk_lines: List[str] = []
        current_non_ws_count = 0

        for line in lines:
            line_non_ws = self._count_non_whitespace_chars(line)

            if (current_non_ws_count + line_non_ws > self.chunk_size and
                    current_chunk_lines and
                    current_non_ws_count >= self.min_chunk_size):

                #åˆ›å»ºå½“å‰å—
                chunk_content = '\n'.join(current_chunk_lines)
                chunk_doc = self._create_chunk_document(
                    chunk_content, doc, len(chunks), file_path, language
                )
                chunks.append(chunk_doc)

                # å¤„ç†é‡å 
                overlap_lines = self._get_overlap_lines(current_chunk_lines)
                current_chunk_lines = overlap_lines + [line]
                current_non_ws_count = self._count_non_whitespace_chars('\n'.join(current_chunk_lines))
            else:
                current_chunk_lines.append(line)
                current_non_ws_count += line_non_ws
                
        #å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk_lines:
            chunk_content = '\n'.join(current_chunk_lines)
            chunk_doc = self._create_chunk_document(
                chunk_content, doc, len(chunks), file_path, language
            )
            chunks.append(chunk_doc)

        logger.debug(
            f"ğŸ“„ å¤§æ–‡æ¡£åˆ†å—(è¡Œ): {doc.metadata.get('element_name', 'Unknown')} -> {len(chunks)} ä¸ªå—")
        return chunks

    def _get_text_overlap(self, text: str) -> str:
        """æ ¹æ® chunk_overlap ä»ç»“å°¾å›æº¯æ„é€ é‡å æ–‡æœ¬ï¼ˆæŒ‰éç©ºç™½å­—ç¬¦æ•°ï¼Œå°½é‡åœ¨è¡Œè¾¹ç•Œï¼‰ã€‚"""
        if self.chunk_overlap <= 0 or not text:
            return ""
        
        lines = text.split('\n')
        overlap_lines = []
        total_non_ws = 0
        
        # ä»æœ«å°¾å¼€å§‹æ·»åŠ å®Œæ•´çš„è¡Œï¼Œç›´åˆ°æ¥è¿‘é‡å å¤§å°
        for line in reversed(lines):
            line_non_ws = self._count_non_whitespace_chars(line)
            if total_non_ws + line_non_ws <= self.chunk_overlap:
                overlap_lines.insert(0, line)
                total_non_ws += line_non_ws
            else:
                break
        
        return '\n'.join(overlap_lines) if overlap_lines else ""

    def _is_major_boundary(self, content: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¦è¯­æ³•è¾¹ç•Œï¼ˆç±»ã€å‡½æ•°å®šä¹‰ç­‰ï¼‰"""
        content_strip = content.strip()
        # Python
        if (content_strip.startswith('class ') or 
            content_strip.startswith('def ') or 
            content_strip.startswith('async def ') or
            content_strip.startswith('@')):
            return True
        # JavaScript/TypeScript
        if (content_strip.startswith('class ') or 
            content_strip.startswith('function ') or 
            content_strip.startswith('export ') or
            content_strip.startswith('import ') or
            content_strip.startswith('const ') or
            content_strip.startswith('let ') or
            content_strip.startswith('var ')):
            return True
        # Java/C#
        if (content_strip.startswith('public class ') or 
            content_strip.startswith('private class ') or 
            content_strip.startswith('protected class ') or
            content_strip.startswith('internal class ') or
            content_strip.startswith('public interface ') or
            content_strip.startswith('public struct ') or
            content_strip.startswith('public enum ') or
            content_strip.startswith('public ') or
            content_strip.startswith('private ') or
            content_strip.startswith('protected ') or
            content_strip.startswith('namespace ') or
            content_strip.startswith('using ')):
            return True
        # Go
        if (content_strip.startswith('func ') or 
            content_strip.startswith('type ') or 
            content_strip.startswith('var ') or
            content_strip.startswith('const ') or
            content_strip.startswith('package ') or
            content_strip.startswith('import ')):
            return True
        # Rust
        if (content_strip.startswith('fn ') or 
            content_strip.startswith('struct ') or 
            content_strip.startswith('enum ') or
            content_strip.startswith('impl ') or
            content_strip.startswith('trait ') or
            content_strip.startswith('mod ') or
            content_strip.startswith('use ') or
            content_strip.startswith('pub fn ') or
            content_strip.startswith('pub struct ') or
            content_strip.startswith('pub enum ') or
            content_strip.startswith('pub trait ') or
            content_strip.startswith('pub mod ')):
            return True
        # C/C++
        if (content_strip.startswith('class ') or 
            content_strip.startswith('struct ') or 
            content_strip.startswith('namespace ') or
            content_strip.startswith('template ') or
            content_strip.startswith('template<') or
            content_strip.startswith('#include ') or
            content_strip.startswith('#define ') or
            content_strip.startswith('extern ') or
            content_strip.startswith('static ') or
            content_strip.startswith('inline ') or
            content_strip.startswith('virtual ') or
            content_strip.startswith('public:') or
            content_strip.startswith('private:') or
            content_strip.startswith('protected:')):
            return True
        return False

    def _decompose_large_unit(self, content: str, original_doc: Document, 
                             start_chunk_idx: int, file_path: str, language: str) -> List[Document]:
        """åˆ†è§£è¶…å¤§çš„è¯­æ³•å•å…ƒï¼ˆå¦‚éå¸¸é•¿çš„æ–¹æ³•ï¼‰"""
        # å¯¹äºè¶…å¤§å•å…ƒï¼Œå›é€€åˆ°è¡Œçº§åˆ†å—
        lines = content.split('\n')
        sub_chunks = []
        current_lines = []
        current_non_ws = 0
        
        for line in lines:
            line_non_ws = self._count_non_whitespace_chars(line)
            
            if (current_non_ws + line_non_ws > self.chunk_size and 
                current_lines and 
                current_non_ws >= self.min_chunk_size):
                
                # åˆ›å»ºå­å—
                sub_content = '\n'.join(current_lines)
                sub_chunk = self._create_chunk_document(
                    sub_content, original_doc, start_chunk_idx + len(sub_chunks), file_path, language
                )
                sub_chunk.metadata['is_decomposed_unit'] = True
                sub_chunks.append(sub_chunk)
                
                # å¤„ç†é‡å 
                overlap_lines = self._get_overlap_lines(current_lines)
                current_lines = overlap_lines + [line]
                current_non_ws = self._count_non_whitespace_chars('\n'.join(current_lines))
            else:
                current_lines.append(line)
                current_non_ws += line_non_ws
        
        # å¤„ç†æœ€åçš„å­å—
        if current_lines:
            sub_content = '\n'.join(current_lines)
            sub_chunk = self._create_chunk_document(
                sub_content, original_doc, start_chunk_idx + len(sub_chunks), file_path, language
            )
            sub_chunk.metadata['is_decomposed_unit'] = True
            sub_chunks.append(sub_chunk)
        
        logger.debug(f"è¶…å¤§å•å…ƒåˆ†è§£: {len(content)} å­—ç¬¦ -> {len(sub_chunks)} ä¸ªå­å—")
        return sub_chunks

    def _get_syntax_units_for_chunking(self, root: Node, source_bytes: bytes, language: str) -> List[tuple]:
        """
        è·å–ç”¨äºåˆ†å—çš„è¯­æ³•å•å…ƒåŒºé—´åˆ—è¡¨ï¼ˆstart_byte, end_byteï¼‰ã€‚
        ä¼šå°½é‡å®šä½åˆ°â€œè¯­å¥åˆ—è¡¨/æˆå‘˜åˆ—è¡¨â€ï¼Œå¦åˆ™é€€åŒ–ä¸º root çš„å‘½åå­èŠ‚ç‚¹ã€‚
        """
        def node_spans_all(n: Node) -> bool:
            # åˆ¤æ–­ n æ˜¯å¦å‡ ä¹è¦†ç›–æ•´ä¸ª rootï¼ˆé¿å…é€‰é”™å®¹å™¨ï¼‰
            total = root.end_byte - root.start_byte
            span = n.end_byte - n.start_byte
            return span >= max(0, total - 1)  # å®¹å¿ 1 å­—èŠ‚è¯¯å·®

        lang = language.lower()
        container = root

        # å°è¯•æ‰¾åˆ°æ›´åˆé€‚çš„å®¹å™¨ï¼ˆå‡½æ•°/ç±»æ•´ä¸ªä½œä¸ºå†…å®¹æ—¶ï¼‰
        if len(root.children) == 1 and root.children[0].is_named and node_spans_all(root.children[0]):
            container = root.children[0]

        def named_children(n: Node) -> List[Node]:
            return [c for c in n.children if c.is_named]

        # è¯­è¨€ç‰¹å®šï¼šå¯»æ‰¾è¯­å¥/æˆå‘˜åˆ—è¡¨
        units_nodes: List[Node] = []

        try:
            if lang == 'python':
                # å¦‚æœå®¹å™¨æ˜¯ç±»å®šä¹‰ï¼Œä¼˜å…ˆæå–ç±»å†…çš„æ–¹æ³•å’Œå±æ€§
                if container.type == 'class_definition':
                    # æ‰¾åˆ°ç±»ä½“ (block/suite)
                    class_body = None
                    for c in container.children:
                        if c.type in ('block', 'suite'):
                            class_body = c
                            break
                    if class_body:
                        units_nodes = [n for n in named_children(class_body)]
                    else:
                        units_nodes = [container]  # å›é€€åˆ°æ•´ä¸ªç±»
                else:
                    # æ¨¡å—çº§åˆ«æˆ–å…¶ä»–å®¹å™¨
                    # function/class/decorated çš„ block é‡Œæ˜¯è¯­å¥åˆ—è¡¨
                    block = None
                    for c in container.children:
                        if c.type in ('block', 'suite'):
                            block = c
                            break
                    if block:
                        units_nodes = [n for n in named_children(block)]
                    else:
                        # æ¨¡å—çº§åˆ«ï¼šç›´æ¥å–å‘½åå­èŠ‚ç‚¹ï¼Œä½†å¦‚æœæœ‰å¤§çš„ç±»å®šä¹‰ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†è§£
                        initial_units = [n for n in named_children(container)]
                        units_nodes = []
                        for unit in initial_units:
                            if unit.type == 'class_definition':
                                # å¦‚æœç±»å¾ˆå¤§ï¼Œåˆ†è§£ä¸ºç±»å£°æ˜+æ–¹æ³•
                                class_size = unit.end_byte - unit.start_byte
                                if class_size > self.chunk_size * self.class_decompose_threshold:
                                    # æ·»åŠ ç±»å£°æ˜è¡Œ
                                    class_header = None
                                    class_body = None
                                    for c in unit.children:
                                        if c.type == 'identifier' or c.type == ':':
                                            continue
                                        elif c.type in ('block', 'suite'):
                                            class_body = c
                                            break
                                    
                                    # æ·»åŠ ç±»å¤´éƒ¨ï¼ˆåˆ°å†’å·ï¼‰
                                    if class_body:
                                        header_end = class_body.start_byte
                                        units_nodes.append(MockNode(
                                            start_byte=unit.start_byte,
                                            end_byte=header_end,
                                            type='class_header'
                                        ))
                                        # æ·»åŠ ç±»ä½“å†…çš„å„ä¸ªæ–¹æ³•
                                        for method in named_children(class_body):
                                            units_nodes.append(method)
                                    else:
                                        units_nodes.append(unit)
                                else:
                                    units_nodes.append(unit)
                            else:
                                units_nodes.append(unit)

            elif lang in ('javascript', 'typescript'):
                # å¦‚æœå®¹å™¨æ˜¯ç±»å£°æ˜ï¼Œä¼˜å…ˆæå–ç±»å†…çš„æ–¹æ³•å’Œå±æ€§
                if container.type in ('class_declaration', 'class'):
                    # æ‰¾åˆ°ç±»ä½“
                    class_body = None
                    for c in container.children:
                        if c.type in ('class_body', 'object_type'):
                            class_body = c
                            break
                    if class_body:
                        units_nodes = [n for n in named_children(class_body)]
                    else:
                        units_nodes = [container]  # å›é€€åˆ°æ•´ä¸ªç±»
                else:
                    # æ¨¡å—çº§åˆ«ï¼šç›´æ¥å–å‘½åå­èŠ‚ç‚¹ï¼Œä½†å¦‚æœæœ‰å¤§çš„ç±»å®šä¹‰ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†è§£
                    initial_units = [n for n in named_children(container)]
                    units_nodes = []
                    for unit in initial_units:
                        if unit.type in ('class_declaration', 'class'):
                            # å¦‚æœç±»å¾ˆå¤§ï¼Œåˆ†è§£ä¸ºç±»å£°æ˜+æ–¹æ³•
                            class_size = unit.end_byte - unit.start_byte
                            if class_size > self.chunk_size * self.class_decompose_threshold:
                                # æ‰¾åˆ°ç±»ä½“
                                class_body = None
                                for c in unit.children:
                                    if c.type in ('class_body', 'object_type'):
                                        class_body = c
                                        break
                                
                                # æ·»åŠ ç±»å¤´éƒ¨ï¼ˆåˆ°å¤§æ‹¬å·ï¼‰
                                if class_body:
                                    header_end = class_body.start_byte
                                    units_nodes.append(MockNode(
                                        start_byte=unit.start_byte,
                                        end_byte=header_end,
                                        type='class_header'
                                    ))
                                    # æ·»åŠ ç±»ä½“å†…çš„å„ä¸ªæ–¹æ³•
                                    for method in named_children(class_body):
                                        units_nodes.append(method)
                                else:
                                    units_nodes.append(unit)
                            else:
                                units_nodes.append(unit)
                        else:
                            units_nodes.append(unit)

            elif lang in ('java', 'csharp'):
                # å¦‚æœå®¹å™¨æ˜¯ç±»/æ¥å£/ç»“æ„ä½“å£°æ˜ï¼Œä¼˜å…ˆæå–å†…éƒ¨æˆå‘˜
                if container.type in ('class_declaration', 'interface_declaration', 'struct_declaration'):
                    # æ‰¾åˆ°ç±»ä½“
                    body = None
                    for c in container.children:
                        if c.type in ('class_body', 'interface_body', 'struct_body'):
                            body = c
                            break
                    if body:
                        units_nodes = [n for n in named_children(body)]
                    else:
                        units_nodes = [container]  # å›é€€åˆ°æ•´ä¸ªç±»
                else:
                    # æ¨¡å—çº§åˆ«ï¼šç›´æ¥å–å‘½åå­èŠ‚ç‚¹ï¼Œä½†å¦‚æœæœ‰å¤§çš„ç±»å®šä¹‰ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†è§£
                    initial_units = [n for n in named_children(container)]
                    units_nodes = []
                    for unit in initial_units:
                        if unit.type in ('class_declaration', 'interface_declaration', 'struct_declaration'):
                            # å¦‚æœç±»å¾ˆå¤§ï¼Œåˆ†è§£ä¸ºç±»å£°æ˜+æ–¹æ³•
                            class_size = unit.end_byte - unit.start_byte
                            if class_size > self.chunk_size * self.class_decompose_threshold:
                                # æ‰¾åˆ°ç±»ä½“
                                body = None
                                for c in unit.children:
                                    if c.type in ('class_body', 'interface_body', 'struct_body'):
                                        body = c
                                        break
                                
                                # æ·»åŠ ç±»å¤´éƒ¨ï¼ˆåˆ°å¤§æ‹¬å·ï¼‰
                                if body:
                                    header_end = body.start_byte
                                    units_nodes.append(MockNode(
                                        start_byte=unit.start_byte,
                                        end_byte=header_end,
                                        type='class_header'
                                    ))
                                    # æ·»åŠ ç±»ä½“å†…çš„å„ä¸ªæˆå‘˜
                                    for member in named_children(body):
                                        units_nodes.append(member)
                                else:
                                    units_nodes.append(unit)
                            else:
                                units_nodes.append(unit)
                        else:
                            units_nodes.append(unit)

            elif lang == 'go':
                # Goè¯­è¨€çš„ç»“æ„ä½“å’Œæ¥å£å¤„ç†
                if container.type in ('type_declaration', 'source_file'):
                    initial_units = [n for n in named_children(container)]
                    units_nodes = []
                    for unit in initial_units:
                        if unit.type == 'type_declaration':
                            # æ£€æŸ¥æ˜¯å¦æ˜¯å¤§çš„ç»“æ„ä½“æˆ–æ¥å£
                            type_size = unit.end_byte - unit.start_byte
                            if type_size > self.chunk_size * self.class_decompose_threshold:
                                # åˆ†è§£ä¸ºç±»å‹å£°æ˜+æ–¹æ³•
                                units_nodes.append(unit)  # Goçš„ç±»å‹å£°æ˜ç›¸å¯¹ç®€å•ï¼Œæš‚æ—¶ä¸åˆ†è§£
                            else:
                                units_nodes.append(unit)
                        else:
                            units_nodes.append(unit)
                else:
                    units_nodes = [n for n in named_children(container)]
                    
            elif lang == 'rust':
                # Rustçš„ç»“æ„ä½“ã€æšä¸¾ã€implå—å¤„ç†
                if container.type in ('source_file', 'mod_item'):
                    initial_units = [n for n in named_children(container)]
                    units_nodes = []
                    for unit in initial_units:
                        if unit.type in ('struct_item', 'enum_item', 'impl_item'):
                            # å¦‚æœç»“æ„ä½“/æšä¸¾/implå¾ˆå¤§ï¼Œåˆ†è§£å®ƒ
                            item_size = unit.end_byte - unit.start_byte
                            if item_size > self.chunk_size * self.class_decompose_threshold:
                                if unit.type == 'impl_item':
                                    # implå—å¯ä»¥åˆ†è§£ä¸ºimplå£°æ˜+å„ä¸ªæ–¹æ³•
                                    impl_body = None
                                    for c in unit.children:
                                        if c.type == 'declaration_list':
                                            impl_body = c
                                            break
                                    
                                    if impl_body:
                                        header_end = impl_body.start_byte
                                        units_nodes.append(MockNode(
                                            start_byte=unit.start_byte,
                                            end_byte=header_end,
                                            type='impl_header'
                                        ))
                                        # æ·»åŠ implä½“å†…çš„å„ä¸ªæ–¹æ³•
                                        for method in named_children(impl_body):
                                            units_nodes.append(method)
                                    else:
                                        units_nodes.append(unit)
                                else:
                                    units_nodes.append(unit)  # ç»“æ„ä½“å’Œæšä¸¾æš‚æ—¶ä¸åˆ†è§£
                            else:
                                units_nodes.append(unit)
                        else:
                            units_nodes.append(unit)
                else:
                    units_nodes = [n for n in named_children(container)]
                    
            elif lang in ('cpp', 'c'):
                # C++çš„ç±»å’Œç»“æ„ä½“å¤„ç†
                if container.type in ('translation_unit',):
                    initial_units = [n for n in named_children(container)]
                    units_nodes = []
                    for unit in initial_units:
                        if unit.type in ('class_specifier', 'struct_specifier'):
                            # å¦‚æœç±»å¾ˆå¤§ï¼Œåˆ†è§£ä¸ºç±»å£°æ˜+æ–¹æ³•
                            class_size = unit.end_byte - unit.start_byte
                            if class_size > self.chunk_size * self.class_decompose_threshold:
                                # C++ç±»ä½“é€šå¸¸åœ¨field_declaration_listä¸­
                                class_body = None
                                for c in unit.children:
                                    if c.type == 'field_declaration_list':
                                        class_body = c
                                        break
                                
                                if class_body:
                                    header_end = class_body.start_byte
                                    units_nodes.append(MockNode(
                                        start_byte=unit.start_byte,
                                        end_byte=header_end,
                                        type='class_header'
                                    ))
                                    # æ·»åŠ ç±»ä½“å†…çš„å„ä¸ªæˆå‘˜
                                    for member in named_children(class_body):
                                        units_nodes.append(member)
                                else:
                                    units_nodes.append(unit)
                            else:
                                units_nodes.append(unit)
                        else:
                            units_nodes.append(unit)
                else:
                    units_nodes = [n for n in named_children(container)]

            else:
                # é€šç”¨å¤„ç†ï¼šå¯¹äºæœªçŸ¥è¯­è¨€ï¼Œå°è¯•åŸºæœ¬çš„å¤§èŠ‚ç‚¹åˆ†è§£
                initial_units = [n for n in named_children(container)]
                units_nodes = []
                for unit in initial_units:
                    # å¦‚æœå•ä¸ªèŠ‚ç‚¹å¾ˆå¤§ï¼Œå°è¯•åˆ†è§£å…¶å­èŠ‚ç‚¹
                    unit_size = unit.end_byte - unit.start_byte
                    if unit_size > self.chunk_size * self.class_decompose_threshold:
                        children = named_children(unit)
                        if len(children) > 1:  # æœ‰å¤šä¸ªå­èŠ‚ç‚¹å¯ä»¥åˆ†è§£
                            units_nodes.extend(children)
                        else:
                            units_nodes.append(unit)
                    else:
                        units_nodes.append(unit)

        except Exception:
            units_nodes = []

        # è¿‡æ»¤æ‰éå¸¸å°æˆ–æ— æ„ä¹‰çš„èŠ‚ç‚¹ï¼ˆå¦‚æ³¨é‡Š/ç©ºæ ‡è®°ï¼‰ï¼Œç¡®ä¿åº
        units_nodes = [n for n in units_nodes if n.end_byte > n.start_byte]
        units_nodes.sort(key=lambda n: n.start_byte)

        # åˆå¹¶ç›¸é‚»è¢«è¯­æ³•æ¼æ‰çš„ç©ºæ´ï¼šç”¨ root çš„èŒƒå›´å…œåº•
        if not units_nodes:
            return [(root.start_byte, root.end_byte)]

        ranges: List[tuple] = []
        prev_end = units_nodes[0].start_byte
        # å¦‚æœå¼€å¤´æœ‰ç©ºæ´ï¼Œå¡«ä¸Š
        if prev_end > root.start_byte:
            ranges.append((root.start_byte, prev_end))

        # å•å…ƒæœ¬èº«
        for n in units_nodes:
            ranges.append((n.start_byte, n.end_byte))
            prev_end = n.end_byte

        # å°¾éƒ¨ç©ºæ´
        if prev_end < root.end_byte:
            ranges.append((prev_end, root.end_byte))

        # å»æ‰å…¨æ˜¯ç©ºç™½çš„æ®µ
        cleaned: List[tuple] = []
        for s, e in ranges:
            seg = source_bytes[s:e].decode('utf8')
            if self._count_non_whitespace_chars(seg) > 0:
                cleaned.append((s, e))

        return cleaned

    def _get_overlap_lines(self, lines: List[str]) -> List[str]:
        """è·å–é‡å çš„è¡Œ"""
        if not lines or self.chunk_overlap <= 0:
            return []
            
        overlap_chars = 0
        overlap_lines = []
        
        # ä»æœ«å°¾å¼€å§‹è®¡ç®—é‡å 
        for line in reversed(lines):
            line_non_ws = self._count_non_whitespace_chars(line)
            if overlap_chars + line_non_ws <= self.chunk_overlap:
                overlap_lines.insert(0, line)
                overlap_chars += line_non_ws
            else:
                break
                
        return overlap_lines

    def _create_chunk_document(self, content: str, original_doc: Document, 
                             chunk_index: int, file_path: str, language: str) -> Document:
        """åˆ›å»ºåˆ†å—æ–‡æ¡£"""
        metadata = original_doc.metadata.copy()
        metadata.update({
            "is_chunk": True,
            "chunk_index": chunk_index,
            "original_element_name": metadata.get("element_name", "Unknown"),
            "chunk_non_ws_chars": self._count_non_whitespace_chars(content)
        })
        
        return Document(
            page_content=content,
            metadata=metadata
        )

    def _merge_small_documents(self, documents: List[Document], file_path: str, language: str) -> List[Document]:
        """
        åˆå¹¶å°æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            file_path: æ–‡ä»¶è·¯å¾„
            language: ç¼–ç¨‹è¯­è¨€
            
        Returns:
            List[Document]: åˆå¹¶åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not documents:
            return documents
            
        merged_docs = []
        current_merge_group = []
        current_merge_size = 0
        
        # æŒ‰å…ƒç´ ç±»å‹åˆ†ç»„ï¼Œä¼˜å…ˆçº§ï¼šimport < assignment < function < class
        element_priority = {
            "import": 1,
            "assignment": 2, 
            "function": 3,
            "decorated_definition": 3,
            "class": 4
        }
        
        # æŒ‰ä¼˜å…ˆçº§å’Œä½ç½®æ’åº
        sorted_docs = sorted(documents, key=lambda doc: (
            element_priority.get(doc.metadata.get("element_type", "unknown"), 5),
            doc.metadata.get("start_line", 0)
        ))
        
        for doc in sorted_docs:
            non_ws_count = self._count_non_whitespace_chars(doc.page_content)
            
            # å¦‚æœæ–‡æ¡£å·²ç»è¶³å¤Ÿå¤§ï¼Œç›´æ¥æ·»åŠ 
            if non_ws_count >= self.min_chunk_size:
                # å…ˆå¤„ç†å½“å‰åˆå¹¶ç»„
                if current_merge_group:
                    merged_doc = self._create_merged_document(current_merge_group, file_path, language)
                    merged_docs.append(merged_doc)
                    current_merge_group = []
                    current_merge_size = 0
                
                merged_docs.append(doc)
                continue
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆå¹¶
            can_merge = self._can_merge_documents(current_merge_group, doc)
            
            if (can_merge and 
                current_merge_size + non_ws_count <= self.chunk_size):
                # åŠ å…¥å½“å‰åˆå¹¶ç»„
                current_merge_group.append(doc)
                current_merge_size += non_ws_count
            else:
                # ç»“æŸå½“å‰åˆå¹¶ç»„ï¼Œå¼€å§‹æ–°çš„
                if current_merge_group:
                    merged_doc = self._create_merged_document(current_merge_group, file_path, language)
                    merged_docs.append(merged_doc)
                
                current_merge_group = [doc]
                current_merge_size = non_ws_count
        
        # å¤„ç†æœ€åä¸€ä¸ªåˆå¹¶ç»„
        if current_merge_group:
            merged_doc = self._create_merged_document(current_merge_group, file_path, language)
            merged_docs.append(merged_doc)
        
        logger.debug(f"ğŸ”— æ–‡æ¡£åˆå¹¶: {len(documents)} -> {len(merged_docs)} ä¸ªæ–‡æ¡£")
        return merged_docs

    def _can_merge_documents(self, current_group: List[Document], new_doc: Document) -> bool:
        """åˆ¤æ–­æ–‡æ¡£æ˜¯å¦å¯ä»¥åˆå¹¶"""
        if not current_group:
            return True
            
        # ç›¸åŒç±»å‹çš„å…ƒç´ å¯ä»¥åˆå¹¶
        last_doc = current_group[-1]
        last_type = last_doc.metadata.get("element_type", "")
        new_type = new_doc.metadata.get("element_type", "")
        
        # å¯¼å…¥è¯­å¥å¯ä»¥åˆå¹¶
        if last_type == "import" and new_type == "import":
            return True
            
        # åŒç±»å‹çš„èµ‹å€¼å¯ä»¥åˆå¹¶
        if last_type == "assignment" and new_type == "assignment":
            return True
            
        # å°å‡½æ•°å¯ä»¥åˆå¹¶
        if (last_type in ["function", "decorated_definition"] and 
            new_type in ["function", "decorated_definition"]):
            last_size = self._count_non_whitespace_chars(last_doc.page_content)
            new_size = self._count_non_whitespace_chars(new_doc.page_content)
            if last_size < self.min_chunk_size and new_size < self.min_chunk_size:
                return True
        
        return False

    def _create_merged_document(self, docs: List[Document], file_path: str, language: str) -> Document:
        """åˆ›å»ºåˆå¹¶æ–‡æ¡£"""
        if len(docs) == 1:
            return docs[0]
            
        # åˆå¹¶å†…å®¹
        contents = [doc.page_content for doc in docs]
        merged_content = '\n\n'.join(contents)
        
        # åˆå¹¶å…ƒæ•°æ®
        element_types = [doc.metadata.get("element_type", "") for doc in docs]
        element_names = [doc.metadata.get("element_name", "") for doc in docs]
        
        # ç¡®å®šä¸»è¦ç±»å‹
        type_counts = {}
        for et in element_types:
            type_counts[et] = type_counts.get(et, 0) + 1
        main_type = max(type_counts, key=type_counts.get) if type_counts else "merged"
        
        # åˆ›å»ºåˆå¹¶çš„å…ƒæ•°æ®
        merged_metadata = {
            "file_path": file_path,
            "language": language,
            "element_type": main_type,
            "element_name": f"merged_{main_type}",
            "is_merged": True,
            "merged_count": len(docs),
            "merged_elements": json.dumps(element_names),
            "start_line": min(doc.metadata.get("start_line", 0) for doc in docs),
            "end_line": max(doc.metadata.get("end_line", 0) for doc in docs),
            "merged_non_ws_chars": self._count_non_whitespace_chars(merged_content)
        }
        
        return Document(
            page_content=merged_content,
            metadata=merged_metadata
        )

    def _determine_language(self, file_path: str, language: str) -> Optional[str]:
        """ç¡®å®šè¦ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€"""
        # ä¼˜å…ˆä½¿ç”¨æ–‡ä»¶æ‰©å±•åæ£€æµ‹
        detected_lang = self._detect_language_from_extension(file_path)
        if detected_lang and detected_lang in self.parsers:
            return detected_lang
        
        # å…¶æ¬¡ä½¿ç”¨ä¼ å…¥çš„è¯­è¨€å‚æ•°
        normalized_lang = language.lower()
        if normalized_lang in self.parsers:
            return normalized_lang
            
        return None

    def _create_fallback_document(self, content: str, file_path: str, language: str, error_type: str) -> List[Document]:
        """åˆ›å»ºå›é€€æ–‡æ¡£"""
        if error_type == "unsupported_language":
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„è¯­è¨€: {language}, æ–‡ä»¶: {file_path}")
        
        return [Document(
            page_content=content,
            metadata={
                "file_path": file_path,
                "language": language,
                "element_type": "file",
                error_type: True
            }
        )]

    def _extract_code_elements(self, node: Node, source_bytes: bytes, file_path: str, 
                             documents: List[Document], language: str):
        """
        é€’å½’æå–ä»£ç å…ƒç´ 
        
        Args:
            node: ASTèŠ‚ç‚¹
            source_bytes: æºä»£ç å­—èŠ‚
            file_path: æ–‡ä»¶è·¯å¾„
            documents: æ–‡æ¡£åˆ—è¡¨
            language: ç¼–ç¨‹è¯­è¨€
        """
        # è·å–å…ƒç´ æå–å™¨ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        extractors = self._get_element_extractors_cached(language)
        
        # å¦‚æœå½“å‰èŠ‚ç‚¹æ˜¯ç›®æ ‡ç±»å‹ï¼Œæå–å®ƒ
        if node.type in extractors:
            try:
                doc = extractors[node.type](node, source_bytes, file_path, language)
                if doc:
                    documents.append(doc)
            except Exception as e:
                logger.warning(f"âš ï¸ æå–èŠ‚ç‚¹å¤±è´¥: {node.type} in {file_path}, é”™è¯¯: {str(e)}")
        
        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        for child in node.children:
            self._extract_code_elements(child, source_bytes, file_path, documents, language)

    def _get_element_extractors_cached(self, language: str) -> Dict[str, Callable]:
        """è·å–å…ƒç´ æå–å™¨ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if language not in self._element_extractors_cache:
            self._element_extractors_cache[language] = self._build_element_extractors(language)
        return self._element_extractors_cache[language]

    def _build_element_extractors(self, language: str) -> Dict[str, Callable]:
        """æ„å»ºå…ƒç´ æå–å™¨æ˜ å°„"""
        # åŸºç¡€æå–å™¨
        extractors = {}
        
        # æ ¹æ®è¯­è¨€é…ç½®æ·»åŠ æå–å™¨
        config = self._LANGUAGE_CONFIGS.get(language, {})
        node_types = config.get('node_types', set())
        
        for node_type in node_types:
            if 'class' in node_type or 'struct' in node_type or 'interface' in node_type or 'enum' in node_type:
                extractors[node_type] = self._extract_class
            elif 'function' in node_type or 'method' in node_type:
                extractors[node_type] = self._extract_function
            elif 'import' in node_type or 'export' in node_type or 'package' in node_type or 'using' in node_type or 'use_declaration' in node_type:
                extractors[node_type] = self._extract_import
            elif 'assignment' in node_type or 'declaration' in node_type or 'var_' in node_type or 'let_' in node_type or 'field_' in node_type or 'property_' in node_type:
                extractors[node_type] = self._extract_assignment
            elif 'decorated' in node_type:
                extractors[node_type] = self._extract_decorated_definition
                
        return extractors

    def _extract_class(self, node: Node, source_bytes: bytes, file_path: str, language: str = "python") -> Document:
        """æå–ç±»å®šä¹‰"""
        content = source_bytes[node.start_byte:node.end_byte].decode('utf8')
        
        # æå–ç±»å - æ ¹æ®è¯­è¨€è°ƒæ•´
        class_name = self._extract_identifier(node, source_bytes, language)
        
        return Document(
            page_content=content,
            metadata={
                "file_path": file_path,
                "element_type": "class",
                "element_name": class_name,
                "start_line": node.start_point[0] + 1,
                "end_line": node.end_point[0] + 1,
                "language": language
            }
        )

    def _extract_function(self, node: Node, source_bytes: bytes, file_path: str, language: str = "python") -> Document:
        """æå–å‡½æ•°å®šä¹‰"""
        content = source_bytes[node.start_byte:node.end_byte].decode('utf8')
        
        # æå–å‡½æ•°å
        function_name = self._extract_identifier(node, source_bytes, language)
        
        return Document(
            page_content=content,
            metadata={
                "file_path": file_path,
                "element_type": "function",
                "element_name": function_name,
                "start_line": node.start_point[0] + 1,
                "end_line": node.end_point[0] + 1,
                "language": language
            }
        )

    def _extract_import(self, node: Node, source_bytes: bytes, file_path: str, language: str = "python") -> Document:
        """æå–å¯¼å…¥è¯­å¥"""
        content = source_bytes[node.start_byte:node.end_byte].decode('utf8')
        
        return Document(
            page_content=content,
            metadata={
                "file_path": file_path,
                "element_type": "import",
                "element_name": content.strip(),
                "start_line": node.start_point[0] + 1,
                "end_line": node.end_point[0] + 1,
                "language": language
            }
        )

    def _extract_assignment(self, node: Node, source_bytes: bytes, file_path: str, language: str = "python") -> Document:
        """æå–å˜é‡èµ‹å€¼"""
        content = source_bytes[node.start_byte:node.end_byte].decode('utf8')
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ¨¡å—çº§åˆ«çš„èµ‹å€¼
        if language == 'python':
            parent = node.parent
            while parent:
                if parent.type in ['function_definition', 'class_definition']:
                    return None
                parent = parent.parent
        
        # æå–å˜é‡å
        variable_name = self._extract_variable_name(node, source_bytes, language)
        
        return Document(
            page_content=content,
            metadata={
                "file_path": file_path,
                "element_type": "assignment",
                "element_name": variable_name,
                "start_line": node.start_point[0] + 1,
                "end_line": node.end_point[0] + 1,
                "language": language
            }
        )

    def _extract_decorated_definition(self, node: Node, source_bytes: bytes, file_path: str, language: str = "python") -> Document:
        """æå–è£…é¥°å™¨å®šä¹‰"""
        content = source_bytes[node.start_byte:node.end_byte].decode('utf8')
        
        # æŸ¥æ‰¾è¢«è£…é¥°çš„å®šä¹‰
        definition_name = self._extract_identifier(node, source_bytes, language)
        
        return Document(
            page_content=content,
            metadata={
                "file_path": file_path,
                "element_type": "decorated_definition",
                "element_name": definition_name,
                "start_line": node.start_point[0] + 1,
                "end_line": node.end_point[0] + 1,
                "language": language
            }
        )

    def _extract_identifier(self, node: Node, source_bytes: bytes, language: str) -> str:
        """æå–æ ‡è¯†ç¬¦åç§°"""
        # è¯­è¨€ç‰¹å®šçš„æ ‡è¯†ç¬¦æå–ç­–ç•¥
        if language == 'javascript' or language == 'typescript':
            return self._extract_js_identifier(node, source_bytes)
        elif language == 'java':
            return self._extract_java_identifier(node, source_bytes)
        elif language == 'python':
            return self._extract_python_identifier(node, source_bytes)
        else:
            # é€šç”¨æå–é€»è¾‘
            return self._extract_generic_identifier(node, source_bytes)

    def _extract_js_identifier(self, node: Node, source_bytes: bytes) -> str:
        """æå–JavaScript/TypeScriptæ ‡è¯†ç¬¦"""
        # JavaScriptæ–¹æ³•å®šä¹‰çš„ç‰¹æ®Šå¤„ç†
        if node.type == 'method_definition':
            # æŸ¥æ‰¾property_identifierèŠ‚ç‚¹
            for child in node.children:
                if child.type == 'property_identifier':
                    return source_bytes[child.start_byte:child.end_byte].decode('utf8')
        
        # å‡½æ•°å£°æ˜å’Œç®­å¤´å‡½æ•°
        if node.type in ['function_declaration', 'arrow_function']:
            for child in node.children:
                if child.type == 'identifier':
                    return source_bytes[child.start_byte:child.end_byte].decode('utf8')
        
        # å˜é‡å£°æ˜
        if node.type == 'variable_declaration':
            for child in node.children:
                if child.type == 'variable_declarator':
                    for grandchild in child.children:
                        if grandchild.type == 'identifier':
                            return source_bytes[grandchild.start_byte:grandchild.end_byte].decode('utf8')
        
        # é€šç”¨æŸ¥æ‰¾
        return self._extract_generic_identifier(node, source_bytes)

    def _extract_python_identifier(self, node: Node, source_bytes: bytes) -> str:
        """æå–Pythonæ ‡è¯†ç¬¦"""
        # ç›´æ¥æŸ¥æ‰¾identifierèŠ‚ç‚¹
        for child in node.children:
            if child.type == "identifier":
                return source_bytes[child.start_byte:child.end_byte].decode('utf8')
        
        # é€’å½’æŸ¥æ‰¾
        return self._extract_identifier_recursive(node, source_bytes, max_depth=2)

    def _extract_java_identifier(self, node: Node, source_bytes: bytes) -> str:
        """æå–Javaæ ‡è¯†ç¬¦"""
        # Javaç‰¹å®šçš„æ ‡è¯†ç¬¦æå–
        for child in node.children:
            if child.type == "identifier":
                return source_bytes[child.start_byte:child.end_byte].decode('utf8')
        
        return self._extract_generic_identifier(node, source_bytes)

    def _extract_generic_identifier(self, node: Node, source_bytes: bytes) -> str:
        """é€šç”¨æ ‡è¯†ç¬¦æå–"""
        # ç›´æ¥æŸ¥æ‰¾identifierèŠ‚ç‚¹
        for child in node.children:
            if child.type == "identifier":
                return source_bytes[child.start_byte:child.end_byte].decode('utf8')
        
        # é€’å½’æŸ¥æ‰¾ï¼ˆé™åˆ¶æ·±åº¦ï¼‰
        return self._extract_identifier_recursive(node, source_bytes, max_depth=3)

    def _extract_identifier_recursive(self, node: Node, source_bytes: bytes, max_depth: int = 2) -> str:
        """é€’å½’æå–æ ‡è¯†ç¬¦ï¼ˆé™åˆ¶æ·±åº¦ï¼‰"""
        if max_depth <= 0:
            return "Unknown"
            
        for child in node.children:
            if child.type in ["identifier", "property_identifier"]:
                return source_bytes[child.start_byte:child.end_byte].decode('utf8')
            
            # é€’å½’æŸ¥æ‰¾
            result = self._extract_identifier_recursive(child, source_bytes, max_depth - 1)
            if result != "Unknown":
                return result
        
        return "Unknown"

    def _extract_variable_name(self, node: Node, source_bytes: bytes, language: str) -> str:
        """æå–å˜é‡å"""
        # å¿«é€Ÿè·¯å¾„ï¼šPythonç®€å•èµ‹å€¼
        if language == 'python':
            content = source_bytes[node.start_byte:node.end_byte].decode('utf8')
            if '=' in content:
                return content.split('=')[0].strip()
        
        # é€šç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾variable_declaratoræˆ–identifier
        for child in node.children:
            if child.type == 'variable_declarator':
                identifier = self._extract_identifier(child, source_bytes, language)
                if identifier != "Unknown":
                    return identifier
            elif child.type == 'identifier':
                return source_bytes[child.start_byte:child.end_byte].decode('utf8')
        
        return "Unknown"

    def get_supported_languages(self) -> List[str]:
        """è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨"""
        return list(self.parsers.keys())

    def get_language_extensions(self, language: str) -> Set[str]:
        """è·å–è¯­è¨€æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å"""
        config = self._LANGUAGE_CONFIGS.get(language, {})
        return config.get('extensions', set())
